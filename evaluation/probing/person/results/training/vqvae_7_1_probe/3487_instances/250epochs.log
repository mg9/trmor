
number of params: 2247 
Namespace(batchsize=64, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=250, fig_path='evaluation/probing/person/results/training/vqvae_7_1_probe/3487_instances/32dim/250epochs.png', log_path='evaluation/probing/person/results/training/vqvae_7_1_probe/3487_instances/32dim/250epochs.log', logger=<common.utils.Logger object at 0x7fcd71089b50>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_7_1_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (linear): Linear(in_features=320, out_features=7, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/person/results/training/vqvae_7_1_probe/3487_instances/32dim/', nh=512, ni=256, num_dicts=7, nz=512, opt='Adam', orddict_emb_num=100, pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), rootdict_emb_dim=320, rootdict_emb_num=1000, save_path='evaluation/probing/person/results/training/vqvae_7_1_probe/3487_instances/32dim/250epochs.pt', seq_to_no_pad='surface', task='surf2person', trndata='evaluation/probing/person/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/person/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/person/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_root.weight, torch.Size([320, 512]): False
linear_root.bias, torch.Size([320]): False
vq_layer_root.embedding.weight, torch.Size([1000, 320]): False
ord_linears.0.weight, torch.Size([32, 512]): False
ord_linears.0.bias, torch.Size([32]): False
ord_linears.1.weight, torch.Size([32, 512]): False
ord_linears.1.bias, torch.Size([32]): False
ord_linears.2.weight, torch.Size([32, 512]): False
ord_linears.2.bias, torch.Size([32]): False
ord_linears.3.weight, torch.Size([32, 512]): False
ord_linears.3.bias, torch.Size([32]): False
ord_linears.4.weight, torch.Size([32, 512]): False
ord_linears.4.bias, torch.Size([32]): False
ord_linears.5.weight, torch.Size([32, 512]): False
ord_linears.5.bias, torch.Size([32]): False
ord_vq_layers.0.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.1.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.2.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.3.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.4.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.5.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([7, 320]): True
linear.bias, torch.Size([7]): True
epoch: 0 avg_loss: 1.6963, acc: 0.3843 
val --- avg_loss: 1.6842, acc: 0.3636  
update best loss 

epoch: 1 avg_loss: 1.4964, acc: 0.4654 
val --- avg_loss: 1.6457, acc: 0.3589  
update best loss 

epoch: 2 avg_loss: 1.4610, acc: 0.4652 
val --- avg_loss: 1.6444, acc: 0.3589  
update best loss 

epoch: 3 avg_loss: 1.4251, acc: 0.4672 
val --- avg_loss: 1.6303, acc: 0.3589  
update best loss 

epoch: 4 avg_loss: 1.4107, acc: 0.4643 
val --- avg_loss: 1.6349, acc: 0.3589  

epoch: 5 avg_loss: 1.3942, acc: 0.4712 
val --- avg_loss: 1.6376, acc: 0.3541  

epoch: 6 avg_loss: 1.3805, acc: 0.4703 
val --- avg_loss: 1.6743, acc: 0.3493  

epoch: 7 avg_loss: 1.3746, acc: 0.4752 
val --- avg_loss: 1.6645, acc: 0.3589  

epoch: 8 avg_loss: 1.3649, acc: 0.4715 
val --- avg_loss: 1.6622, acc: 0.3541  

epoch: 9 avg_loss: 1.3577, acc: 0.4695 
val --- avg_loss: 1.6618, acc: 0.3636  

epoch: 10 avg_loss: 1.3522, acc: 0.4786 
val --- avg_loss: 1.6812, acc: 0.3541  

epoch: 11 avg_loss: 1.3470, acc: 0.4715 
val --- avg_loss: 1.6765, acc: 0.3541  

epoch: 12 avg_loss: 1.3553, acc: 0.4703 
val --- avg_loss: 1.6969, acc: 0.3636  

epoch: 13 avg_loss: 1.3393, acc: 0.4763 
val --- avg_loss: 1.6539, acc: 0.3541  

epoch: 14 avg_loss: 1.3368, acc: 0.4743 
val --- avg_loss: 1.6720, acc: 0.3684  

epoch: 15 avg_loss: 1.3356, acc: 0.4709 
val --- avg_loss: 1.6992, acc: 0.3541  

epoch: 16 avg_loss: 1.3342, acc: 0.4763 
val --- avg_loss: 1.6815, acc: 0.3589  

epoch: 17 avg_loss: 1.3336, acc: 0.4703 
val --- avg_loss: 1.6953, acc: 0.3732  

epoch: 18 avg_loss: 1.3272, acc: 0.4778 
val --- avg_loss: 1.6954, acc: 0.3636  

epoch: 19 avg_loss: 1.3242, acc: 0.4735 
val --- avg_loss: 1.7172, acc: 0.3445  

epoch: 20 avg_loss: 1.3243, acc: 0.4829 
val --- avg_loss: 1.6874, acc: 0.3589  

epoch: 21 avg_loss: 1.3210, acc: 0.4712 
val --- avg_loss: 1.6993, acc: 0.3493  

epoch: 22 avg_loss: 1.3241, acc: 0.4763 
val --- avg_loss: 1.7209, acc: 0.3589  

epoch: 23 avg_loss: 1.3155, acc: 0.4829 
val --- avg_loss: 1.7183, acc: 0.3541  

epoch: 24 avg_loss: 1.3160, acc: 0.4726 
val --- avg_loss: 1.7084, acc: 0.3636  

epoch: 25 avg_loss: 1.3187, acc: 0.4695 
val --- avg_loss: 1.7432, acc: 0.3541  

epoch: 26 avg_loss: 1.3132, acc: 0.4749 
val --- avg_loss: 1.6966, acc: 0.3636  

epoch: 27 avg_loss: 1.3157, acc: 0.4706 
val --- avg_loss: 1.7144, acc: 0.3589  

epoch: 28 avg_loss: 1.3130, acc: 0.4778 
val --- avg_loss: 1.7110, acc: 0.3493  

epoch: 29 avg_loss: 1.3090, acc: 0.4723 
val --- avg_loss: 1.7120, acc: 0.3636  

epoch: 30 avg_loss: 1.3025, acc: 0.4766 
val --- avg_loss: 1.7290, acc: 0.3684  

epoch: 31 avg_loss: 1.3094, acc: 0.4740 
val --- avg_loss: 1.7245, acc: 0.3589  

epoch: 32 avg_loss: 1.3033, acc: 0.4752 
val --- avg_loss: 1.7125, acc: 0.3589  

epoch: 33 avg_loss: 1.3012, acc: 0.4783 
val --- avg_loss: 1.7313, acc: 0.3541  

epoch: 34 avg_loss: 1.3093, acc: 0.4677 
val --- avg_loss: 1.7228, acc: 0.3636  

epoch: 35 avg_loss: 1.3025, acc: 0.4749 
val --- avg_loss: 1.7268, acc: 0.3589  

epoch: 36 avg_loss: 1.3058, acc: 0.4743 
val --- avg_loss: 1.7321, acc: 0.3636  

epoch: 37 avg_loss: 1.2975, acc: 0.4758 
val --- avg_loss: 1.7185, acc: 0.3684  

epoch: 38 avg_loss: 1.2981, acc: 0.4772 
val --- avg_loss: 1.7206, acc: 0.3445  

epoch: 39 avg_loss: 1.2970, acc: 0.4761 
val --- avg_loss: 1.7302, acc: 0.3589  

epoch: 40 avg_loss: 1.2954, acc: 0.4795 
val --- avg_loss: 1.7377, acc: 0.3589  

epoch: 41 avg_loss: 1.2973, acc: 0.4732 
val --- avg_loss: 1.7120, acc: 0.3636  

epoch: 42 avg_loss: 1.3029, acc: 0.4746 
val --- avg_loss: 1.7125, acc: 0.3493  

epoch: 43 avg_loss: 1.3010, acc: 0.4689 
val --- avg_loss: 1.7140, acc: 0.3541  

epoch: 44 avg_loss: 1.2974, acc: 0.4824 
val --- avg_loss: 1.7227, acc: 0.3684  

epoch: 45 avg_loss: 1.2941, acc: 0.4766 
val --- avg_loss: 1.7447, acc: 0.3636  

epoch: 46 avg_loss: 1.2932, acc: 0.4732 
val --- avg_loss: 1.7353, acc: 0.3589  

epoch: 47 avg_loss: 1.2909, acc: 0.4752 
val --- avg_loss: 1.7268, acc: 0.3732  

epoch: 48 avg_loss: 1.2917, acc: 0.4806 
val --- avg_loss: 1.7487, acc: 0.3684  

epoch: 49 avg_loss: 1.2918, acc: 0.4752 
val --- avg_loss: 1.7446, acc: 0.3589  

epoch: 50 avg_loss: 1.2881, acc: 0.4789 
val --- avg_loss: 1.7416, acc: 0.3541  

epoch: 51 avg_loss: 1.2924, acc: 0.4761 
val --- avg_loss: 1.7524, acc: 0.3684  

epoch: 52 avg_loss: 1.2875, acc: 0.4847 
val --- avg_loss: 1.7473, acc: 0.3541  

epoch: 53 avg_loss: 1.2850, acc: 0.4746 
val --- avg_loss: 1.7310, acc: 0.3589  

epoch: 54 avg_loss: 1.2876, acc: 0.4763 
val --- avg_loss: 1.7426, acc: 0.3541  

epoch: 55 avg_loss: 1.2867, acc: 0.4806 
val --- avg_loss: 1.7545, acc: 0.3589  

epoch: 56 avg_loss: 1.2840, acc: 0.4749 
val --- avg_loss: 1.7239, acc: 0.3397  

epoch: 57 avg_loss: 1.2886, acc: 0.4718 
val --- avg_loss: 1.7353, acc: 0.3636  

epoch: 58 avg_loss: 1.2882, acc: 0.4786 
val --- avg_loss: 1.7476, acc: 0.3636  

epoch: 59 avg_loss: 1.2889, acc: 0.4763 
val --- avg_loss: 1.7389, acc: 0.3684  

epoch: 60 avg_loss: 1.2851, acc: 0.4798 
val --- avg_loss: 1.7460, acc: 0.3636  

epoch: 61 avg_loss: 1.2845, acc: 0.4766 
val --- avg_loss: 1.7461, acc: 0.3541  

epoch: 62 avg_loss: 1.2797, acc: 0.4743 
val --- avg_loss: 1.7504, acc: 0.3732  

epoch: 63 avg_loss: 1.2828, acc: 0.4795 
val --- avg_loss: 1.7429, acc: 0.3684  

epoch: 64 avg_loss: 1.2793, acc: 0.4755 
val --- avg_loss: 1.7544, acc: 0.3589  

epoch: 65 avg_loss: 1.2806, acc: 0.4804 
val --- avg_loss: 1.7527, acc: 0.3636  

epoch: 66 avg_loss: 1.2813, acc: 0.4812 
val --- avg_loss: 1.7474, acc: 0.3541  

epoch: 67 avg_loss: 1.2817, acc: 0.4761 
val --- avg_loss: 1.7588, acc: 0.3493  

epoch: 68 avg_loss: 1.2800, acc: 0.4786 
val --- avg_loss: 1.7333, acc: 0.3589  

epoch: 69 avg_loss: 1.2790, acc: 0.4720 
val --- avg_loss: 1.7594, acc: 0.3589  

epoch: 70 avg_loss: 1.2807, acc: 0.4798 
val --- avg_loss: 1.7677, acc: 0.3541  

epoch: 71 avg_loss: 1.2797, acc: 0.4746 
val --- avg_loss: 1.7686, acc: 0.3589  

epoch: 72 avg_loss: 1.2774, acc: 0.4772 
val --- avg_loss: 1.7747, acc: 0.3589  

epoch: 73 avg_loss: 1.2805, acc: 0.4789 
val --- avg_loss: 1.7639, acc: 0.3493  

epoch: 74 avg_loss: 1.2824, acc: 0.4841 
val --- avg_loss: 1.7493, acc: 0.3589  

epoch: 75 avg_loss: 1.2814, acc: 0.4763 
val --- avg_loss: 1.7561, acc: 0.3541  

epoch: 76 avg_loss: 1.2751, acc: 0.4749 
val --- avg_loss: 1.7610, acc: 0.3636  

epoch: 77 avg_loss: 1.2788, acc: 0.4732 
val --- avg_loss: 1.7494, acc: 0.3589  

epoch: 78 avg_loss: 1.2744, acc: 0.4700 
val --- avg_loss: 1.7756, acc: 0.3684  

epoch: 79 avg_loss: 1.2717, acc: 0.4890 
val --- avg_loss: 1.7557, acc: 0.3541  

epoch: 80 avg_loss: 1.2752, acc: 0.4709 
val --- avg_loss: 1.7604, acc: 0.3732  

epoch: 81 avg_loss: 1.2726, acc: 0.4818 
val --- avg_loss: 1.7511, acc: 0.3589  

epoch: 82 avg_loss: 1.2711, acc: 0.4769 
val --- avg_loss: 1.7498, acc: 0.3541  

epoch: 83 avg_loss: 1.2755, acc: 0.4740 
val --- avg_loss: 1.7644, acc: 0.3732  

epoch: 84 avg_loss: 1.2786, acc: 0.4775 
val --- avg_loss: 1.7666, acc: 0.3732  

epoch: 85 avg_loss: 1.2760, acc: 0.4815 
val --- avg_loss: 1.7589, acc: 0.3541  

epoch: 86 avg_loss: 1.2748, acc: 0.4772 
val --- avg_loss: 1.7634, acc: 0.3493  

epoch: 87 avg_loss: 1.2716, acc: 0.4798 
val --- avg_loss: 1.7707, acc: 0.3636  

epoch: 88 avg_loss: 1.2711, acc: 0.4761 
val --- avg_loss: 1.7595, acc: 0.3541  

epoch: 89 avg_loss: 1.2715, acc: 0.4801 
val --- avg_loss: 1.7548, acc: 0.3541  

epoch: 90 avg_loss: 1.2774, acc: 0.4766 
val --- avg_loss: 1.7433, acc: 0.3636  

epoch: 91 avg_loss: 1.2636, acc: 0.4775 
val --- avg_loss: 1.7935, acc: 0.3684  

epoch: 92 avg_loss: 1.2771, acc: 0.4832 
val --- avg_loss: 1.7772, acc: 0.3541  

epoch: 93 avg_loss: 1.2689, acc: 0.4801 
val --- avg_loss: 1.7590, acc: 0.3636  

epoch: 94 avg_loss: 1.2695, acc: 0.4743 
val --- avg_loss: 1.7685, acc: 0.3589  

epoch: 95 avg_loss: 1.2687, acc: 0.4832 
val --- avg_loss: 1.7539, acc: 0.3541  

epoch: 96 avg_loss: 1.2682, acc: 0.4729 
val --- avg_loss: 1.7994, acc: 0.3684  

epoch: 97 avg_loss: 1.2731, acc: 0.4752 
val --- avg_loss: 1.7626, acc: 0.3541  

epoch: 98 avg_loss: 1.2842, acc: 0.4752 
val --- avg_loss: 1.7516, acc: 0.3541  

epoch: 99 avg_loss: 1.2800, acc: 0.4649 
val --- avg_loss: 1.7569, acc: 0.3684  

epoch: 100 avg_loss: 1.2694, acc: 0.4792 
val --- avg_loss: 1.7638, acc: 0.3636  

epoch: 101 avg_loss: 1.2679, acc: 0.4781 
val --- avg_loss: 1.7492, acc: 0.3636  

epoch: 102 avg_loss: 1.2644, acc: 0.4818 
val --- avg_loss: 1.7615, acc: 0.3828  

epoch: 103 avg_loss: 1.2711, acc: 0.4786 
val --- avg_loss: 1.7626, acc: 0.3684  

epoch: 104 avg_loss: 1.2652, acc: 0.4824 
val --- avg_loss: 1.7704, acc: 0.3636  

epoch: 105 avg_loss: 1.2691, acc: 0.4795 
val --- avg_loss: 1.7595, acc: 0.3541  

epoch: 106 avg_loss: 1.2686, acc: 0.4824 
val --- avg_loss: 1.7602, acc: 0.3732  

epoch: 107 avg_loss: 1.2648, acc: 0.4738 
val --- avg_loss: 1.7780, acc: 0.3636  

epoch: 108 avg_loss: 1.2645, acc: 0.4809 
val --- avg_loss: 1.7865, acc: 0.3541  

epoch: 109 avg_loss: 1.2642, acc: 0.4766 
val --- avg_loss: 1.7919, acc: 0.3636  

epoch: 110 avg_loss: 1.2681, acc: 0.4772 
val --- avg_loss: 1.7989, acc: 0.3732  

epoch: 111 avg_loss: 1.2675, acc: 0.4763 
val --- avg_loss: 1.7667, acc: 0.3732  

epoch: 112 avg_loss: 1.2655, acc: 0.4735 
val --- avg_loss: 1.7766, acc: 0.3541  

epoch: 113 avg_loss: 1.2608, acc: 0.4844 
val --- avg_loss: 1.7589, acc: 0.3541  

epoch: 114 avg_loss: 1.2653, acc: 0.4809 
val --- avg_loss: 1.7559, acc: 0.3636  

epoch: 115 avg_loss: 1.2690, acc: 0.4763 
val --- avg_loss: 1.7723, acc: 0.3397  

epoch: 116 avg_loss: 1.2619, acc: 0.4732 
val --- avg_loss: 1.7737, acc: 0.3636  

epoch: 117 avg_loss: 1.2685, acc: 0.4815 
val --- avg_loss: 1.7846, acc: 0.3732  

epoch: 118 avg_loss: 1.2603, acc: 0.4864 
val --- avg_loss: 1.7649, acc: 0.3541  

epoch: 119 avg_loss: 1.2663, acc: 0.4755 
val --- avg_loss: 1.7811, acc: 0.3636  

epoch: 120 avg_loss: 1.2630, acc: 0.4769 
val --- avg_loss: 1.7553, acc: 0.3589  

epoch: 121 avg_loss: 1.2637, acc: 0.4783 
val --- avg_loss: 1.7849, acc: 0.3636  

epoch: 122 avg_loss: 1.2623, acc: 0.4835 
val --- avg_loss: 1.7933, acc: 0.3541  

epoch: 123 avg_loss: 1.2633, acc: 0.4792 
val --- avg_loss: 1.7682, acc: 0.3493  

epoch: 124 avg_loss: 1.2667, acc: 0.4786 
val --- avg_loss: 1.7717, acc: 0.3636  

epoch: 125 avg_loss: 1.2585, acc: 0.4821 
val --- avg_loss: 1.8020, acc: 0.3541  

epoch: 126 avg_loss: 1.2605, acc: 0.4786 
val --- avg_loss: 1.7743, acc: 0.3684  

epoch: 127 avg_loss: 1.2663, acc: 0.4804 
val --- avg_loss: 1.7829, acc: 0.3636  

epoch: 128 avg_loss: 1.2595, acc: 0.4798 
val --- avg_loss: 1.7728, acc: 0.3589  

epoch: 129 avg_loss: 1.2655, acc: 0.4755 
val --- avg_loss: 1.7725, acc: 0.3589  

epoch: 130 avg_loss: 1.2602, acc: 0.4864 
val --- avg_loss: 1.7776, acc: 0.3541  

epoch: 131 avg_loss: 1.2594, acc: 0.4775 
val --- avg_loss: 1.7584, acc: 0.3445  

epoch: 132 avg_loss: 1.2604, acc: 0.4792 
val --- avg_loss: 1.7936, acc: 0.3493  

epoch: 133 avg_loss: 1.2636, acc: 0.4726 
val --- avg_loss: 1.7910, acc: 0.3636  

epoch: 134 avg_loss: 1.2597, acc: 0.4809 
val --- avg_loss: 1.7709, acc: 0.3445  

epoch: 135 avg_loss: 1.2601, acc: 0.4806 
val --- avg_loss: 1.7875, acc: 0.3684  

epoch: 136 avg_loss: 1.2611, acc: 0.4769 
val --- avg_loss: 1.7865, acc: 0.3636  

epoch: 137 avg_loss: 1.2631, acc: 0.4832 
val --- avg_loss: 1.7717, acc: 0.3828  

epoch: 138 avg_loss: 1.2596, acc: 0.4706 
val --- avg_loss: 1.7703, acc: 0.3636  

epoch: 139 avg_loss: 1.2608, acc: 0.4826 
val --- avg_loss: 1.7707, acc: 0.3493  

epoch: 140 avg_loss: 1.2583, acc: 0.4735 
val --- avg_loss: 1.7901, acc: 0.3397  

epoch: 141 avg_loss: 1.2576, acc: 0.4832 
val --- avg_loss: 1.7983, acc: 0.3636  

epoch: 142 avg_loss: 1.2563, acc: 0.4795 
val --- avg_loss: 1.7821, acc: 0.3445  

epoch: 143 avg_loss: 1.2551, acc: 0.4829 
val --- avg_loss: 1.7773, acc: 0.3589  

epoch: 144 avg_loss: 1.2635, acc: 0.4821 
val --- avg_loss: 1.7716, acc: 0.3493  

epoch: 145 avg_loss: 1.2568, acc: 0.4815 
val --- avg_loss: 1.7986, acc: 0.3636  

epoch: 146 avg_loss: 1.2587, acc: 0.4763 
val --- avg_loss: 1.8097, acc: 0.3780  

epoch: 147 avg_loss: 1.2550, acc: 0.4781 
val --- avg_loss: 1.7884, acc: 0.3636  

epoch: 148 avg_loss: 1.2634, acc: 0.4861 
val --- avg_loss: 1.7661, acc: 0.3589  

epoch: 149 avg_loss: 1.2520, acc: 0.4726 
val --- avg_loss: 1.8027, acc: 0.3636  

epoch: 150 avg_loss: 1.2548, acc: 0.4775 
val --- avg_loss: 1.7818, acc: 0.3780  

epoch: 151 avg_loss: 1.2580, acc: 0.4812 
val --- avg_loss: 1.7774, acc: 0.3636  

epoch: 152 avg_loss: 1.2531, acc: 0.4867 
val --- avg_loss: 1.7718, acc: 0.3732  

epoch: 153 avg_loss: 1.2595, acc: 0.4792 
val --- avg_loss: 1.7926, acc: 0.3732  

epoch: 154 avg_loss: 1.2565, acc: 0.4735 
val --- avg_loss: 1.7767, acc: 0.3589  

epoch: 155 avg_loss: 1.2563, acc: 0.4844 
val --- avg_loss: 1.7684, acc: 0.3493  

epoch: 156 avg_loss: 1.2570, acc: 0.4758 
val --- avg_loss: 1.7937, acc: 0.3732  

epoch: 157 avg_loss: 1.2528, acc: 0.4855 
val --- avg_loss: 1.7904, acc: 0.3636  

epoch: 158 avg_loss: 1.2580, acc: 0.4852 
val --- avg_loss: 1.8002, acc: 0.3493  

epoch: 159 avg_loss: 1.2553, acc: 0.4801 
val --- avg_loss: 1.7738, acc: 0.3589  
