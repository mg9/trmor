
number of params: 3584 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=300, fig_path='evaluation/probing/person/results/training/mvqvae_101_probe/3487_instances/32dim/300epochs.png', log_path='evaluation/probing/person/results/training/mvqvae_101_probe/3487_instances/32dim/300epochs.log', logger=<common.utils.Logger object at 0x7f7c0740d9d0>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='mvqvae_101_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (linear_9): Linear(in_features=512, out_features=512, bias=True)
  (linear_10): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_10): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (linear): Linear(in_features=512, out_features=7, bias=False)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/person/results/training/mvqvae_101_probe/3487_instances/32dim/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (linear_9): Linear(in_features=512, out_features=512, bias=True)
  (linear_10): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_10): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/person/results/training/mvqvae_101_probe/3487_instances/32dim/300epochs.pt', seq_to_no_pad='surface', task='surf2person', trndata='evaluation/probing/person/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/person/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/person/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([512, 512]): False
linear_2.bias, torch.Size([512]): False
linear_3.weight, torch.Size([512, 512]): False
linear_3.bias, torch.Size([512]): False
linear_4.weight, torch.Size([512, 512]): False
linear_4.bias, torch.Size([512]): False
linear_5.weight, torch.Size([512, 512]): False
linear_5.bias, torch.Size([512]): False
linear_6.weight, torch.Size([512, 512]): False
linear_6.bias, torch.Size([512]): False
linear_7.weight, torch.Size([512, 512]): False
linear_7.bias, torch.Size([512]): False
linear_8.weight, torch.Size([512, 512]): False
linear_8.bias, torch.Size([512]): False
linear_9.weight, torch.Size([512, 512]): False
linear_9.bias, torch.Size([512]): False
linear_10.weight, torch.Size([512, 512]): False
linear_10.bias, torch.Size([512]): False
vq_layer.embedding.weight, torch.Size([704, 512]): False
vq_layer_2.embedding.weight, torch.Size([16, 512]): False
vq_layer_3.embedding.weight, torch.Size([16, 512]): False
vq_layer_4.embedding.weight, torch.Size([16, 512]): False
vq_layer_5.embedding.weight, torch.Size([16, 512]): False
vq_layer_6.embedding.weight, torch.Size([16, 512]): False
vq_layer_7.embedding.weight, torch.Size([16, 512]): False
vq_layer_8.embedding.weight, torch.Size([16, 512]): False
vq_layer_9.embedding.weight, torch.Size([16, 512]): False
vq_layer_10.embedding.weight, torch.Size([16, 512]): False
linear.weight, torch.Size([7, 512]): True
epoch: 0 avg_loss: 1.8059, acc: 0.3163 

epoch: 1 avg_loss: 1.5545, acc: 0.4551 

epoch: 2 avg_loss: 1.5278, acc: 0.4583 

epoch: 3 avg_loss: 1.5161, acc: 0.4531 

epoch: 4 avg_loss: 1.5146, acc: 0.4439 

epoch: 5 avg_loss: 1.5052, acc: 0.4471 

epoch: 6 avg_loss: 1.5014, acc: 0.4477 

epoch: 7 avg_loss: 1.5093, acc: 0.4342 

epoch: 8 avg_loss: 1.5038, acc: 0.4683 

epoch: 9 avg_loss: 1.5027, acc: 0.4597 

epoch: 10 avg_loss: 1.5115, acc: 0.4368 

epoch: 11 avg_loss: 1.5188, acc: 0.4712 

epoch: 12 avg_loss: 1.5081, acc: 0.4259 

epoch: 13 avg_loss: 1.5167, acc: 0.4703 

epoch: 14 avg_loss: 1.5191, acc: 0.4365 

epoch: 15 avg_loss: 1.5082, acc: 0.4545 

epoch: 16 avg_loss: 1.5055, acc: 0.4350 

epoch: 17 avg_loss: 1.5224, acc: 0.4649 

epoch: 18 avg_loss: 1.4984, acc: 0.4290 

epoch: 19 avg_loss: 1.5074, acc: 0.4677 

epoch: 20 avg_loss: 1.4988, acc: 0.4299 

epoch: 21 avg_loss: 1.5247, acc: 0.4637 

epoch: 22 avg_loss: 1.5067, acc: 0.4505 

epoch: 23 avg_loss: 1.4980, acc: 0.4680 

epoch: 24 avg_loss: 1.5245, acc: 0.4500 

epoch: 25 avg_loss: 1.5069, acc: 0.4634 

epoch: 26 avg_loss: 1.5158, acc: 0.4287 

epoch: 27 avg_loss: 1.5117, acc: 0.4586 

epoch: 28 avg_loss: 1.4870, acc: 0.4477 

epoch: 29 avg_loss: 1.5040, acc: 0.4485 

epoch: 30 avg_loss: 1.5008, acc: 0.4454 

epoch: 31 avg_loss: 1.5157, acc: 0.4482 

epoch: 32 avg_loss: 1.5232, acc: 0.4428 

epoch: 33 avg_loss: 1.5094, acc: 0.4566 

epoch: 34 avg_loss: 1.5246, acc: 0.4224 

epoch: 35 avg_loss: 1.5166, acc: 0.4560 

epoch: 36 avg_loss: 1.5164, acc: 0.4411 

epoch: 37 avg_loss: 1.5312, acc: 0.4560 

epoch: 38 avg_loss: 1.5128, acc: 0.4500 

epoch: 39 avg_loss: 1.5239, acc: 0.4419 

epoch: 40 avg_loss: 1.5127, acc: 0.4657 

epoch: 41 avg_loss: 1.5244, acc: 0.4402 

epoch: 42 avg_loss: 1.5051, acc: 0.4290 

epoch: 43 avg_loss: 1.5317, acc: 0.4654 

epoch: 44 avg_loss: 1.5146, acc: 0.4497 

epoch: 45 avg_loss: 1.5093, acc: 0.4577 

epoch: 46 avg_loss: 1.5095, acc: 0.4560 

epoch: 47 avg_loss: 1.5281, acc: 0.4132 

epoch: 48 avg_loss: 1.5117, acc: 0.4563 

epoch: 49 avg_loss: 1.4912, acc: 0.4574 

epoch: 50 avg_loss: 1.4937, acc: 0.4442 

epoch: 51 avg_loss: 1.5198, acc: 0.4382 

epoch: 52 avg_loss: 1.4956, acc: 0.4511 

epoch: 53 avg_loss: 1.5060, acc: 0.4703 

epoch: 54 avg_loss: 1.5010, acc: 0.4511 

epoch: 55 avg_loss: 1.5140, acc: 0.4468 

epoch: 56 avg_loss: 1.5098, acc: 0.4362 

epoch: 57 avg_loss: 1.5092, acc: 0.4603 

epoch: 58 avg_loss: 1.5124, acc: 0.4626 

epoch: 59 avg_loss: 1.5078, acc: 0.4505 

epoch: 60 avg_loss: 1.5244, acc: 0.4485 

epoch: 61 avg_loss: 1.5189, acc: 0.4537 

epoch: 62 avg_loss: 1.5247, acc: 0.4689 

epoch: 63 avg_loss: 1.5178, acc: 0.4170 

epoch: 64 avg_loss: 1.5114, acc: 0.4488 

epoch: 65 avg_loss: 1.5027, acc: 0.4371 

epoch: 66 avg_loss: 1.5487, acc: 0.4531 

epoch: 67 avg_loss: 1.5083, acc: 0.4454 

epoch: 68 avg_loss: 1.5106, acc: 0.4488 

epoch: 69 avg_loss: 1.5092, acc: 0.4477 

epoch: 70 avg_loss: 1.5193, acc: 0.4548 

epoch: 71 avg_loss: 1.5157, acc: 0.4422 

epoch: 72 avg_loss: 1.5283, acc: 0.4508 

epoch: 73 avg_loss: 1.5219, acc: 0.4500 

epoch: 74 avg_loss: 1.5333, acc: 0.4422 

epoch: 75 avg_loss: 1.5190, acc: 0.4543 

epoch: 76 avg_loss: 1.5086, acc: 0.4626 

epoch: 77 avg_loss: 1.5101, acc: 0.4568 

epoch: 78 avg_loss: 1.5030, acc: 0.4176 

epoch: 79 avg_loss: 1.5267, acc: 0.4680 

epoch: 80 avg_loss: 1.5086, acc: 0.4439 

epoch: 81 avg_loss: 1.5021, acc: 0.4491 

epoch: 82 avg_loss: 1.5236, acc: 0.4451 

epoch: 83 avg_loss: 1.5078, acc: 0.4328 

epoch: 84 avg_loss: 1.5054, acc: 0.4479 

epoch: 85 avg_loss: 1.4995, acc: 0.4769 

epoch: 86 avg_loss: 1.4955, acc: 0.4514 

epoch: 87 avg_loss: 1.5083, acc: 0.4373 

epoch: 88 avg_loss: 1.5201, acc: 0.4617 

epoch: 89 avg_loss: 1.5070, acc: 0.4551 

epoch: 90 avg_loss: 1.5165, acc: 0.4391 

epoch: 91 avg_loss: 1.4999, acc: 0.4540 

epoch: 92 avg_loss: 1.5047, acc: 0.4574 

epoch: 93 avg_loss: 1.4902, acc: 0.4419 

epoch: 94 avg_loss: 1.5094, acc: 0.4669 

epoch: 95 avg_loss: 1.5116, acc: 0.4485 

epoch: 96 avg_loss: 1.5023, acc: 0.4672 

epoch: 97 avg_loss: 1.5117, acc: 0.4482 

epoch: 98 avg_loss: 1.5254, acc: 0.4393 

epoch: 99 avg_loss: 1.5263, acc: 0.4454 

epoch: 100 avg_loss: 1.5091, acc: 0.4689 

epoch: 101 avg_loss: 1.5124, acc: 0.4505 

epoch: 102 avg_loss: 1.4955, acc: 0.4594 

epoch: 103 avg_loss: 1.5112, acc: 0.4482 

epoch: 104 avg_loss: 1.5155, acc: 0.4649 

epoch: 105 avg_loss: 1.5066, acc: 0.4457 

epoch: 106 avg_loss: 1.5174, acc: 0.4462 

epoch: 107 avg_loss: 1.5037, acc: 0.4502 

epoch: 108 avg_loss: 1.4994, acc: 0.4477 

epoch: 109 avg_loss: 1.5121, acc: 0.4746 

epoch: 110 avg_loss: 1.5300, acc: 0.4055 

epoch: 111 avg_loss: 1.5418, acc: 0.4643 

epoch: 112 avg_loss: 1.5232, acc: 0.4517 

epoch: 113 avg_loss: 1.5098, acc: 0.4457 

epoch: 114 avg_loss: 1.5006, acc: 0.4253 

epoch: 115 avg_loss: 1.5044, acc: 0.4571 

epoch: 116 avg_loss: 1.5213, acc: 0.4577 

epoch: 117 avg_loss: 1.5206, acc: 0.4247 

epoch: 118 avg_loss: 1.5133, acc: 0.4560 

epoch: 119 avg_loss: 1.5024, acc: 0.4479 

epoch: 120 avg_loss: 1.5102, acc: 0.4477 

epoch: 121 avg_loss: 1.5091, acc: 0.4603 

epoch: 122 avg_loss: 1.5149, acc: 0.4451 

epoch: 123 avg_loss: 1.5198, acc: 0.4505 

epoch: 124 avg_loss: 1.4964, acc: 0.4428 

epoch: 125 avg_loss: 1.5285, acc: 0.4732 

epoch: 126 avg_loss: 1.5246, acc: 0.4092 

epoch: 127 avg_loss: 1.5221, acc: 0.4606 

epoch: 128 avg_loss: 1.5274, acc: 0.4508 

epoch: 129 avg_loss: 1.5104, acc: 0.4591 

epoch: 130 avg_loss: 1.5065, acc: 0.4305 

epoch: 131 avg_loss: 1.5252, acc: 0.4586 

epoch: 132 avg_loss: 1.5054, acc: 0.4436 

epoch: 133 avg_loss: 1.4937, acc: 0.4554 

epoch: 134 avg_loss: 1.5007, acc: 0.4373 

epoch: 135 avg_loss: 1.5133, acc: 0.4660 

epoch: 136 avg_loss: 1.5150, acc: 0.4362 

epoch: 137 avg_loss: 1.5161, acc: 0.4554 

epoch: 138 avg_loss: 1.4914, acc: 0.4474 

epoch: 139 avg_loss: 1.5140, acc: 0.4675 

epoch: 140 avg_loss: 1.5404, acc: 0.4089 

epoch: 141 avg_loss: 1.5051, acc: 0.4669 

epoch: 142 avg_loss: 1.5101, acc: 0.4373 

epoch: 143 avg_loss: 1.4966, acc: 0.4393 

epoch: 144 avg_loss: 1.5132, acc: 0.4428 

epoch: 145 avg_loss: 1.5073, acc: 0.4649 

epoch: 146 avg_loss: 1.5298, acc: 0.4402 

epoch: 147 avg_loss: 1.5032, acc: 0.4566 

epoch: 148 avg_loss: 1.5223, acc: 0.4462 

epoch: 149 avg_loss: 1.5154, acc: 0.4391 

epoch: 150 avg_loss: 1.4950, acc: 0.4640 

epoch: 151 avg_loss: 1.4977, acc: 0.4517 

epoch: 152 avg_loss: 1.5089, acc: 0.4328 

epoch: 153 avg_loss: 1.5170, acc: 0.4514 

epoch: 154 avg_loss: 1.5013, acc: 0.4692 

epoch: 155 avg_loss: 1.5036, acc: 0.4391 

epoch: 156 avg_loss: 1.5104, acc: 0.4689 

epoch: 157 avg_loss: 1.5162, acc: 0.4465 

epoch: 158 avg_loss: 1.5107, acc: 0.4293 

epoch: 159 avg_loss: 1.5144, acc: 0.4531 

epoch: 160 avg_loss: 1.5106, acc: 0.4511 

epoch: 161 avg_loss: 1.5156, acc: 0.4365 

epoch: 162 avg_loss: 1.5192, acc: 0.4583 

epoch: 163 avg_loss: 1.5140, acc: 0.4583 

epoch: 164 avg_loss: 1.5054, acc: 0.4505 

epoch: 165 avg_loss: 1.5124, acc: 0.4700 

epoch: 166 avg_loss: 1.5008, acc: 0.4405 

epoch: 167 avg_loss: 1.5160, acc: 0.4631 

epoch: 168 avg_loss: 1.5174, acc: 0.4434 

epoch: 169 avg_loss: 1.5085, acc: 0.4373 

epoch: 170 avg_loss: 1.5097, acc: 0.4643 

epoch: 171 avg_loss: 1.5125, acc: 0.4405 

epoch: 172 avg_loss: 1.5299, acc: 0.4293 

epoch: 173 avg_loss: 1.5051, acc: 0.4434 

epoch: 174 avg_loss: 1.5292, acc: 0.4540 

epoch: 175 avg_loss: 1.5341, acc: 0.4505 

epoch: 176 avg_loss: 1.5165, acc: 0.4276 

epoch: 177 avg_loss: 1.5068, acc: 0.4491 

epoch: 178 avg_loss: 1.5211, acc: 0.4408 

epoch: 179 avg_loss: 1.5341, acc: 0.4388 

epoch: 180 avg_loss: 1.5028, acc: 0.4445 

epoch: 181 avg_loss: 1.5220, acc: 0.4563 

epoch: 182 avg_loss: 1.5033, acc: 0.4597 

epoch: 183 avg_loss: 1.5095, acc: 0.4368 

epoch: 184 avg_loss: 1.5094, acc: 0.4566 

epoch: 185 avg_loss: 1.5175, acc: 0.4511 

epoch: 186 avg_loss: 1.5127, acc: 0.4649 

epoch: 187 avg_loss: 1.5231, acc: 0.4296 

epoch: 188 avg_loss: 1.5150, acc: 0.4448 

epoch: 189 avg_loss: 1.5234, acc: 0.4606 

epoch: 190 avg_loss: 1.5275, acc: 0.4566 

epoch: 191 avg_loss: 1.5002, acc: 0.4479 

epoch: 192 avg_loss: 1.5108, acc: 0.4775 

epoch: 193 avg_loss: 1.4954, acc: 0.4462 

epoch: 194 avg_loss: 1.5287, acc: 0.4227 

epoch: 195 avg_loss: 1.5081, acc: 0.4471 

epoch: 196 avg_loss: 1.5092, acc: 0.4465 

epoch: 197 avg_loss: 1.5136, acc: 0.4571 

epoch: 198 avg_loss: 1.5178, acc: 0.4497 

epoch: 199 avg_loss: 1.5096, acc: 0.4528 

epoch: 200 avg_loss: 1.5399, acc: 0.4485 

epoch: 201 avg_loss: 1.4911, acc: 0.4471 

epoch: 202 avg_loss: 1.5058, acc: 0.4132 

epoch: 203 avg_loss: 1.5379, acc: 0.4672 

epoch: 204 avg_loss: 1.5074, acc: 0.4666 

epoch: 205 avg_loss: 1.5151, acc: 0.4649 

epoch: 206 avg_loss: 1.5386, acc: 0.4210 

epoch: 207 avg_loss: 1.5536, acc: 0.4345 

epoch: 208 avg_loss: 1.5164, acc: 0.4305 

epoch: 209 avg_loss: 1.5182, acc: 0.4603 

epoch: 210 avg_loss: 1.5098, acc: 0.4348 

epoch: 211 avg_loss: 1.5156, acc: 0.4649 

epoch: 212 avg_loss: 1.5079, acc: 0.4471 

epoch: 213 avg_loss: 1.5130, acc: 0.4761 

epoch: 214 avg_loss: 1.5134, acc: 0.4221 

epoch: 215 avg_loss: 1.5197, acc: 0.4445 

epoch: 216 avg_loss: 1.5000, acc: 0.4614 

epoch: 217 avg_loss: 1.5159, acc: 0.4614 

epoch: 218 avg_loss: 1.5278, acc: 0.4471 

epoch: 219 avg_loss: 1.5056, acc: 0.4250 

epoch: 220 avg_loss: 1.5180, acc: 0.4485 

epoch: 221 avg_loss: 1.5149, acc: 0.4376 

epoch: 222 avg_loss: 1.5174, acc: 0.4525 

epoch: 223 avg_loss: 1.5020, acc: 0.4333 

epoch: 224 avg_loss: 1.5217, acc: 0.4672 

epoch: 225 avg_loss: 1.5182, acc: 0.4402 

epoch: 226 avg_loss: 1.4989, acc: 0.4376 

epoch: 227 avg_loss: 1.5184, acc: 0.4657 

epoch: 228 avg_loss: 1.5137, acc: 0.4500 

epoch: 229 avg_loss: 1.5144, acc: 0.4479 

epoch: 230 avg_loss: 1.5210, acc: 0.4416 

epoch: 231 avg_loss: 1.5146, acc: 0.4488 

epoch: 232 avg_loss: 1.5157, acc: 0.4654 

epoch: 233 avg_loss: 1.5071, acc: 0.4273 

epoch: 234 avg_loss: 1.5075, acc: 0.4497 

epoch: 235 avg_loss: 1.5025, acc: 0.4554 

epoch: 236 avg_loss: 1.5038, acc: 0.4500 

epoch: 237 avg_loss: 1.5251, acc: 0.4362 

epoch: 238 avg_loss: 1.5203, acc: 0.4652 

epoch: 239 avg_loss: 1.5146, acc: 0.4514 

epoch: 240 avg_loss: 1.5077, acc: 0.4379 

epoch: 241 avg_loss: 1.5311, acc: 0.4293 

epoch: 242 avg_loss: 1.5025, acc: 0.4502 

epoch: 243 avg_loss: 1.5227, acc: 0.4442 

epoch: 244 avg_loss: 1.5094, acc: 0.4554 

epoch: 245 avg_loss: 1.5206, acc: 0.4643 

epoch: 246 avg_loss: 1.5051, acc: 0.4428 

epoch: 247 avg_loss: 1.5274, acc: 0.4296 

epoch: 248 avg_loss: 1.5113, acc: 0.4557 

epoch: 249 avg_loss: 1.5033, acc: 0.4545 

epoch: 250 avg_loss: 1.5158, acc: 0.4594 

epoch: 251 avg_loss: 1.4996, acc: 0.4422 

epoch: 252 avg_loss: 1.5211, acc: 0.4505 

epoch: 253 avg_loss: 1.5012, acc: 0.4442 

epoch: 254 avg_loss: 1.5155, acc: 0.4525 

epoch: 255 avg_loss: 1.4993, acc: 0.4594 

epoch: 256 avg_loss: 1.5234, acc: 0.4339 

epoch: 257 avg_loss: 1.4952, acc: 0.4376 

epoch: 258 avg_loss: 1.5161, acc: 0.4657 

epoch: 259 avg_loss: 1.5207, acc: 0.4525 

epoch: 260 avg_loss: 1.5067, acc: 0.4402 

epoch: 261 avg_loss: 1.5126, acc: 0.4333 

epoch: 262 avg_loss: 1.5122, acc: 0.4591 

epoch: 263 avg_loss: 1.5014, acc: 0.4548 

epoch: 264 avg_loss: 1.5152, acc: 0.4462 

epoch: 265 avg_loss: 1.5039, acc: 0.4385 

epoch: 266 avg_loss: 1.5235, acc: 0.4479 

epoch: 267 avg_loss: 1.5147, acc: 0.4683 

epoch: 268 avg_loss: 1.5215, acc: 0.3963 

epoch: 269 avg_loss: 1.5067, acc: 0.4591 

epoch: 270 avg_loss: 1.5123, acc: 0.4379 

epoch: 271 avg_loss: 1.5204, acc: 0.4531 

epoch: 272 avg_loss: 1.4989, acc: 0.4462 

epoch: 273 avg_loss: 1.5286, acc: 0.4505 

epoch: 274 avg_loss: 1.5018, acc: 0.4620 

epoch: 275 avg_loss: 1.5091, acc: 0.4302 

epoch: 276 avg_loss: 1.5359, acc: 0.4531 

epoch: 277 avg_loss: 1.5132, acc: 0.4282 

epoch: 278 avg_loss: 1.5238, acc: 0.4514 

epoch: 279 avg_loss: 1.5140, acc: 0.4706 

epoch: 280 avg_loss: 1.5008, acc: 0.4459 

epoch: 281 avg_loss: 1.4938, acc: 0.4267 

epoch: 282 avg_loss: 1.5341, acc: 0.4459 

epoch: 283 avg_loss: 1.5005, acc: 0.4244 

epoch: 284 avg_loss: 1.5069, acc: 0.4723 

epoch: 285 avg_loss: 1.5071, acc: 0.4586 

epoch: 286 avg_loss: 1.5107, acc: 0.4465 

epoch: 287 avg_loss: 1.4953, acc: 0.4500 

epoch: 288 avg_loss: 1.5144, acc: 0.4649 

epoch: 289 avg_loss: 1.5034, acc: 0.4253 

epoch: 290 avg_loss: 1.5010, acc: 0.4508 

epoch: 291 avg_loss: 1.5283, acc: 0.4482 

epoch: 292 avg_loss: 1.5210, acc: 0.4640 

epoch: 293 avg_loss: 1.5069, acc: 0.4399 

epoch: 294 avg_loss: 1.5061, acc: 0.4462 

epoch: 295 avg_loss: 1.5049, acc: 0.4279 

epoch: 296 avg_loss: 1.5131, acc: 0.4586 

epoch: 297 avg_loss: 1.5168, acc: 0.4631 

epoch: 298 avg_loss: 1.5054, acc: 0.4448 

epoch: 299 avg_loss: 1.5291, acc: 0.4497 
