
number of params: 99 
Namespace(batchsize=512, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=100, fig_path='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/100epochs.png', log_path='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/100epochs.log', logger=<common.utils.Logger object at 0x7f18738b7350>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_7_1_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (linear): Linear(in_features=32, out_features=3, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/', nh=512, ni=256, num_dicts=7, nz=512, opt='Adam', orddict_emb_num=100, pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), rootdict_emb_dim=320, rootdict_emb_num=1000, save_path='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/100epochs.pt', seq_to_no_pad='surface', task='surf2polar', trndata='evaluation/probing/polarity/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/polarity/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/polarity/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_root.weight, torch.Size([320, 512]): False
linear_root.bias, torch.Size([320]): False
vq_layer_root.embedding.weight, torch.Size([1000, 320]): False
ord_linears.0.weight, torch.Size([32, 512]): False
ord_linears.0.bias, torch.Size([32]): False
ord_linears.1.weight, torch.Size([32, 512]): False
ord_linears.1.bias, torch.Size([32]): False
ord_linears.2.weight, torch.Size([32, 512]): False
ord_linears.2.bias, torch.Size([32]): False
ord_linears.3.weight, torch.Size([32, 512]): False
ord_linears.3.bias, torch.Size([32]): False
ord_linears.4.weight, torch.Size([32, 512]): False
ord_linears.4.bias, torch.Size([32]): False
ord_linears.5.weight, torch.Size([32, 512]): False
ord_linears.5.bias, torch.Size([32]): False
ord_vq_layers.0.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.1.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.2.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.3.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.4.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.5.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([3, 32]): True
linear.bias, torch.Size([3]): True
epoch: 0 avg_loss: 0.7198, acc: 0.7907 
val --- avg_loss: 0.5071, acc: 0.7512  
update best loss 

epoch: 1 avg_loss: 0.4909, acc: 0.8271 
val --- avg_loss: 0.4680, acc: 0.7656  
update best loss 

epoch: 2 avg_loss: 0.4157, acc: 0.8297 
val --- avg_loss: 0.4392, acc: 0.7943  
update best loss 

epoch: 3 avg_loss: 0.3886, acc: 0.8305 
val --- avg_loss: 0.4343, acc: 0.7943  
update best loss 

epoch: 4 avg_loss: 0.3765, acc: 0.8345 
val --- avg_loss: 0.4343, acc: 0.7943  
update best loss 

epoch: 5 avg_loss: 0.3672, acc: 0.8345 
val --- avg_loss: 0.4319, acc: 0.7943  
update best loss 

epoch: 6 avg_loss: 0.3626, acc: 0.8340 
val --- avg_loss: 0.4301, acc: 0.7943  
update best loss 

epoch: 7 avg_loss: 0.3601, acc: 0.8385 
val --- avg_loss: 0.4252, acc: 0.7990  
update best loss 

epoch: 8 avg_loss: 0.3650, acc: 0.8388 
val --- avg_loss: 0.4271, acc: 0.7943  

epoch: 9 avg_loss: 0.3662, acc: 0.8388 
val --- avg_loss: 0.4270, acc: 0.7943  

epoch: 10 avg_loss: 0.3684, acc: 0.8460 
val --- avg_loss: 0.4264, acc: 0.7943  

epoch: 11 avg_loss: 0.3620, acc: 0.8351 
val --- avg_loss: 0.4279, acc: 0.7943  

epoch: 12 avg_loss: 0.3612, acc: 0.8451 
val --- avg_loss: 0.4302, acc: 0.7943  

epoch: 13 avg_loss: 0.3523, acc: 0.8383 
val --- avg_loss: 0.4225, acc: 0.7990  
update best loss 

epoch: 14 avg_loss: 0.3551, acc: 0.8477 
val --- avg_loss: 0.4169, acc: 0.7990  
update best loss 

epoch: 15 avg_loss: 0.3747, acc: 0.8362 
val --- avg_loss: 0.4348, acc: 0.7943  

epoch: 16 avg_loss: 0.3479, acc: 0.8474 
val --- avg_loss: 0.4175, acc: 0.7990  

epoch: 17 avg_loss: 0.3506, acc: 0.8477 
val --- avg_loss: 0.4217, acc: 0.7990  

epoch: 18 avg_loss: 0.3515, acc: 0.8308 
val --- avg_loss: 0.4253, acc: 0.7943  

epoch: 19 avg_loss: 0.3538, acc: 0.8477 
val --- avg_loss: 0.4215, acc: 0.7943  

epoch: 20 avg_loss: 0.3471, acc: 0.8420 
val --- avg_loss: 0.4267, acc: 0.7943  

epoch: 21 avg_loss: 0.3479, acc: 0.8380 
val --- avg_loss: 0.4234, acc: 0.7943  

epoch: 22 avg_loss: 0.3529, acc: 0.8397 
val --- avg_loss: 0.4233, acc: 0.7943  

epoch: 23 avg_loss: 0.3477, acc: 0.8408 
val --- avg_loss: 0.4195, acc: 0.7943  

epoch: 24 avg_loss: 0.3501, acc: 0.8477 
val --- avg_loss: 0.4145, acc: 0.7990  
update best loss 

epoch: 25 avg_loss: 0.3489, acc: 0.8377 
val --- avg_loss: 0.4333, acc: 0.7943  

epoch: 26 avg_loss: 0.3648, acc: 0.8446 
val --- avg_loss: 0.4120, acc: 0.7990  
update best loss 

epoch: 27 avg_loss: 0.3462, acc: 0.8426 
val --- avg_loss: 0.4209, acc: 0.7943  

epoch: 28 avg_loss: 0.3460, acc: 0.8383 
val --- avg_loss: 0.4194, acc: 0.7943  

epoch: 29 avg_loss: 0.3463, acc: 0.8380 
val --- avg_loss: 0.4195, acc: 0.7990  

epoch: 30 avg_loss: 0.3487, acc: 0.8420 
val --- avg_loss: 0.4174, acc: 0.7943  

epoch: 31 avg_loss: 0.3454, acc: 0.8388 
val --- avg_loss: 0.4129, acc: 0.7990  

epoch: 32 avg_loss: 0.3526, acc: 0.8471 
val --- avg_loss: 0.4105, acc: 0.7990  
update best loss 

epoch: 33 avg_loss: 0.3581, acc: 0.8285 
val --- avg_loss: 0.4322, acc: 0.7847  

epoch: 34 avg_loss: 0.3469, acc: 0.8431 
val --- avg_loss: 0.4175, acc: 0.7943  

epoch: 35 avg_loss: 0.3456, acc: 0.8420 
val --- avg_loss: 0.4124, acc: 0.7990  

epoch: 36 avg_loss: 0.3463, acc: 0.8388 
val --- avg_loss: 0.4152, acc: 0.7990  

epoch: 37 avg_loss: 0.3476, acc: 0.8469 
val --- avg_loss: 0.4087, acc: 0.7990  
update best loss 

epoch: 38 avg_loss: 0.3501, acc: 0.8394 
val --- avg_loss: 0.4121, acc: 0.7943  

epoch: 39 avg_loss: 0.3464, acc: 0.8374 
val --- avg_loss: 0.4241, acc: 0.7943  

epoch: 40 avg_loss: 0.3403, acc: 0.8420 
val --- avg_loss: 0.4119, acc: 0.7990  

epoch: 41 avg_loss: 0.3458, acc: 0.8428 
val --- avg_loss: 0.4194, acc: 0.7847  

epoch: 42 avg_loss: 0.3478, acc: 0.8377 
val --- avg_loss: 0.4110, acc: 0.7990  

epoch: 43 avg_loss: 0.3505, acc: 0.8471 
val --- avg_loss: 0.4118, acc: 0.7990  

epoch: 44 avg_loss: 0.3502, acc: 0.8383 
val --- avg_loss: 0.4120, acc: 0.7990  

epoch: 45 avg_loss: 0.3449, acc: 0.8423 
val --- avg_loss: 0.4143, acc: 0.7990  

epoch: 46 avg_loss: 0.3421, acc: 0.8400 
val --- avg_loss: 0.4286, acc: 0.7847  

epoch: 47 avg_loss: 0.3447, acc: 0.8385 
val --- avg_loss: 0.4200, acc: 0.7943  

epoch: 48 avg_loss: 0.3459, acc: 0.8380 
val --- avg_loss: 0.4152, acc: 0.7847  

epoch: 49 avg_loss: 0.3421, acc: 0.8388 
val --- avg_loss: 0.4243, acc: 0.7943  

epoch: 50 avg_loss: 0.3499, acc: 0.8400 
val --- avg_loss: 0.4152, acc: 0.7943  

epoch: 51 avg_loss: 0.3504, acc: 0.8411 
val --- avg_loss: 0.4125, acc: 0.7990  

epoch: 52 avg_loss: 0.3502, acc: 0.8414 
val --- avg_loss: 0.4212, acc: 0.7847  

epoch: 53 avg_loss: 0.3478, acc: 0.8406 
val --- avg_loss: 0.4119, acc: 0.7990  

epoch: 54 avg_loss: 0.3439, acc: 0.8385 
val --- avg_loss: 0.4247, acc: 0.7847  

epoch: 55 avg_loss: 0.3411, acc: 0.8380 
val --- avg_loss: 0.4152, acc: 0.7990  

epoch: 56 avg_loss: 0.3462, acc: 0.8434 
val --- avg_loss: 0.4182, acc: 0.7847  

epoch: 57 avg_loss: 0.3446, acc: 0.8423 
val --- avg_loss: 0.4149, acc: 0.7943  

epoch: 58 avg_loss: 0.3418, acc: 0.8385 
val --- avg_loss: 0.4116, acc: 0.7990  

epoch: 59 avg_loss: 0.3442, acc: 0.8411 
val --- avg_loss: 0.4121, acc: 0.7990  

epoch: 60 avg_loss: 0.3472, acc: 0.8469 
val --- avg_loss: 0.4122, acc: 0.7943  

epoch: 61 avg_loss: 0.3429, acc: 0.8391 
val --- avg_loss: 0.4185, acc: 0.7847  

epoch: 62 avg_loss: 0.3443, acc: 0.8403 
val --- avg_loss: 0.4213, acc: 0.7943  

epoch: 63 avg_loss: 0.3425, acc: 0.8371 
val --- avg_loss: 0.4170, acc: 0.7847  

epoch: 64 avg_loss: 0.3450, acc: 0.8428 
val --- avg_loss: 0.4213, acc: 0.7943  

epoch: 65 avg_loss: 0.3434, acc: 0.8388 
val --- avg_loss: 0.4197, acc: 0.7847  

epoch: 66 avg_loss: 0.3418, acc: 0.8394 
val --- avg_loss: 0.4161, acc: 0.7943  

epoch: 67 avg_loss: 0.3416, acc: 0.8397 
val --- avg_loss: 0.4137, acc: 0.7943  

epoch: 68 avg_loss: 0.3419, acc: 0.8377 
val --- avg_loss: 0.4150, acc: 0.7943  

epoch: 69 avg_loss: 0.3445, acc: 0.8437 
val --- avg_loss: 0.4199, acc: 0.7847  

epoch: 70 avg_loss: 0.3421, acc: 0.8403 
val --- avg_loss: 0.4189, acc: 0.7943  

epoch: 71 avg_loss: 0.3410, acc: 0.8437 
val --- avg_loss: 0.4167, acc: 0.7943  

epoch: 72 avg_loss: 0.3427, acc: 0.8411 
val --- avg_loss: 0.4154, acc: 0.7943  

epoch: 73 avg_loss: 0.3512, acc: 0.8391 
val --- avg_loss: 0.4144, acc: 0.7847  

epoch: 74 avg_loss: 0.3412, acc: 0.8403 
val --- avg_loss: 0.4186, acc: 0.7943  

epoch: 75 avg_loss: 0.3391, acc: 0.8400 
val --- avg_loss: 0.4085, acc: 0.7990  
update best loss 

epoch: 76 avg_loss: 0.3448, acc: 0.8383 
val --- avg_loss: 0.4142, acc: 0.7943  

epoch: 77 avg_loss: 0.3417, acc: 0.8391 
val --- avg_loss: 0.4108, acc: 0.7990  

epoch: 78 avg_loss: 0.3444, acc: 0.8454 
val --- avg_loss: 0.4142, acc: 0.7943  

epoch: 79 avg_loss: 0.3411, acc: 0.8408 
val --- avg_loss: 0.4235, acc: 0.7847  

epoch: 80 avg_loss: 0.3444, acc: 0.8471 
val --- avg_loss: 0.4166, acc: 0.7943  

epoch: 81 avg_loss: 0.3403, acc: 0.8348 
val --- avg_loss: 0.4235, acc: 0.7847  

epoch: 82 avg_loss: 0.3416, acc: 0.8377 
val --- avg_loss: 0.4179, acc: 0.7943  

epoch: 83 avg_loss: 0.3406, acc: 0.8385 
val --- avg_loss: 0.4187, acc: 0.7847  

epoch: 84 avg_loss: 0.3419, acc: 0.8403 
val --- avg_loss: 0.4154, acc: 0.7847  

epoch: 85 avg_loss: 0.3362, acc: 0.8440 
val --- avg_loss: 0.4078, acc: 0.7847  
update best loss 

epoch: 86 avg_loss: 0.3437, acc: 0.8471 
val --- avg_loss: 0.4067, acc: 0.7990  
update best loss 

epoch: 87 avg_loss: 0.3628, acc: 0.8274 
val --- avg_loss: 0.4198, acc: 0.7847  

epoch: 88 avg_loss: 0.3464, acc: 0.8331 
val --- avg_loss: 0.4149, acc: 0.7943  

epoch: 89 avg_loss: 0.3421, acc: 0.8469 
val --- avg_loss: 0.4096, acc: 0.7990  

epoch: 90 avg_loss: 0.3451, acc: 0.8325 
val --- avg_loss: 0.4178, acc: 0.7943  

epoch: 91 avg_loss: 0.3364, acc: 0.8385 
val --- avg_loss: 0.4095, acc: 0.7990  

epoch: 92 avg_loss: 0.3439, acc: 0.8463 
val --- avg_loss: 0.4194, acc: 0.7847  

epoch: 93 avg_loss: 0.3430, acc: 0.8388 
val --- avg_loss: 0.4170, acc: 0.7943  

epoch: 94 avg_loss: 0.3402, acc: 0.8377 
val --- avg_loss: 0.4197, acc: 0.7847  

epoch: 95 avg_loss: 0.3453, acc: 0.8414 
val --- avg_loss: 0.4105, acc: 0.7990  

epoch: 96 avg_loss: 0.3613, acc: 0.8368 
val --- avg_loss: 0.4244, acc: 0.7847  

epoch: 97 avg_loss: 0.3580, acc: 0.8449 
val --- avg_loss: 0.4047, acc: 0.7990  
update best loss 

epoch: 98 avg_loss: 0.3370, acc: 0.8334 
val --- avg_loss: 0.4173, acc: 0.7847  

epoch: 99 avg_loss: 0.3434, acc: 0.8431 
val --- avg_loss: 0.4142, acc: 0.7943  
