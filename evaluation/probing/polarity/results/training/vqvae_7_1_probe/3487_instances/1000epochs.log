
number of params: 99 
Namespace(batchsize=256, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=1000, fig_path='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/1000epochs.png', log_path='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/1000epochs.log', logger=<common.utils.Logger object at 0x7f80d9db4dd0>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_7_1_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (linear): Linear(in_features=32, out_features=3, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/', nh=512, ni=256, num_dicts=7, nz=512, opt='Adam', orddict_emb_num=100, pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), rootdict_emb_dim=320, rootdict_emb_num=1000, save_path='evaluation/probing/polarity/results/training/vqvae_7_1_probe/3487_instances/1000epochs.pt', seq_to_no_pad='surface', task='surf2polar', trndata='evaluation/probing/polarity/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/polarity/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/polarity/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_root.weight, torch.Size([320, 512]): False
linear_root.bias, torch.Size([320]): False
vq_layer_root.embedding.weight, torch.Size([1000, 320]): False
ord_linears.0.weight, torch.Size([32, 512]): False
ord_linears.0.bias, torch.Size([32]): False
ord_linears.1.weight, torch.Size([32, 512]): False
ord_linears.1.bias, torch.Size([32]): False
ord_linears.2.weight, torch.Size([32, 512]): False
ord_linears.2.bias, torch.Size([32]): False
ord_linears.3.weight, torch.Size([32, 512]): False
ord_linears.3.bias, torch.Size([32]): False
ord_linears.4.weight, torch.Size([32, 512]): False
ord_linears.4.bias, torch.Size([32]): False
ord_linears.5.weight, torch.Size([32, 512]): False
ord_linears.5.bias, torch.Size([32]): False
ord_vq_layers.0.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.1.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.2.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.3.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.4.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.5.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([3, 32]): True
linear.bias, torch.Size([3]): True
epoch: 0 avg_loss: 1.0178, acc: 0.5672 
val --- avg_loss: 0.6316, acc: 0.7703  
update best loss 

epoch: 1 avg_loss: 0.6449, acc: 0.8202 
val --- avg_loss: 0.5445, acc: 0.7703  
update best loss 

epoch: 2 avg_loss: 0.5653, acc: 0.8202 
val --- avg_loss: 0.5200, acc: 0.7703  
update best loss 

epoch: 3 avg_loss: 0.5340, acc: 0.8202 
val --- avg_loss: 0.5280, acc: 0.7703  

epoch: 4 avg_loss: 0.5162, acc: 0.8202 
val --- avg_loss: 0.4981, acc: 0.7703  
update best loss 

epoch: 5 avg_loss: 0.5041, acc: 0.8202 
val --- avg_loss: 0.5008, acc: 0.7703  

epoch: 6 avg_loss: 0.4914, acc: 0.8202 
val --- avg_loss: 0.4834, acc: 0.7703  
update best loss 

epoch: 7 avg_loss: 0.4866, acc: 0.8202 
val --- avg_loss: 0.4889, acc: 0.7703  

epoch: 8 avg_loss: 0.4825, acc: 0.8196 
val --- avg_loss: 0.4728, acc: 0.7703  
update best loss 

epoch: 9 avg_loss: 0.4754, acc: 0.8202 
val --- avg_loss: 0.4831, acc: 0.7703  

epoch: 10 avg_loss: 0.4699, acc: 0.8196 
val --- avg_loss: 0.4830, acc: 0.7703  

epoch: 11 avg_loss: 0.4704, acc: 0.8199 
val --- avg_loss: 0.4812, acc: 0.7703  

epoch: 12 avg_loss: 0.4695, acc: 0.8190 
val --- avg_loss: 0.4800, acc: 0.7703  

epoch: 13 avg_loss: 0.4684, acc: 0.8190 
val --- avg_loss: 0.4720, acc: 0.7703  
update best loss 

epoch: 14 avg_loss: 0.4632, acc: 0.8190 
val --- avg_loss: 0.4747, acc: 0.7703  

epoch: 15 avg_loss: 0.4586, acc: 0.8190 
val --- avg_loss: 0.4714, acc: 0.7703  
update best loss 

epoch: 16 avg_loss: 0.4568, acc: 0.8190 
val --- avg_loss: 0.4643, acc: 0.7703  
update best loss 

epoch: 17 avg_loss: 0.4573, acc: 0.8190 
val --- avg_loss: 0.4765, acc: 0.7703  

epoch: 18 avg_loss: 0.4567, acc: 0.8190 
val --- avg_loss: 0.4682, acc: 0.7703  

epoch: 19 avg_loss: 0.4562, acc: 0.8190 
val --- avg_loss: 0.4641, acc: 0.7703  
update best loss 

epoch: 20 avg_loss: 0.4552, acc: 0.8190 
val --- avg_loss: 0.4703, acc: 0.7703  

epoch: 21 avg_loss: 0.4487, acc: 0.8190 
val --- avg_loss: 0.4674, acc: 0.7703  

epoch: 22 avg_loss: 0.4538, acc: 0.8190 
val --- avg_loss: 0.4719, acc: 0.7703  

epoch: 23 avg_loss: 0.4487, acc: 0.8190 
val --- avg_loss: 0.4640, acc: 0.7703  
update best loss 

epoch: 24 avg_loss: 0.4454, acc: 0.8188 
val --- avg_loss: 0.4662, acc: 0.7703  

epoch: 25 avg_loss: 0.4453, acc: 0.8188 
val --- avg_loss: 0.4639, acc: 0.7703  
update best loss 

epoch: 26 avg_loss: 0.4453, acc: 0.8190 
val --- avg_loss: 0.4670, acc: 0.7703  

epoch: 27 avg_loss: 0.4441, acc: 0.8185 
val --- avg_loss: 0.4647, acc: 0.7703  

epoch: 28 avg_loss: 0.4451, acc: 0.8188 
val --- avg_loss: 0.4593, acc: 0.7703  
update best loss 

epoch: 29 avg_loss: 0.4447, acc: 0.8188 
val --- avg_loss: 0.4637, acc: 0.7703  

epoch: 30 avg_loss: 0.4417, acc: 0.8188 
val --- avg_loss: 0.4665, acc: 0.7703  

epoch: 31 avg_loss: 0.4426, acc: 0.8188 
val --- avg_loss: 0.4622, acc: 0.7703  

epoch: 32 avg_loss: 0.4411, acc: 0.8188 
val --- avg_loss: 0.4566, acc: 0.7703  
update best loss 

epoch: 33 avg_loss: 0.4435, acc: 0.8188 
val --- avg_loss: 0.4679, acc: 0.7703  

epoch: 34 avg_loss: 0.4433, acc: 0.8188 
val --- avg_loss: 0.4575, acc: 0.7703  

epoch: 35 avg_loss: 0.4412, acc: 0.8188 
val --- avg_loss: 0.4625, acc: 0.7703  

epoch: 36 avg_loss: 0.4394, acc: 0.8188 
val --- avg_loss: 0.4574, acc: 0.7703  

epoch: 37 avg_loss: 0.4416, acc: 0.8188 
val --- avg_loss: 0.4538, acc: 0.7703  
update best loss 

epoch: 38 avg_loss: 0.4454, acc: 0.8188 
val --- avg_loss: 0.4893, acc: 0.7703  

epoch: 39 avg_loss: 0.4399, acc: 0.8188 
val --- avg_loss: 0.4596, acc: 0.7703  

epoch: 40 avg_loss: 0.4408, acc: 0.8188 
val --- avg_loss: 0.4623, acc: 0.7703  

epoch: 41 avg_loss: 0.4453, acc: 0.8188 
val --- avg_loss: 0.4744, acc: 0.7703  

epoch: 42 avg_loss: 0.4452, acc: 0.8188 
val --- avg_loss: 0.4616, acc: 0.7703  

epoch: 43 avg_loss: 0.4382, acc: 0.8188 
val --- avg_loss: 0.4596, acc: 0.7703  

epoch: 44 avg_loss: 0.4381, acc: 0.8188 
val --- avg_loss: 0.4590, acc: 0.7703  

epoch: 45 avg_loss: 0.4397, acc: 0.8188 
val --- avg_loss: 0.4616, acc: 0.7703  

epoch: 46 avg_loss: 0.4373, acc: 0.8188 
val --- avg_loss: 0.4591, acc: 0.7703  

epoch: 47 avg_loss: 0.4385, acc: 0.8188 
val --- avg_loss: 0.4601, acc: 0.7703  

epoch: 48 avg_loss: 0.4388, acc: 0.8188 
val --- avg_loss: 0.4610, acc: 0.7703  

epoch: 49 avg_loss: 0.4367, acc: 0.8188 
val --- avg_loss: 0.4577, acc: 0.7703  

epoch: 50 avg_loss: 0.4396, acc: 0.8188 
val --- avg_loss: 0.4716, acc: 0.7703  

epoch: 51 avg_loss: 0.4385, acc: 0.8188 
val --- avg_loss: 0.4613, acc: 0.7703  

epoch: 52 avg_loss: 0.4365, acc: 0.8188 
val --- avg_loss: 0.4595, acc: 0.7703  

epoch: 53 avg_loss: 0.4398, acc: 0.8188 
val --- avg_loss: 0.4655, acc: 0.7703  

epoch: 54 avg_loss: 0.4360, acc: 0.8188 
val --- avg_loss: 0.4656, acc: 0.7703  

epoch: 55 avg_loss: 0.4370, acc: 0.8188 
val --- avg_loss: 0.4585, acc: 0.7703  

epoch: 56 avg_loss: 0.4362, acc: 0.8188 
val --- avg_loss: 0.4630, acc: 0.7703  

epoch: 57 avg_loss: 0.4352, acc: 0.8188 
val --- avg_loss: 0.4529, acc: 0.7703  
update best loss 

epoch: 58 avg_loss: 0.4341, acc: 0.8188 
val --- avg_loss: 0.4633, acc: 0.7703  

epoch: 59 avg_loss: 0.4351, acc: 0.8188 
val --- avg_loss: 0.4613, acc: 0.7703  

epoch: 60 avg_loss: 0.4358, acc: 0.8188 
val --- avg_loss: 0.4506, acc: 0.7703  
update best loss 

epoch: 61 avg_loss: 0.4360, acc: 0.8188 
val --- avg_loss: 0.4572, acc: 0.7703  

epoch: 62 avg_loss: 0.4353, acc: 0.8188 
val --- avg_loss: 0.4580, acc: 0.7703  

epoch: 63 avg_loss: 0.4378, acc: 0.8188 
val --- avg_loss: 0.4663, acc: 0.7703  

epoch: 64 avg_loss: 0.4357, acc: 0.8188 
val --- avg_loss: 0.4537, acc: 0.7703  

epoch: 65 avg_loss: 0.4341, acc: 0.8188 
val --- avg_loss: 0.4605, acc: 0.7703  

epoch: 66 avg_loss: 0.4334, acc: 0.8188 
val --- avg_loss: 0.4555, acc: 0.7703  

epoch: 67 avg_loss: 0.4324, acc: 0.8188 
val --- avg_loss: 0.4561, acc: 0.7703  

epoch: 68 avg_loss: 0.4333, acc: 0.8188 
val --- avg_loss: 0.4573, acc: 0.7703  

epoch: 69 avg_loss: 0.4337, acc: 0.8190 
val --- avg_loss: 0.4571, acc: 0.7703  

epoch: 70 avg_loss: 0.4304, acc: 0.8188 
val --- avg_loss: 0.4550, acc: 0.7703  

epoch: 71 avg_loss: 0.4353, acc: 0.8188 
val --- avg_loss: 0.4552, acc: 0.7703  

epoch: 72 avg_loss: 0.4351, acc: 0.8188 
val --- avg_loss: 0.4575, acc: 0.7703  
