
number of params: 1536 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/polarity/results/training/mvqvae_003_probe/3487_instances/200epochs.png', log_path='evaluation/probing/polarity/results/training/mvqvae_003_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7f8cd8311690>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='mvqvae_003_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=False)
  (linear_3): Linear(in_features=512, out_features=512, bias=False)
  (linear_4): Linear(in_features=512, out_features=512, bias=False)
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(710, 512)
  )
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(5, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(6, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(2, 512)
  )
  (linear): Linear(in_features=512, out_features=3, bias=False)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/polarity/results/training/mvqvae_003_probe/3487_instances/', nh=512, ni=256, num_embeddings=710, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(710, 512)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=False)
  (linear_3): Linear(in_features=512, out_features=512, bias=False)
  (linear_4): Linear(in_features=512, out_features=512, bias=False)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(5, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(6, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(2, 512)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/polarity/results/training/mvqvae_003_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2polar', trndata='evaluation/probing/polarity/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/polarity/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/polarity/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([512, 512]): False
linear_3.weight, torch.Size([512, 512]): False
linear_4.weight, torch.Size([512, 512]): False
vq_layer.embedding.weight, torch.Size([710, 512]): False
vq_layer_2.embedding.weight, torch.Size([5, 512]): False
vq_layer_3.embedding.weight, torch.Size([6, 512]): False
vq_layer_4.embedding.weight, torch.Size([2, 512]): False
linear.weight, torch.Size([3, 512]): True
epoch: 0 avg_loss: 0.8301, acc: 0.6808 

epoch: 1 avg_loss: 0.4592, acc: 0.8580 

epoch: 2 avg_loss: 0.3452, acc: 0.8927 

epoch: 3 avg_loss: 0.2866, acc: 0.9143 

epoch: 4 avg_loss: 0.2509, acc: 0.9249 

epoch: 5 avg_loss: 0.2232, acc: 0.9289 

epoch: 6 avg_loss: 0.2021, acc: 0.9389 

epoch: 7 avg_loss: 0.1859, acc: 0.9472 

epoch: 8 avg_loss: 0.1727, acc: 0.9527 

epoch: 9 avg_loss: 0.1615, acc: 0.9555 

epoch: 10 avg_loss: 0.1523, acc: 0.9607 

epoch: 11 avg_loss: 0.1439, acc: 0.9619 

epoch: 12 avg_loss: 0.1368, acc: 0.9664 

epoch: 13 avg_loss: 0.1305, acc: 0.9687 

epoch: 14 avg_loss: 0.1250, acc: 0.9702 

epoch: 15 avg_loss: 0.1197, acc: 0.9722 

epoch: 16 avg_loss: 0.1154, acc: 0.9725 

epoch: 17 avg_loss: 0.1110, acc: 0.9730 

epoch: 18 avg_loss: 0.1071, acc: 0.9742 

epoch: 19 avg_loss: 0.1037, acc: 0.9751 

epoch: 20 avg_loss: 0.1005, acc: 0.9748 

epoch: 21 avg_loss: 0.0974, acc: 0.9782 

epoch: 22 avg_loss: 0.0947, acc: 0.9779 

epoch: 23 avg_loss: 0.0918, acc: 0.9785 

epoch: 24 avg_loss: 0.0898, acc: 0.9788 

epoch: 25 avg_loss: 0.0870, acc: 0.9799 

epoch: 26 avg_loss: 0.0851, acc: 0.9805 

epoch: 27 avg_loss: 0.0830, acc: 0.9811 

epoch: 28 avg_loss: 0.0806, acc: 0.9808 

epoch: 29 avg_loss: 0.0789, acc: 0.9819 

epoch: 30 avg_loss: 0.0773, acc: 0.9819 

epoch: 31 avg_loss: 0.0755, acc: 0.9828 

epoch: 32 avg_loss: 0.0739, acc: 0.9831 

epoch: 33 avg_loss: 0.0722, acc: 0.9828 

epoch: 34 avg_loss: 0.0708, acc: 0.9831 

epoch: 35 avg_loss: 0.0695, acc: 0.9839 

epoch: 36 avg_loss: 0.0682, acc: 0.9834 

epoch: 37 avg_loss: 0.0667, acc: 0.9831 

epoch: 38 avg_loss: 0.0655, acc: 0.9834 

epoch: 39 avg_loss: 0.0645, acc: 0.9845 

epoch: 40 avg_loss: 0.0631, acc: 0.9848 

epoch: 41 avg_loss: 0.0619, acc: 0.9851 

epoch: 42 avg_loss: 0.0607, acc: 0.9851 

epoch: 43 avg_loss: 0.0600, acc: 0.9851 

epoch: 44 avg_loss: 0.0587, acc: 0.9854 

epoch: 45 avg_loss: 0.0577, acc: 0.9857 

epoch: 46 avg_loss: 0.0567, acc: 0.9868 

epoch: 47 avg_loss: 0.0559, acc: 0.9865 

epoch: 48 avg_loss: 0.0549, acc: 0.9871 

epoch: 49 avg_loss: 0.0541, acc: 0.9874 

epoch: 50 avg_loss: 0.0530, acc: 0.9880 

epoch: 51 avg_loss: 0.0524, acc: 0.9882 

epoch: 52 avg_loss: 0.0515, acc: 0.9880 

epoch: 53 avg_loss: 0.0508, acc: 0.9888 

epoch: 54 avg_loss: 0.0498, acc: 0.9888 

epoch: 55 avg_loss: 0.0493, acc: 0.9885 

epoch: 56 avg_loss: 0.0484, acc: 0.9891 

epoch: 57 avg_loss: 0.0477, acc: 0.9908 

epoch: 58 avg_loss: 0.0473, acc: 0.9902 

epoch: 59 avg_loss: 0.0463, acc: 0.9905 

epoch: 60 avg_loss: 0.0456, acc: 0.9917 

epoch: 61 avg_loss: 0.0452, acc: 0.9911 

epoch: 62 avg_loss: 0.0448, acc: 0.9911 

epoch: 63 avg_loss: 0.0439, acc: 0.9914 

epoch: 64 avg_loss: 0.0431, acc: 0.9914 

epoch: 65 avg_loss: 0.0427, acc: 0.9920 

epoch: 66 avg_loss: 0.0419, acc: 0.9917 

epoch: 67 avg_loss: 0.0414, acc: 0.9920 

epoch: 68 avg_loss: 0.0409, acc: 0.9923 

epoch: 69 avg_loss: 0.0405, acc: 0.9923 

epoch: 70 avg_loss: 0.0399, acc: 0.9920 

epoch: 71 avg_loss: 0.0393, acc: 0.9928 

epoch: 72 avg_loss: 0.0386, acc: 0.9928 

epoch: 73 avg_loss: 0.0382, acc: 0.9925 

epoch: 74 avg_loss: 0.0378, acc: 0.9920 

epoch: 75 avg_loss: 0.0372, acc: 0.9931 

epoch: 76 avg_loss: 0.0367, acc: 0.9934 

epoch: 77 avg_loss: 0.0365, acc: 0.9937 

epoch: 78 avg_loss: 0.0359, acc: 0.9934 

epoch: 79 avg_loss: 0.0353, acc: 0.9934 

epoch: 80 avg_loss: 0.0353, acc: 0.9934 

epoch: 81 avg_loss: 0.0345, acc: 0.9940 

epoch: 82 avg_loss: 0.0339, acc: 0.9937 

epoch: 83 avg_loss: 0.0338, acc: 0.9943 

epoch: 84 avg_loss: 0.0333, acc: 0.9946 

epoch: 85 avg_loss: 0.0328, acc: 0.9948 

epoch: 86 avg_loss: 0.0324, acc: 0.9948 

epoch: 87 avg_loss: 0.0321, acc: 0.9951 

epoch: 88 avg_loss: 0.0317, acc: 0.9943 

epoch: 89 avg_loss: 0.0312, acc: 0.9946 

epoch: 90 avg_loss: 0.0308, acc: 0.9954 

epoch: 91 avg_loss: 0.0304, acc: 0.9951 

epoch: 92 avg_loss: 0.0302, acc: 0.9951 

epoch: 93 avg_loss: 0.0298, acc: 0.9957 

epoch: 94 avg_loss: 0.0294, acc: 0.9954 

epoch: 95 avg_loss: 0.0292, acc: 0.9957 

epoch: 96 avg_loss: 0.0288, acc: 0.9954 

epoch: 97 avg_loss: 0.0284, acc: 0.9957 

epoch: 98 avg_loss: 0.0281, acc: 0.9957 

epoch: 99 avg_loss: 0.0277, acc: 0.9960 

epoch: 100 avg_loss: 0.0275, acc: 0.9957 

epoch: 101 avg_loss: 0.0272, acc: 0.9960 

epoch: 102 avg_loss: 0.0268, acc: 0.9957 

epoch: 103 avg_loss: 0.0265, acc: 0.9957 

epoch: 104 avg_loss: 0.0261, acc: 0.9963 

epoch: 105 avg_loss: 0.0258, acc: 0.9960 

epoch: 106 avg_loss: 0.0256, acc: 0.9963 

epoch: 107 avg_loss: 0.0254, acc: 0.9963 

epoch: 108 avg_loss: 0.0250, acc: 0.9966 

epoch: 109 avg_loss: 0.0247, acc: 0.9960 

epoch: 110 avg_loss: 0.0243, acc: 0.9960 

epoch: 111 avg_loss: 0.0244, acc: 0.9963 

epoch: 112 avg_loss: 0.0238, acc: 0.9963 

epoch: 113 avg_loss: 0.0237, acc: 0.9966 

epoch: 114 avg_loss: 0.0234, acc: 0.9966 

epoch: 115 avg_loss: 0.0229, acc: 0.9968 

epoch: 116 avg_loss: 0.0227, acc: 0.9968 

epoch: 117 avg_loss: 0.0225, acc: 0.9966 

epoch: 118 avg_loss: 0.0224, acc: 0.9963 

epoch: 119 avg_loss: 0.0220, acc: 0.9968 

epoch: 120 avg_loss: 0.0218, acc: 0.9971 

epoch: 121 avg_loss: 0.0215, acc: 0.9968 

epoch: 122 avg_loss: 0.0212, acc: 0.9968 

epoch: 123 avg_loss: 0.0209, acc: 0.9974 

epoch: 124 avg_loss: 0.0208, acc: 0.9974 

epoch: 125 avg_loss: 0.0206, acc: 0.9971 

epoch: 126 avg_loss: 0.0203, acc: 0.9971 

epoch: 127 avg_loss: 0.0201, acc: 0.9974 

epoch: 128 avg_loss: 0.0199, acc: 0.9974 

epoch: 129 avg_loss: 0.0196, acc: 0.9974 

epoch: 130 avg_loss: 0.0194, acc: 0.9974 

epoch: 131 avg_loss: 0.0192, acc: 0.9974 

epoch: 132 avg_loss: 0.0189, acc: 0.9977 

epoch: 133 avg_loss: 0.0189, acc: 0.9974 

epoch: 134 avg_loss: 0.0187, acc: 0.9977 

epoch: 135 avg_loss: 0.0183, acc: 0.9977 

epoch: 136 avg_loss: 0.0183, acc: 0.9977 

epoch: 137 avg_loss: 0.0180, acc: 0.9974 

epoch: 138 avg_loss: 0.0179, acc: 0.9977 

epoch: 139 avg_loss: 0.0175, acc: 0.9977 

epoch: 140 avg_loss: 0.0175, acc: 0.9980 

epoch: 141 avg_loss: 0.0174, acc: 0.9980 

epoch: 142 avg_loss: 0.0170, acc: 0.9983 

epoch: 143 avg_loss: 0.0168, acc: 0.9983 

epoch: 144 avg_loss: 0.0166, acc: 0.9983 

epoch: 145 avg_loss: 0.0165, acc: 0.9983 

epoch: 146 avg_loss: 0.0162, acc: 0.9986 

epoch: 147 avg_loss: 0.0161, acc: 0.9983 

epoch: 148 avg_loss: 0.0160, acc: 0.9986 

epoch: 149 avg_loss: 0.0158, acc: 0.9986 

epoch: 150 avg_loss: 0.0156, acc: 0.9989 

epoch: 151 avg_loss: 0.0154, acc: 0.9989 

epoch: 152 avg_loss: 0.0153, acc: 0.9989 

epoch: 153 avg_loss: 0.0152, acc: 0.9989 

epoch: 154 avg_loss: 0.0150, acc: 0.9989 

epoch: 155 avg_loss: 0.0149, acc: 0.9991 

epoch: 156 avg_loss: 0.0147, acc: 0.9991 

epoch: 157 avg_loss: 0.0146, acc: 0.9991 

epoch: 158 avg_loss: 0.0143, acc: 0.9994 

epoch: 159 avg_loss: 0.0141, acc: 0.9994 

epoch: 160 avg_loss: 0.0140, acc: 0.9994 

epoch: 161 avg_loss: 0.0140, acc: 0.9994 

epoch: 162 avg_loss: 0.0138, acc: 0.9994 

epoch: 163 avg_loss: 0.0136, acc: 0.9991 

epoch: 164 avg_loss: 0.0135, acc: 0.9994 

epoch: 165 avg_loss: 0.0134, acc: 0.9994 

epoch: 166 avg_loss: 0.0132, acc: 0.9994 

epoch: 167 avg_loss: 0.0130, acc: 0.9994 

epoch: 168 avg_loss: 0.0130, acc: 0.9994 

epoch: 169 avg_loss: 0.0128, acc: 0.9994 

epoch: 170 avg_loss: 0.0127, acc: 0.9994 

epoch: 171 avg_loss: 0.0126, acc: 0.9994 

epoch: 172 avg_loss: 0.0125, acc: 0.9994 

epoch: 173 avg_loss: 0.0123, acc: 0.9994 

epoch: 174 avg_loss: 0.0121, acc: 0.9994 

epoch: 175 avg_loss: 0.0121, acc: 0.9994 

epoch: 176 avg_loss: 0.0119, acc: 0.9994 

epoch: 177 avg_loss: 0.0118, acc: 0.9994 

epoch: 178 avg_loss: 0.0117, acc: 0.9994 

epoch: 179 avg_loss: 0.0117, acc: 0.9994 

epoch: 180 avg_loss: 0.0114, acc: 0.9994 

epoch: 181 avg_loss: 0.0113, acc: 0.9994 

epoch: 182 avg_loss: 0.0112, acc: 0.9994 

epoch: 183 avg_loss: 0.0111, acc: 0.9994 

epoch: 184 avg_loss: 0.0109, acc: 0.9994 

epoch: 185 avg_loss: 0.0109, acc: 0.9994 

epoch: 186 avg_loss: 0.0108, acc: 0.9994 

epoch: 187 avg_loss: 0.0107, acc: 0.9997 

epoch: 188 avg_loss: 0.0106, acc: 0.9997 

epoch: 189 avg_loss: 0.0104, acc: 0.9997 

epoch: 190 avg_loss: 0.0103, acc: 0.9994 

epoch: 191 avg_loss: 0.0102, acc: 0.9997 

epoch: 192 avg_loss: 0.0102, acc: 0.9994 

epoch: 193 avg_loss: 0.0100, acc: 0.9997 

epoch: 194 avg_loss: 0.0099, acc: 1.0000 

epoch: 195 avg_loss: 0.0099, acc: 1.0000 

epoch: 196 avg_loss: 0.0097, acc: 1.0000 

epoch: 197 avg_loss: 0.0096, acc: 0.9997 

epoch: 198 avg_loss: 0.0095, acc: 0.9997 

epoch: 199 avg_loss: 0.0095, acc: 1.0000 
