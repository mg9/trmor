
number of params: 390 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/tense/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/200epochs.png', log_path='evaluation/probing/tense/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7f5c3109c390>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_7_dict_enlarged_root_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=320, bias=True)
  (linear_3): Linear(in_features=512, out_features=32, bias=True)
  (linear_4): Linear(in_features=512, out_features=32, bias=True)
  (linear_5): Linear(in_features=512, out_features=32, bias=True)
  (linear_6): Linear(in_features=512, out_features=32, bias=True)
  (linear_7): Linear(in_features=512, out_features=32, bias=True)
  (linear_8): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (linear): Linear(in_features=64, out_features=6, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/tense/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=320, bias=True)
  (linear_3): Linear(in_features=512, out_features=32, bias=True)
  (linear_4): Linear(in_features=512, out_features=32, bias=True)
  (linear_5): Linear(in_features=512, out_features=32, bias=True)
  (linear_6): Linear(in_features=512, out_features=32, bias=True)
  (linear_7): Linear(in_features=512, out_features=32, bias=True)
  (linear_8): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/tense/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2tense', trndata='evaluation/probing/tense/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([320, 512]): False
linear_2.bias, torch.Size([320]): False
linear_3.weight, torch.Size([32, 512]): False
linear_3.bias, torch.Size([32]): False
linear_4.weight, torch.Size([32, 512]): False
linear_4.bias, torch.Size([32]): False
linear_5.weight, torch.Size([32, 512]): False
linear_5.bias, torch.Size([32]): False
linear_6.weight, torch.Size([32, 512]): False
linear_6.bias, torch.Size([32]): False
linear_7.weight, torch.Size([32, 512]): False
linear_7.bias, torch.Size([32]): False
linear_8.weight, torch.Size([32, 512]): False
linear_8.bias, torch.Size([32]): False
vq_layer_2.embedding.weight, torch.Size([1000, 320]): False
vq_layer_3.embedding.weight, torch.Size([100, 32]): False
vq_layer_4.embedding.weight, torch.Size([100, 32]): False
vq_layer_5.embedding.weight, torch.Size([100, 32]): False
vq_layer_6.embedding.weight, torch.Size([100, 32]): False
vq_layer_7.embedding.weight, torch.Size([100, 32]): False
vq_layer_8.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([6, 64]): True
linear.bias, torch.Size([6]): True
epoch: 0 avg_loss: 1.7534, acc: 0.3037 

epoch: 1 avg_loss: 1.4847, acc: 0.5128 

epoch: 2 avg_loss: 1.2954, acc: 0.6154 

epoch: 3 avg_loss: 1.1567, acc: 0.6656 

epoch: 4 avg_loss: 1.0628, acc: 0.6794 

epoch: 5 avg_loss: 1.0005, acc: 0.6702 

epoch: 6 avg_loss: 0.9477, acc: 0.6791 

epoch: 7 avg_loss: 0.9113, acc: 0.6751 

epoch: 8 avg_loss: 0.8858, acc: 0.6788 

epoch: 9 avg_loss: 0.8561, acc: 0.6851 

epoch: 10 avg_loss: 0.8375, acc: 0.6888 

epoch: 11 avg_loss: 0.8192, acc: 0.6900 

epoch: 12 avg_loss: 0.8036, acc: 0.6943 

epoch: 13 avg_loss: 0.7906, acc: 0.6960 

epoch: 14 avg_loss: 0.7790, acc: 0.6989 

epoch: 15 avg_loss: 0.7722, acc: 0.6997 

epoch: 16 avg_loss: 0.7628, acc: 0.7049 

epoch: 17 avg_loss: 0.7513, acc: 0.7049 

epoch: 18 avg_loss: 0.7451, acc: 0.7058 

epoch: 19 avg_loss: 0.7380, acc: 0.7052 

epoch: 20 avg_loss: 0.7318, acc: 0.7052 

epoch: 21 avg_loss: 0.7270, acc: 0.7066 

epoch: 22 avg_loss: 0.7218, acc: 0.7049 

epoch: 23 avg_loss: 0.7181, acc: 0.7078 

epoch: 24 avg_loss: 0.7121, acc: 0.7072 

epoch: 25 avg_loss: 0.7072, acc: 0.7081 

epoch: 26 avg_loss: 0.7032, acc: 0.7098 

epoch: 27 avg_loss: 0.6994, acc: 0.7135 

epoch: 28 avg_loss: 0.6982, acc: 0.7115 

epoch: 29 avg_loss: 0.6937, acc: 0.7135 

epoch: 30 avg_loss: 0.6897, acc: 0.7129 

epoch: 31 avg_loss: 0.6886, acc: 0.7138 

epoch: 32 avg_loss: 0.6824, acc: 0.7144 

epoch: 33 avg_loss: 0.6808, acc: 0.7155 

epoch: 34 avg_loss: 0.6801, acc: 0.7147 

epoch: 35 avg_loss: 0.6769, acc: 0.7135 

epoch: 36 avg_loss: 0.6752, acc: 0.7132 

epoch: 37 avg_loss: 0.6707, acc: 0.7158 

epoch: 38 avg_loss: 0.6705, acc: 0.7172 

epoch: 39 avg_loss: 0.6682, acc: 0.7149 

epoch: 40 avg_loss: 0.6655, acc: 0.7147 

epoch: 41 avg_loss: 0.6631, acc: 0.7158 

epoch: 42 avg_loss: 0.6612, acc: 0.7172 

epoch: 43 avg_loss: 0.6618, acc: 0.7172 

epoch: 44 avg_loss: 0.6620, acc: 0.7144 

epoch: 45 avg_loss: 0.6570, acc: 0.7178 

epoch: 46 avg_loss: 0.6591, acc: 0.7161 

epoch: 47 avg_loss: 0.6543, acc: 0.7169 

epoch: 48 avg_loss: 0.6549, acc: 0.7164 

epoch: 49 avg_loss: 0.6541, acc: 0.7161 

epoch: 50 avg_loss: 0.6494, acc: 0.7192 

epoch: 51 avg_loss: 0.6491, acc: 0.7181 

epoch: 52 avg_loss: 0.6507, acc: 0.7172 

epoch: 53 avg_loss: 0.6463, acc: 0.7198 

epoch: 54 avg_loss: 0.6463, acc: 0.7192 

epoch: 55 avg_loss: 0.6473, acc: 0.7201 

epoch: 56 avg_loss: 0.6424, acc: 0.7175 

epoch: 57 avg_loss: 0.6449, acc: 0.7167 

epoch: 58 avg_loss: 0.6416, acc: 0.7198 

epoch: 59 avg_loss: 0.6405, acc: 0.7192 

epoch: 60 avg_loss: 0.6400, acc: 0.7198 

epoch: 61 avg_loss: 0.6435, acc: 0.7172 

epoch: 62 avg_loss: 0.6372, acc: 0.7204 

epoch: 63 avg_loss: 0.6403, acc: 0.7178 

epoch: 64 avg_loss: 0.6382, acc: 0.7195 

epoch: 65 avg_loss: 0.6404, acc: 0.7161 

epoch: 66 avg_loss: 0.6357, acc: 0.7187 

epoch: 67 avg_loss: 0.6388, acc: 0.7190 

epoch: 68 avg_loss: 0.6337, acc: 0.7215 

epoch: 69 avg_loss: 0.6349, acc: 0.7210 

epoch: 70 avg_loss: 0.6336, acc: 0.7213 

epoch: 71 avg_loss: 0.6337, acc: 0.7218 

epoch: 72 avg_loss: 0.6316, acc: 0.7207 

epoch: 73 avg_loss: 0.6367, acc: 0.7213 

epoch: 74 avg_loss: 0.6303, acc: 0.7204 

epoch: 75 avg_loss: 0.6313, acc: 0.7235 

epoch: 76 avg_loss: 0.6302, acc: 0.7221 

epoch: 77 avg_loss: 0.6302, acc: 0.7230 

epoch: 78 avg_loss: 0.6286, acc: 0.7233 

epoch: 79 avg_loss: 0.6278, acc: 0.7218 

epoch: 80 avg_loss: 0.6285, acc: 0.7241 

epoch: 81 avg_loss: 0.6277, acc: 0.7235 

epoch: 82 avg_loss: 0.6310, acc: 0.7230 

epoch: 83 avg_loss: 0.6299, acc: 0.7244 

epoch: 84 avg_loss: 0.6270, acc: 0.7233 

epoch: 85 avg_loss: 0.6291, acc: 0.7233 

epoch: 86 avg_loss: 0.6268, acc: 0.7238 

epoch: 87 avg_loss: 0.6249, acc: 0.7258 

epoch: 88 avg_loss: 0.6255, acc: 0.7261 

epoch: 89 avg_loss: 0.6258, acc: 0.7241 

epoch: 90 avg_loss: 0.6236, acc: 0.7256 

epoch: 91 avg_loss: 0.6229, acc: 0.7227 

epoch: 92 avg_loss: 0.6238, acc: 0.7250 

epoch: 93 avg_loss: 0.6273, acc: 0.7233 

epoch: 94 avg_loss: 0.6230, acc: 0.7261 

epoch: 95 avg_loss: 0.6258, acc: 0.7253 

epoch: 96 avg_loss: 0.6225, acc: 0.7247 

epoch: 97 avg_loss: 0.6235, acc: 0.7244 

epoch: 98 avg_loss: 0.6212, acc: 0.7258 

epoch: 99 avg_loss: 0.6210, acc: 0.7256 

epoch: 100 avg_loss: 0.6209, acc: 0.7256 

epoch: 101 avg_loss: 0.6206, acc: 0.7264 

epoch: 102 avg_loss: 0.6203, acc: 0.7270 

epoch: 103 avg_loss: 0.6202, acc: 0.7276 

epoch: 104 avg_loss: 0.6196, acc: 0.7267 

epoch: 105 avg_loss: 0.6189, acc: 0.7276 

epoch: 106 avg_loss: 0.6197, acc: 0.7244 

epoch: 107 avg_loss: 0.6177, acc: 0.7258 

epoch: 108 avg_loss: 0.6201, acc: 0.7261 

epoch: 109 avg_loss: 0.6172, acc: 0.7270 

epoch: 110 avg_loss: 0.6186, acc: 0.7250 

epoch: 111 avg_loss: 0.6183, acc: 0.7273 

epoch: 112 avg_loss: 0.6180, acc: 0.7256 

epoch: 113 avg_loss: 0.6186, acc: 0.7270 

epoch: 114 avg_loss: 0.6188, acc: 0.7267 

epoch: 115 avg_loss: 0.6159, acc: 0.7256 

epoch: 116 avg_loss: 0.6181, acc: 0.7276 

epoch: 117 avg_loss: 0.6168, acc: 0.7273 

epoch: 118 avg_loss: 0.6177, acc: 0.7241 

epoch: 119 avg_loss: 0.6184, acc: 0.7281 

epoch: 120 avg_loss: 0.6151, acc: 0.7276 

epoch: 121 avg_loss: 0.6185, acc: 0.7267 

epoch: 122 avg_loss: 0.6182, acc: 0.7270 

epoch: 123 avg_loss: 0.6166, acc: 0.7281 

epoch: 124 avg_loss: 0.6186, acc: 0.7261 

epoch: 125 avg_loss: 0.6170, acc: 0.7284 

epoch: 126 avg_loss: 0.6150, acc: 0.7278 

epoch: 127 avg_loss: 0.6148, acc: 0.7287 

epoch: 128 avg_loss: 0.6144, acc: 0.7281 

epoch: 129 avg_loss: 0.6135, acc: 0.7278 

epoch: 130 avg_loss: 0.6142, acc: 0.7290 

epoch: 131 avg_loss: 0.6152, acc: 0.7278 

epoch: 132 avg_loss: 0.6163, acc: 0.7284 

epoch: 133 avg_loss: 0.6137, acc: 0.7293 

epoch: 134 avg_loss: 0.6184, acc: 0.7290 

epoch: 135 avg_loss: 0.6145, acc: 0.7281 

epoch: 136 avg_loss: 0.6128, acc: 0.7284 

epoch: 137 avg_loss: 0.6135, acc: 0.7293 

epoch: 138 avg_loss: 0.6132, acc: 0.7281 

epoch: 139 avg_loss: 0.6115, acc: 0.7307 

epoch: 140 avg_loss: 0.6133, acc: 0.7304 

epoch: 141 avg_loss: 0.6134, acc: 0.7296 

epoch: 142 avg_loss: 0.6153, acc: 0.7307 

epoch: 143 avg_loss: 0.6124, acc: 0.7287 

epoch: 144 avg_loss: 0.6130, acc: 0.7281 

epoch: 145 avg_loss: 0.6134, acc: 0.7304 

epoch: 146 avg_loss: 0.6114, acc: 0.7304 

epoch: 147 avg_loss: 0.6116, acc: 0.7321 

epoch: 148 avg_loss: 0.6120, acc: 0.7316 

epoch: 149 avg_loss: 0.6114, acc: 0.7324 

epoch: 150 avg_loss: 0.6143, acc: 0.7287 

epoch: 151 avg_loss: 0.6103, acc: 0.7307 

epoch: 152 avg_loss: 0.6112, acc: 0.7304 

epoch: 153 avg_loss: 0.6126, acc: 0.7319 

epoch: 154 avg_loss: 0.6122, acc: 0.7304 

epoch: 155 avg_loss: 0.6130, acc: 0.7313 

epoch: 156 avg_loss: 0.6113, acc: 0.7307 

epoch: 157 avg_loss: 0.6114, acc: 0.7321 

epoch: 158 avg_loss: 0.6106, acc: 0.7313 

epoch: 159 avg_loss: 0.6115, acc: 0.7316 

epoch: 160 avg_loss: 0.6103, acc: 0.7319 

epoch: 161 avg_loss: 0.6108, acc: 0.7304 

epoch: 162 avg_loss: 0.6119, acc: 0.7307 

epoch: 163 avg_loss: 0.6121, acc: 0.7301 

epoch: 164 avg_loss: 0.6086, acc: 0.7359 

epoch: 165 avg_loss: 0.6149, acc: 0.7296 

epoch: 166 avg_loss: 0.6094, acc: 0.7342 

epoch: 167 avg_loss: 0.6106, acc: 0.7327 

epoch: 168 avg_loss: 0.6084, acc: 0.7336 

epoch: 169 avg_loss: 0.6071, acc: 0.7342 

epoch: 170 avg_loss: 0.6097, acc: 0.7324 

epoch: 171 avg_loss: 0.6095, acc: 0.7321 

epoch: 172 avg_loss: 0.6099, acc: 0.7319 

epoch: 173 avg_loss: 0.6082, acc: 0.7313 

epoch: 174 avg_loss: 0.6091, acc: 0.7339 

epoch: 175 avg_loss: 0.6101, acc: 0.7324 

epoch: 176 avg_loss: 0.6089, acc: 0.7342 

epoch: 177 avg_loss: 0.6097, acc: 0.7316 

epoch: 178 avg_loss: 0.6082, acc: 0.7321 

epoch: 179 avg_loss: 0.6089, acc: 0.7313 

epoch: 180 avg_loss: 0.6079, acc: 0.7319 

epoch: 181 avg_loss: 0.6112, acc: 0.7287 

epoch: 182 avg_loss: 0.6075, acc: 0.7342 

epoch: 183 avg_loss: 0.6081, acc: 0.7347 

epoch: 184 avg_loss: 0.6095, acc: 0.7324 

epoch: 185 avg_loss: 0.6086, acc: 0.7316 

epoch: 186 avg_loss: 0.6079, acc: 0.7333 

epoch: 187 avg_loss: 0.6095, acc: 0.7310 

epoch: 188 avg_loss: 0.6096, acc: 0.7321 

epoch: 189 avg_loss: 0.6104, acc: 0.7342 

epoch: 190 avg_loss: 0.6080, acc: 0.7339 

epoch: 191 avg_loss: 0.6112, acc: 0.7301 

epoch: 192 avg_loss: 0.6082, acc: 0.7336 

epoch: 193 avg_loss: 0.6077, acc: 0.7301 

epoch: 194 avg_loss: 0.6069, acc: 0.7330 

epoch: 195 avg_loss: 0.6094, acc: 0.7310 

epoch: 196 avg_loss: 0.6089, acc: 0.7367 

epoch: 197 avg_loss: 0.6064, acc: 0.7304 

epoch: 198 avg_loss: 0.6129, acc: 0.7310 

epoch: 199 avg_loss: 0.6061, acc: 0.7339 
