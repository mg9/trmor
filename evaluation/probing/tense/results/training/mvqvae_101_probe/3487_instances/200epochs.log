
number of params: 3072 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/tense/results/training/mvqvae_101_probe/3487_instances/200epochs.png', log_path='evaluation/probing/tense/results/training/mvqvae_101_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7fd4c805acd0>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='mvqvae_101_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (linear_9): Linear(in_features=512, out_features=512, bias=True)
  (linear_10): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_10): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (linear): Linear(in_features=512, out_features=6, bias=False)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/tense/results/training/mvqvae_101_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (linear_9): Linear(in_features=512, out_features=512, bias=True)
  (linear_10): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (vq_layer_10): VectorQuantizer(
    (embedding): Embedding(16, 512)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/tense/results/training/mvqvae_101_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2tense', trndata='evaluation/probing/tense/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([512, 512]): False
linear_2.bias, torch.Size([512]): False
linear_3.weight, torch.Size([512, 512]): False
linear_3.bias, torch.Size([512]): False
linear_4.weight, torch.Size([512, 512]): False
linear_4.bias, torch.Size([512]): False
linear_5.weight, torch.Size([512, 512]): False
linear_5.bias, torch.Size([512]): False
linear_6.weight, torch.Size([512, 512]): False
linear_6.bias, torch.Size([512]): False
linear_7.weight, torch.Size([512, 512]): False
linear_7.bias, torch.Size([512]): False
linear_8.weight, torch.Size([512, 512]): False
linear_8.bias, torch.Size([512]): False
linear_9.weight, torch.Size([512, 512]): False
linear_9.bias, torch.Size([512]): False
linear_10.weight, torch.Size([512, 512]): False
linear_10.bias, torch.Size([512]): False
vq_layer.embedding.weight, torch.Size([704, 512]): False
vq_layer_2.embedding.weight, torch.Size([16, 512]): False
vq_layer_3.embedding.weight, torch.Size([16, 512]): False
vq_layer_4.embedding.weight, torch.Size([16, 512]): False
vq_layer_5.embedding.weight, torch.Size([16, 512]): False
vq_layer_6.embedding.weight, torch.Size([16, 512]): False
vq_layer_7.embedding.weight, torch.Size([16, 512]): False
vq_layer_8.embedding.weight, torch.Size([16, 512]): False
vq_layer_9.embedding.weight, torch.Size([16, 512]): False
vq_layer_10.embedding.weight, torch.Size([16, 512]): False
linear.weight, torch.Size([6, 512]): True
epoch: 0 avg_loss: 1.3462, acc: 0.4611 

epoch: 1 avg_loss: 1.0669, acc: 0.5784 

epoch: 2 avg_loss: 1.0367, acc: 0.5604 

epoch: 3 avg_loss: 1.0657, acc: 0.5733 

epoch: 4 avg_loss: 1.0299, acc: 0.5475 

epoch: 5 avg_loss: 1.0138, acc: 0.5845 

epoch: 6 avg_loss: 1.0307, acc: 0.5526 

epoch: 7 avg_loss: 1.0432, acc: 0.5635 

epoch: 8 avg_loss: 1.0757, acc: 0.5644 

epoch: 9 avg_loss: 1.0346, acc: 0.5753 

epoch: 10 avg_loss: 1.0379, acc: 0.5434 

epoch: 11 avg_loss: 1.0147, acc: 0.5764 

epoch: 12 avg_loss: 1.0247, acc: 0.5541 

epoch: 13 avg_loss: 1.0103, acc: 0.5779 

epoch: 14 avg_loss: 1.0283, acc: 0.5317 

epoch: 15 avg_loss: 1.0322, acc: 0.5724 

epoch: 16 avg_loss: 1.0261, acc: 0.5624 

epoch: 17 avg_loss: 1.0252, acc: 0.5738 

epoch: 18 avg_loss: 1.0118, acc: 0.5552 

epoch: 19 avg_loss: 1.0339, acc: 0.5664 

epoch: 20 avg_loss: 1.0145, acc: 0.5896 

epoch: 21 avg_loss: 1.0218, acc: 0.5618 

epoch: 22 avg_loss: 1.0304, acc: 0.5856 

epoch: 23 avg_loss: 1.0417, acc: 0.5297 

epoch: 24 avg_loss: 1.0301, acc: 0.5472 

epoch: 25 avg_loss: 1.0344, acc: 0.5607 

epoch: 26 avg_loss: 1.0352, acc: 0.5366 

epoch: 27 avg_loss: 1.0256, acc: 0.5939 

epoch: 28 avg_loss: 1.0125, acc: 0.5738 

epoch: 29 avg_loss: 1.0303, acc: 0.5555 

epoch: 30 avg_loss: 1.0322, acc: 0.5397 

epoch: 31 avg_loss: 1.0345, acc: 0.5480 

epoch: 32 avg_loss: 1.0004, acc: 0.5612 

epoch: 33 avg_loss: 1.0142, acc: 0.5684 

epoch: 34 avg_loss: 1.0286, acc: 0.5693 

epoch: 35 avg_loss: 1.0293, acc: 0.5260 

epoch: 36 avg_loss: 1.0267, acc: 0.5747 

epoch: 37 avg_loss: 1.0180, acc: 0.5432 

epoch: 38 avg_loss: 1.0275, acc: 0.5515 

epoch: 39 avg_loss: 1.0214, acc: 0.5773 

epoch: 40 avg_loss: 1.0172, acc: 0.5730 

epoch: 41 avg_loss: 1.0283, acc: 0.5604 

epoch: 42 avg_loss: 1.0270, acc: 0.5495 

epoch: 43 avg_loss: 1.0248, acc: 0.5609 

epoch: 44 avg_loss: 1.0424, acc: 0.5446 

epoch: 45 avg_loss: 1.0153, acc: 0.5506 

epoch: 46 avg_loss: 1.0113, acc: 0.5529 

epoch: 47 avg_loss: 1.0192, acc: 0.5822 

epoch: 48 avg_loss: 1.0229, acc: 0.5713 

epoch: 49 avg_loss: 1.0153, acc: 0.5802 

epoch: 50 avg_loss: 1.0148, acc: 0.5627 

epoch: 51 avg_loss: 1.0211, acc: 0.5908 

epoch: 52 avg_loss: 1.0317, acc: 0.5394 

epoch: 53 avg_loss: 1.0576, acc: 0.5374 

epoch: 54 avg_loss: 1.0088, acc: 0.5779 

epoch: 55 avg_loss: 1.0378, acc: 0.5759 

epoch: 56 avg_loss: 1.0378, acc: 0.5561 

epoch: 57 avg_loss: 1.0238, acc: 0.5363 

epoch: 58 avg_loss: 1.0497, acc: 0.5776 

epoch: 59 avg_loss: 1.0200, acc: 0.5555 

epoch: 60 avg_loss: 1.0206, acc: 0.5477 

epoch: 61 avg_loss: 1.0227, acc: 0.5667 

epoch: 62 avg_loss: 1.0141, acc: 0.5618 

epoch: 63 avg_loss: 1.0480, acc: 0.5271 

epoch: 64 avg_loss: 0.9989, acc: 0.5724 

epoch: 65 avg_loss: 1.0517, acc: 0.5615 

epoch: 66 avg_loss: 1.0337, acc: 0.5595 

epoch: 67 avg_loss: 1.0140, acc: 0.5627 

epoch: 68 avg_loss: 1.0540, acc: 0.5323 

epoch: 69 avg_loss: 1.0446, acc: 0.5434 

epoch: 70 avg_loss: 1.0235, acc: 0.5913 

epoch: 71 avg_loss: 1.0200, acc: 0.5245 

epoch: 72 avg_loss: 1.0252, acc: 0.5483 

epoch: 73 avg_loss: 1.0194, acc: 0.5896 

epoch: 74 avg_loss: 1.0124, acc: 0.5733 

epoch: 75 avg_loss: 1.0188, acc: 0.5833 

epoch: 76 avg_loss: 1.0159, acc: 0.5518 

epoch: 77 avg_loss: 1.0208, acc: 0.5750 

epoch: 78 avg_loss: 1.0245, acc: 0.5303 

epoch: 79 avg_loss: 1.0130, acc: 0.5770 

epoch: 80 avg_loss: 1.0170, acc: 0.5804 

epoch: 81 avg_loss: 1.0453, acc: 0.4935 

epoch: 82 avg_loss: 1.0517, acc: 0.5555 

epoch: 83 avg_loss: 1.0205, acc: 0.5581 

epoch: 84 avg_loss: 1.0437, acc: 0.5073 

epoch: 85 avg_loss: 1.0108, acc: 0.5842 

epoch: 86 avg_loss: 1.0211, acc: 0.5793 

epoch: 87 avg_loss: 1.0221, acc: 0.5426 

epoch: 88 avg_loss: 1.0122, acc: 0.5819 

epoch: 89 avg_loss: 1.0318, acc: 0.5512 

epoch: 90 avg_loss: 1.0347, acc: 0.5893 

epoch: 91 avg_loss: 1.0132, acc: 0.5868 

epoch: 92 avg_loss: 1.0496, acc: 0.5383 

epoch: 93 avg_loss: 1.0122, acc: 0.5690 

epoch: 94 avg_loss: 1.0216, acc: 0.5759 

epoch: 95 avg_loss: 1.0107, acc: 0.6014 

epoch: 96 avg_loss: 1.0183, acc: 0.5759 

epoch: 97 avg_loss: 1.0143, acc: 0.5847 

epoch: 98 avg_loss: 1.0159, acc: 0.5621 

epoch: 99 avg_loss: 1.0334, acc: 0.5437 

epoch: 100 avg_loss: 1.0398, acc: 0.5480 

epoch: 101 avg_loss: 1.0068, acc: 0.5747 

epoch: 102 avg_loss: 1.0216, acc: 0.5781 

epoch: 103 avg_loss: 1.0061, acc: 0.5664 

epoch: 104 avg_loss: 1.0369, acc: 0.5870 

epoch: 105 avg_loss: 1.0343, acc: 0.5463 

epoch: 106 avg_loss: 1.0468, acc: 0.5526 

epoch: 107 avg_loss: 1.0432, acc: 0.5366 

epoch: 108 avg_loss: 1.0248, acc: 0.5819 

epoch: 109 avg_loss: 1.0238, acc: 0.5667 

epoch: 110 avg_loss: 1.0074, acc: 0.5753 

epoch: 111 avg_loss: 1.0374, acc: 0.5658 

epoch: 112 avg_loss: 1.0207, acc: 0.5913 

epoch: 113 avg_loss: 1.0324, acc: 0.5701 

epoch: 114 avg_loss: 1.0677, acc: 0.5125 

epoch: 115 avg_loss: 1.0240, acc: 0.5635 

epoch: 116 avg_loss: 1.0179, acc: 0.5859 

epoch: 117 avg_loss: 1.0334, acc: 0.5271 

epoch: 118 avg_loss: 1.0361, acc: 0.5584 

epoch: 119 avg_loss: 1.0517, acc: 0.5690 

epoch: 120 avg_loss: 1.0408, acc: 0.5423 

epoch: 121 avg_loss: 1.0219, acc: 0.5615 

epoch: 122 avg_loss: 1.0320, acc: 0.5632 

epoch: 123 avg_loss: 1.0373, acc: 0.5420 

epoch: 124 avg_loss: 1.0386, acc: 0.5466 

epoch: 125 avg_loss: 1.0243, acc: 0.5816 

epoch: 126 avg_loss: 1.0146, acc: 0.5661 

epoch: 127 avg_loss: 1.0224, acc: 0.5856 

epoch: 128 avg_loss: 1.0248, acc: 0.5658 

epoch: 129 avg_loss: 1.0136, acc: 0.5773 

epoch: 130 avg_loss: 1.0244, acc: 0.5647 

epoch: 131 avg_loss: 1.0206, acc: 0.5842 

epoch: 132 avg_loss: 1.0033, acc: 0.5543 

epoch: 133 avg_loss: 1.0145, acc: 0.5736 

epoch: 134 avg_loss: 1.0310, acc: 0.5658 

epoch: 135 avg_loss: 1.0175, acc: 0.5931 

epoch: 136 avg_loss: 1.0247, acc: 0.5830 

epoch: 137 avg_loss: 1.0244, acc: 0.5721 

epoch: 138 avg_loss: 1.0232, acc: 0.5647 

epoch: 139 avg_loss: 1.0116, acc: 0.5770 

epoch: 140 avg_loss: 1.0207, acc: 0.5475 

epoch: 141 avg_loss: 1.0134, acc: 0.5942 

epoch: 142 avg_loss: 1.0068, acc: 0.5842 

epoch: 143 avg_loss: 1.0178, acc: 0.5340 

epoch: 144 avg_loss: 1.0378, acc: 0.5561 

epoch: 145 avg_loss: 1.0261, acc: 0.5655 

epoch: 146 avg_loss: 1.0176, acc: 0.5870 

epoch: 147 avg_loss: 0.9999, acc: 0.5882 

epoch: 148 avg_loss: 1.0326, acc: 0.5595 

epoch: 149 avg_loss: 1.0086, acc: 0.5658 

epoch: 150 avg_loss: 1.0278, acc: 0.5561 

epoch: 151 avg_loss: 1.0168, acc: 0.5684 

epoch: 152 avg_loss: 1.0254, acc: 0.5566 

epoch: 153 avg_loss: 1.0298, acc: 0.5615 

epoch: 154 avg_loss: 1.0050, acc: 0.5638 

epoch: 155 avg_loss: 1.0159, acc: 0.5822 

epoch: 156 avg_loss: 1.0166, acc: 0.5816 

epoch: 157 avg_loss: 1.0350, acc: 0.5466 

epoch: 158 avg_loss: 1.0100, acc: 0.5707 

epoch: 159 avg_loss: 1.0152, acc: 0.5787 

epoch: 160 avg_loss: 1.0418, acc: 0.5799 

epoch: 161 avg_loss: 1.0212, acc: 0.5621 

epoch: 162 avg_loss: 1.0318, acc: 0.5787 

epoch: 163 avg_loss: 1.0410, acc: 0.5839 

epoch: 164 avg_loss: 1.0351, acc: 0.5543 

epoch: 165 avg_loss: 1.0132, acc: 0.5761 

epoch: 166 avg_loss: 1.0283, acc: 0.5615 

epoch: 167 avg_loss: 1.0106, acc: 0.5730 

epoch: 168 avg_loss: 1.0282, acc: 0.5776 

epoch: 169 avg_loss: 1.0109, acc: 0.5759 

epoch: 170 avg_loss: 1.0122, acc: 0.5804 

epoch: 171 avg_loss: 1.0320, acc: 0.5139 

epoch: 172 avg_loss: 1.0234, acc: 0.5767 

epoch: 173 avg_loss: 1.0401, acc: 0.5357 

epoch: 174 avg_loss: 1.0324, acc: 0.5564 

epoch: 175 avg_loss: 1.0117, acc: 0.5724 

epoch: 176 avg_loss: 1.0137, acc: 0.5515 

epoch: 177 avg_loss: 1.0188, acc: 0.5475 

epoch: 178 avg_loss: 1.0298, acc: 0.5802 

epoch: 179 avg_loss: 1.0112, acc: 0.5532 

epoch: 180 avg_loss: 1.0027, acc: 0.5650 

epoch: 181 avg_loss: 1.0203, acc: 0.5492 

epoch: 182 avg_loss: 1.0050, acc: 0.5475 

epoch: 183 avg_loss: 1.0241, acc: 0.5687 

epoch: 184 avg_loss: 1.0367, acc: 0.5449 

epoch: 185 avg_loss: 1.0105, acc: 0.5595 

epoch: 186 avg_loss: 1.0264, acc: 0.5790 

epoch: 187 avg_loss: 1.0186, acc: 0.5601 

epoch: 188 avg_loss: 1.0215, acc: 0.5770 

epoch: 189 avg_loss: 1.0099, acc: 0.5744 

epoch: 190 avg_loss: 1.0169, acc: 0.5655 

epoch: 191 avg_loss: 1.0039, acc: 0.5856 

epoch: 192 avg_loss: 1.0189, acc: 0.5779 

epoch: 193 avg_loss: 1.0356, acc: 0.5779 

epoch: 194 avg_loss: 0.9975, acc: 0.5707 

epoch: 195 avg_loss: 1.0201, acc: 0.5753 

epoch: 196 avg_loss: 1.0335, acc: 0.5575 

epoch: 197 avg_loss: 1.0316, acc: 0.5658 

epoch: 198 avg_loss: 1.0214, acc: 0.5813 

epoch: 199 avg_loss: 1.0099, acc: 0.5756 
