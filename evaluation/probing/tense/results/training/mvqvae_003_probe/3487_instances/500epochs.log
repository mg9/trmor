
number of params: 3072 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=500, fig_path='evaluation/probing/tense/results/training/mvqvae_003_probe/3487_instances/500epochs.png', log_path='evaluation/probing/tense/results/training/mvqvae_003_probe/3487_instances/500epochs.log', logger=<common.utils.Logger object at 0x7f83258e4310>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='mvqvae_003_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=False)
  (linear_3): Linear(in_features=512, out_features=512, bias=False)
  (linear_4): Linear(in_features=512, out_features=512, bias=False)
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(710, 512)
  )
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(5, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(6, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(2, 512)
  )
  (linear): Linear(in_features=512, out_features=6, bias=False)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/tense/results/training/mvqvae_003_probe/3487_instances/', nh=512, ni=256, num_embeddings=710, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(710, 512)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=False)
  (linear_3): Linear(in_features=512, out_features=512, bias=False)
  (linear_4): Linear(in_features=512, out_features=512, bias=False)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(5, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(6, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(2, 512)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/tense/results/training/mvqvae_003_probe/3487_instances/500epochs.pt', seq_to_no_pad='surface', task='surf2tense', trndata='evaluation/probing/tense/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([512, 512]): False
linear_3.weight, torch.Size([512, 512]): False
linear_4.weight, torch.Size([512, 512]): False
vq_layer.embedding.weight, torch.Size([710, 512]): False
vq_layer_2.embedding.weight, torch.Size([5, 512]): False
vq_layer_3.embedding.weight, torch.Size([6, 512]): False
vq_layer_4.embedding.weight, torch.Size([2, 512]): False
linear.weight, torch.Size([6, 512]): True
epoch: 0 avg_loss: 1.2959, acc: 0.4766 

epoch: 1 avg_loss: 0.7904, acc: 0.7089 

epoch: 2 avg_loss: 0.6863, acc: 0.7393 

epoch: 3 avg_loss: 0.6420, acc: 0.7560 

epoch: 4 avg_loss: 0.6139, acc: 0.7663 

epoch: 5 avg_loss: 0.5976, acc: 0.7714 

epoch: 6 avg_loss: 0.5825, acc: 0.7752 

epoch: 7 avg_loss: 0.5668, acc: 0.7858 

epoch: 8 avg_loss: 0.5652, acc: 0.7852 

epoch: 9 avg_loss: 0.5560, acc: 0.7846 

epoch: 10 avg_loss: 0.5443, acc: 0.7952 

epoch: 11 avg_loss: 0.5259, acc: 0.7987 

epoch: 12 avg_loss: 0.5326, acc: 0.7958 

epoch: 13 avg_loss: 0.5312, acc: 0.7907 

epoch: 14 avg_loss: 0.5215, acc: 0.7941 

epoch: 15 avg_loss: 0.5128, acc: 0.8056 

epoch: 16 avg_loss: 0.5055, acc: 0.8036 

epoch: 17 avg_loss: 0.4990, acc: 0.8073 

epoch: 18 avg_loss: 0.5045, acc: 0.8036 

epoch: 19 avg_loss: 0.4883, acc: 0.8102 

epoch: 20 avg_loss: 0.4937, acc: 0.8070 

epoch: 21 avg_loss: 0.4821, acc: 0.8162 

epoch: 22 avg_loss: 0.4838, acc: 0.8182 

epoch: 23 avg_loss: 0.4723, acc: 0.8136 

epoch: 24 avg_loss: 0.4740, acc: 0.8153 

epoch: 25 avg_loss: 0.4738, acc: 0.8173 

epoch: 26 avg_loss: 0.4734, acc: 0.8136 

epoch: 27 avg_loss: 0.4772, acc: 0.8084 

epoch: 28 avg_loss: 0.4662, acc: 0.8202 

epoch: 29 avg_loss: 0.4596, acc: 0.8219 

epoch: 30 avg_loss: 0.4604, acc: 0.8231 

epoch: 31 avg_loss: 0.4587, acc: 0.8225 

epoch: 32 avg_loss: 0.4630, acc: 0.8156 

epoch: 33 avg_loss: 0.4524, acc: 0.8248 

epoch: 34 avg_loss: 0.4541, acc: 0.8176 

epoch: 35 avg_loss: 0.4490, acc: 0.8297 

epoch: 36 avg_loss: 0.4532, acc: 0.8185 

epoch: 37 avg_loss: 0.4415, acc: 0.8334 

epoch: 38 avg_loss: 0.4369, acc: 0.8291 

epoch: 39 avg_loss: 0.4422, acc: 0.8268 

epoch: 40 avg_loss: 0.4338, acc: 0.8274 

epoch: 41 avg_loss: 0.4408, acc: 0.8308 

epoch: 42 avg_loss: 0.4414, acc: 0.8271 

epoch: 43 avg_loss: 0.4430, acc: 0.8248 

epoch: 44 avg_loss: 0.4311, acc: 0.8299 

epoch: 45 avg_loss: 0.4379, acc: 0.8219 

epoch: 46 avg_loss: 0.4251, acc: 0.8357 

epoch: 47 avg_loss: 0.4282, acc: 0.8348 

epoch: 48 avg_loss: 0.4240, acc: 0.8351 

epoch: 49 avg_loss: 0.4180, acc: 0.8374 

epoch: 50 avg_loss: 0.4159, acc: 0.8428 

epoch: 51 avg_loss: 0.4174, acc: 0.8400 

epoch: 52 avg_loss: 0.4203, acc: 0.8331 

epoch: 53 avg_loss: 0.4301, acc: 0.8279 

epoch: 54 avg_loss: 0.4251, acc: 0.8331 

epoch: 55 avg_loss: 0.4198, acc: 0.8328 

epoch: 56 avg_loss: 0.4196, acc: 0.8428 

epoch: 57 avg_loss: 0.4196, acc: 0.8342 

epoch: 58 avg_loss: 0.4136, acc: 0.8362 

epoch: 59 avg_loss: 0.4078, acc: 0.8400 

epoch: 60 avg_loss: 0.4069, acc: 0.8411 

epoch: 61 avg_loss: 0.4101, acc: 0.8437 

epoch: 62 avg_loss: 0.4147, acc: 0.8360 

epoch: 63 avg_loss: 0.4040, acc: 0.8417 

epoch: 64 avg_loss: 0.4015, acc: 0.8440 

epoch: 65 avg_loss: 0.4030, acc: 0.8391 

epoch: 66 avg_loss: 0.3987, acc: 0.8474 

epoch: 67 avg_loss: 0.4075, acc: 0.8457 

epoch: 68 avg_loss: 0.3995, acc: 0.8417 

epoch: 69 avg_loss: 0.3979, acc: 0.8457 

epoch: 70 avg_loss: 0.3962, acc: 0.8460 

epoch: 71 avg_loss: 0.3980, acc: 0.8426 

epoch: 72 avg_loss: 0.3975, acc: 0.8477 

epoch: 73 avg_loss: 0.4028, acc: 0.8400 

epoch: 74 avg_loss: 0.4010, acc: 0.8451 

epoch: 75 avg_loss: 0.3879, acc: 0.8514 

epoch: 76 avg_loss: 0.3922, acc: 0.8463 

epoch: 77 avg_loss: 0.4093, acc: 0.8368 

epoch: 78 avg_loss: 0.3972, acc: 0.8446 

epoch: 79 avg_loss: 0.3867, acc: 0.8494 

epoch: 80 avg_loss: 0.3899, acc: 0.8480 

epoch: 81 avg_loss: 0.3855, acc: 0.8517 

epoch: 82 avg_loss: 0.3925, acc: 0.8460 

epoch: 83 avg_loss: 0.3860, acc: 0.8506 

epoch: 84 avg_loss: 0.3928, acc: 0.8457 

epoch: 85 avg_loss: 0.3860, acc: 0.8509 

epoch: 86 avg_loss: 0.3865, acc: 0.8480 

epoch: 87 avg_loss: 0.3800, acc: 0.8546 

epoch: 88 avg_loss: 0.3783, acc: 0.8537 

epoch: 89 avg_loss: 0.3771, acc: 0.8540 

epoch: 90 avg_loss: 0.3834, acc: 0.8460 

epoch: 91 avg_loss: 0.3780, acc: 0.8549 

epoch: 92 avg_loss: 0.3849, acc: 0.8460 

epoch: 93 avg_loss: 0.3807, acc: 0.8520 

epoch: 94 avg_loss: 0.3723, acc: 0.8575 

epoch: 95 avg_loss: 0.3800, acc: 0.8552 

epoch: 96 avg_loss: 0.3736, acc: 0.8529 

epoch: 97 avg_loss: 0.3757, acc: 0.8492 

epoch: 98 avg_loss: 0.3760, acc: 0.8514 

epoch: 99 avg_loss: 0.3872, acc: 0.8492 

epoch: 100 avg_loss: 0.3722, acc: 0.8532 

epoch: 101 avg_loss: 0.3751, acc: 0.8532 

epoch: 102 avg_loss: 0.3732, acc: 0.8506 

epoch: 103 avg_loss: 0.3660, acc: 0.8603 

epoch: 104 avg_loss: 0.3699, acc: 0.8563 

epoch: 105 avg_loss: 0.3726, acc: 0.8566 

epoch: 106 avg_loss: 0.3723, acc: 0.8563 

epoch: 107 avg_loss: 0.3624, acc: 0.8578 

epoch: 108 avg_loss: 0.3746, acc: 0.8569 

epoch: 109 avg_loss: 0.3701, acc: 0.8557 

epoch: 110 avg_loss: 0.3630, acc: 0.8560 

epoch: 111 avg_loss: 0.3637, acc: 0.8621 

epoch: 112 avg_loss: 0.3640, acc: 0.8560 

epoch: 113 avg_loss: 0.3676, acc: 0.8583 

epoch: 114 avg_loss: 0.3756, acc: 0.8503 

epoch: 115 avg_loss: 0.3575, acc: 0.8635 

epoch: 116 avg_loss: 0.3604, acc: 0.8621 

epoch: 117 avg_loss: 0.3600, acc: 0.8566 

epoch: 118 avg_loss: 0.3609, acc: 0.8623 

epoch: 119 avg_loss: 0.3582, acc: 0.8603 

epoch: 120 avg_loss: 0.3574, acc: 0.8623 

epoch: 121 avg_loss: 0.3647, acc: 0.8566 

epoch: 122 avg_loss: 0.3689, acc: 0.8560 

epoch: 123 avg_loss: 0.3599, acc: 0.8618 

epoch: 124 avg_loss: 0.3608, acc: 0.8603 

epoch: 125 avg_loss: 0.3600, acc: 0.8615 

epoch: 126 avg_loss: 0.3573, acc: 0.8586 

epoch: 127 avg_loss: 0.3549, acc: 0.8626 

epoch: 128 avg_loss: 0.3529, acc: 0.8649 

epoch: 129 avg_loss: 0.3584, acc: 0.8592 

epoch: 130 avg_loss: 0.3580, acc: 0.8560 

epoch: 131 avg_loss: 0.3598, acc: 0.8615 

epoch: 132 avg_loss: 0.3523, acc: 0.8621 

epoch: 133 avg_loss: 0.3574, acc: 0.8603 

epoch: 134 avg_loss: 0.3545, acc: 0.8569 

epoch: 135 avg_loss: 0.3519, acc: 0.8644 

epoch: 136 avg_loss: 0.3566, acc: 0.8580 

epoch: 137 avg_loss: 0.3494, acc: 0.8629 

epoch: 138 avg_loss: 0.3560, acc: 0.8575 

epoch: 139 avg_loss: 0.3561, acc: 0.8632 

epoch: 140 avg_loss: 0.3551, acc: 0.8566 

epoch: 141 avg_loss: 0.3574, acc: 0.8580 

epoch: 142 avg_loss: 0.3517, acc: 0.8658 

epoch: 143 avg_loss: 0.3670, acc: 0.8492 

epoch: 144 avg_loss: 0.3536, acc: 0.8615 

epoch: 145 avg_loss: 0.3525, acc: 0.8626 

epoch: 146 avg_loss: 0.3513, acc: 0.8644 

epoch: 147 avg_loss: 0.3565, acc: 0.8609 

epoch: 148 avg_loss: 0.3491, acc: 0.8623 

epoch: 149 avg_loss: 0.3476, acc: 0.8632 

epoch: 150 avg_loss: 0.3609, acc: 0.8494 

epoch: 151 avg_loss: 0.3559, acc: 0.8601 

epoch: 152 avg_loss: 0.3516, acc: 0.8661 

epoch: 153 avg_loss: 0.3443, acc: 0.8675 

epoch: 154 avg_loss: 0.3419, acc: 0.8646 

epoch: 155 avg_loss: 0.3456, acc: 0.8655 

epoch: 156 avg_loss: 0.3450, acc: 0.8666 

epoch: 157 avg_loss: 0.3440, acc: 0.8684 

epoch: 158 avg_loss: 0.3415, acc: 0.8649 

epoch: 159 avg_loss: 0.3473, acc: 0.8692 

epoch: 160 avg_loss: 0.3464, acc: 0.8644 

epoch: 161 avg_loss: 0.3462, acc: 0.8649 

epoch: 162 avg_loss: 0.3432, acc: 0.8652 

epoch: 163 avg_loss: 0.3402, acc: 0.8621 

epoch: 164 avg_loss: 0.3443, acc: 0.8672 

epoch: 165 avg_loss: 0.3406, acc: 0.8675 

epoch: 166 avg_loss: 0.3452, acc: 0.8629 

epoch: 167 avg_loss: 0.3502, acc: 0.8638 

epoch: 168 avg_loss: 0.3448, acc: 0.8569 

epoch: 169 avg_loss: 0.3390, acc: 0.8707 

epoch: 170 avg_loss: 0.3359, acc: 0.8721 

epoch: 171 avg_loss: 0.3396, acc: 0.8644 

epoch: 172 avg_loss: 0.3445, acc: 0.8629 

epoch: 173 avg_loss: 0.3403, acc: 0.8672 

epoch: 174 avg_loss: 0.3379, acc: 0.8672 

epoch: 175 avg_loss: 0.3329, acc: 0.8681 

epoch: 176 avg_loss: 0.3360, acc: 0.8718 

epoch: 177 avg_loss: 0.3448, acc: 0.8669 

epoch: 178 avg_loss: 0.3379, acc: 0.8598 

epoch: 179 avg_loss: 0.3398, acc: 0.8692 

epoch: 180 avg_loss: 0.3364, acc: 0.8704 

epoch: 181 avg_loss: 0.3363, acc: 0.8701 

epoch: 182 avg_loss: 0.3403, acc: 0.8681 

epoch: 183 avg_loss: 0.3387, acc: 0.8727 

epoch: 184 avg_loss: 0.3353, acc: 0.8709 

epoch: 185 avg_loss: 0.3435, acc: 0.8646 

epoch: 186 avg_loss: 0.3347, acc: 0.8675 

epoch: 187 avg_loss: 0.3343, acc: 0.8678 

epoch: 188 avg_loss: 0.3374, acc: 0.8707 

epoch: 189 avg_loss: 0.3339, acc: 0.8707 

epoch: 190 avg_loss: 0.3351, acc: 0.8675 

epoch: 191 avg_loss: 0.3350, acc: 0.8689 

epoch: 192 avg_loss: 0.3334, acc: 0.8678 

epoch: 193 avg_loss: 0.3351, acc: 0.8698 

epoch: 194 avg_loss: 0.3353, acc: 0.8621 

epoch: 195 avg_loss: 0.3296, acc: 0.8715 

epoch: 196 avg_loss: 0.3370, acc: 0.8641 

epoch: 197 avg_loss: 0.3278, acc: 0.8761 

epoch: 198 avg_loss: 0.3343, acc: 0.8687 

epoch: 199 avg_loss: 0.3294, acc: 0.8730 

epoch: 200 avg_loss: 0.3311, acc: 0.8704 

epoch: 201 avg_loss: 0.3417, acc: 0.8609 

epoch: 202 avg_loss: 0.3332, acc: 0.8730 

epoch: 203 avg_loss: 0.3361, acc: 0.8661 

epoch: 204 avg_loss: 0.3263, acc: 0.8724 

epoch: 205 avg_loss: 0.3305, acc: 0.8715 

epoch: 206 avg_loss: 0.3404, acc: 0.8664 

epoch: 207 avg_loss: 0.3261, acc: 0.8704 

epoch: 208 avg_loss: 0.3308, acc: 0.8715 

epoch: 209 avg_loss: 0.3302, acc: 0.8666 

epoch: 210 avg_loss: 0.3277, acc: 0.8727 

epoch: 211 avg_loss: 0.3276, acc: 0.8744 

epoch: 212 avg_loss: 0.3241, acc: 0.8718 

epoch: 213 avg_loss: 0.3264, acc: 0.8764 

epoch: 214 avg_loss: 0.3256, acc: 0.8698 

epoch: 215 avg_loss: 0.3255, acc: 0.8735 

epoch: 216 avg_loss: 0.3285, acc: 0.8747 

epoch: 217 avg_loss: 0.3321, acc: 0.8632 

epoch: 218 avg_loss: 0.3264, acc: 0.8781 

epoch: 219 avg_loss: 0.3202, acc: 0.8747 

epoch: 220 avg_loss: 0.3235, acc: 0.8721 

epoch: 221 avg_loss: 0.3296, acc: 0.8655 

epoch: 222 avg_loss: 0.3280, acc: 0.8687 

epoch: 223 avg_loss: 0.3264, acc: 0.8724 

epoch: 224 avg_loss: 0.3213, acc: 0.8721 

epoch: 225 avg_loss: 0.3329, acc: 0.8666 

epoch: 226 avg_loss: 0.3266, acc: 0.8721 

epoch: 227 avg_loss: 0.3300, acc: 0.8712 

epoch: 228 avg_loss: 0.3185, acc: 0.8761 

epoch: 229 avg_loss: 0.3211, acc: 0.8764 

epoch: 230 avg_loss: 0.3181, acc: 0.8755 

epoch: 231 avg_loss: 0.3257, acc: 0.8735 

epoch: 232 avg_loss: 0.3255, acc: 0.8718 

epoch: 233 avg_loss: 0.3218, acc: 0.8744 

epoch: 234 avg_loss: 0.3264, acc: 0.8698 

epoch: 235 avg_loss: 0.3217, acc: 0.8712 

epoch: 236 avg_loss: 0.3182, acc: 0.8753 

epoch: 237 avg_loss: 0.3214, acc: 0.8787 

epoch: 238 avg_loss: 0.3228, acc: 0.8709 

epoch: 239 avg_loss: 0.3169, acc: 0.8770 

epoch: 240 avg_loss: 0.3159, acc: 0.8787 

epoch: 241 avg_loss: 0.3248, acc: 0.8727 

epoch: 242 avg_loss: 0.3221, acc: 0.8750 

epoch: 243 avg_loss: 0.3158, acc: 0.8735 

epoch: 244 avg_loss: 0.3176, acc: 0.8767 

epoch: 245 avg_loss: 0.3192, acc: 0.8770 

epoch: 246 avg_loss: 0.3307, acc: 0.8747 

epoch: 247 avg_loss: 0.3193, acc: 0.8744 

epoch: 248 avg_loss: 0.3209, acc: 0.8755 

epoch: 249 avg_loss: 0.3204, acc: 0.8750 

epoch: 250 avg_loss: 0.3159, acc: 0.8773 

epoch: 251 avg_loss: 0.3132, acc: 0.8796 

epoch: 252 avg_loss: 0.3247, acc: 0.8727 

epoch: 253 avg_loss: 0.3155, acc: 0.8775 

epoch: 254 avg_loss: 0.3166, acc: 0.8753 

epoch: 255 avg_loss: 0.3157, acc: 0.8773 

epoch: 256 avg_loss: 0.3129, acc: 0.8801 

epoch: 257 avg_loss: 0.3223, acc: 0.8724 

epoch: 258 avg_loss: 0.3170, acc: 0.8753 

epoch: 259 avg_loss: 0.3153, acc: 0.8747 

epoch: 260 avg_loss: 0.3226, acc: 0.8689 

epoch: 261 avg_loss: 0.3183, acc: 0.8747 

epoch: 262 avg_loss: 0.3146, acc: 0.8764 

epoch: 263 avg_loss: 0.3130, acc: 0.8758 

epoch: 264 avg_loss: 0.3167, acc: 0.8764 

epoch: 265 avg_loss: 0.3195, acc: 0.8750 

epoch: 266 avg_loss: 0.3167, acc: 0.8735 

epoch: 267 avg_loss: 0.3178, acc: 0.8790 

epoch: 268 avg_loss: 0.3196, acc: 0.8724 

epoch: 269 avg_loss: 0.3132, acc: 0.8773 

epoch: 270 avg_loss: 0.3202, acc: 0.8730 

epoch: 271 avg_loss: 0.3236, acc: 0.8718 

epoch: 272 avg_loss: 0.3123, acc: 0.8778 

epoch: 273 avg_loss: 0.3185, acc: 0.8747 

epoch: 274 avg_loss: 0.3090, acc: 0.8793 

epoch: 275 avg_loss: 0.3119, acc: 0.8732 

epoch: 276 avg_loss: 0.3153, acc: 0.8761 

epoch: 277 avg_loss: 0.3139, acc: 0.8747 

epoch: 278 avg_loss: 0.3123, acc: 0.8744 

epoch: 279 avg_loss: 0.3081, acc: 0.8798 

epoch: 280 avg_loss: 0.3131, acc: 0.8750 

epoch: 281 avg_loss: 0.3129, acc: 0.8738 

epoch: 282 avg_loss: 0.3105, acc: 0.8767 

epoch: 283 avg_loss: 0.3101, acc: 0.8781 

epoch: 284 avg_loss: 0.3172, acc: 0.8773 

epoch: 285 avg_loss: 0.3191, acc: 0.8712 

epoch: 286 avg_loss: 0.3148, acc: 0.8732 

epoch: 287 avg_loss: 0.3109, acc: 0.8793 

epoch: 288 avg_loss: 0.3106, acc: 0.8781 

epoch: 289 avg_loss: 0.3141, acc: 0.8741 

epoch: 290 avg_loss: 0.3113, acc: 0.8741 

epoch: 291 avg_loss: 0.3095, acc: 0.8778 

epoch: 292 avg_loss: 0.3138, acc: 0.8741 

epoch: 293 avg_loss: 0.3126, acc: 0.8761 

epoch: 294 avg_loss: 0.3112, acc: 0.8747 

epoch: 295 avg_loss: 0.3146, acc: 0.8701 

epoch: 296 avg_loss: 0.3166, acc: 0.8741 

epoch: 297 avg_loss: 0.3122, acc: 0.8770 

epoch: 298 avg_loss: 0.3084, acc: 0.8787 

epoch: 299 avg_loss: 0.3172, acc: 0.8684 

epoch: 300 avg_loss: 0.3114, acc: 0.8750 

epoch: 301 avg_loss: 0.3146, acc: 0.8750 

epoch: 302 avg_loss: 0.3093, acc: 0.8784 

epoch: 303 avg_loss: 0.3072, acc: 0.8753 

epoch: 304 avg_loss: 0.3053, acc: 0.8773 

epoch: 305 avg_loss: 0.3101, acc: 0.8790 

epoch: 306 avg_loss: 0.3075, acc: 0.8778 

epoch: 307 avg_loss: 0.3093, acc: 0.8793 

epoch: 308 avg_loss: 0.3085, acc: 0.8796 

epoch: 309 avg_loss: 0.3062, acc: 0.8784 

epoch: 310 avg_loss: 0.3085, acc: 0.8798 

epoch: 311 avg_loss: 0.3061, acc: 0.8787 

epoch: 312 avg_loss: 0.3090, acc: 0.8735 

epoch: 313 avg_loss: 0.3067, acc: 0.8790 

epoch: 314 avg_loss: 0.3034, acc: 0.8830 

epoch: 315 avg_loss: 0.3072, acc: 0.8781 

epoch: 316 avg_loss: 0.3065, acc: 0.8761 

epoch: 317 avg_loss: 0.3076, acc: 0.8744 

epoch: 318 avg_loss: 0.3034, acc: 0.8773 

epoch: 319 avg_loss: 0.3036, acc: 0.8810 

epoch: 320 avg_loss: 0.3076, acc: 0.8755 

epoch: 321 avg_loss: 0.3043, acc: 0.8787 

epoch: 322 avg_loss: 0.3127, acc: 0.8692 

epoch: 323 avg_loss: 0.3141, acc: 0.8712 

epoch: 324 avg_loss: 0.3124, acc: 0.8741 

epoch: 325 avg_loss: 0.3023, acc: 0.8833 

epoch: 326 avg_loss: 0.3055, acc: 0.8778 

epoch: 327 avg_loss: 0.3034, acc: 0.8781 

epoch: 328 avg_loss: 0.3015, acc: 0.8839 

epoch: 329 avg_loss: 0.2978, acc: 0.8859 

epoch: 330 avg_loss: 0.3001, acc: 0.8816 

epoch: 331 avg_loss: 0.3018, acc: 0.8804 

epoch: 332 avg_loss: 0.3019, acc: 0.8818 

epoch: 333 avg_loss: 0.2995, acc: 0.8824 

epoch: 334 avg_loss: 0.3003, acc: 0.8813 

epoch: 335 avg_loss: 0.3016, acc: 0.8796 

epoch: 336 avg_loss: 0.3030, acc: 0.8804 

epoch: 337 avg_loss: 0.2972, acc: 0.8830 

epoch: 338 avg_loss: 0.3010, acc: 0.8773 

epoch: 339 avg_loss: 0.3029, acc: 0.8804 

epoch: 340 avg_loss: 0.3035, acc: 0.8781 

epoch: 341 avg_loss: 0.3068, acc: 0.8770 

epoch: 342 avg_loss: 0.3056, acc: 0.8818 

epoch: 343 avg_loss: 0.2972, acc: 0.8813 

epoch: 344 avg_loss: 0.2998, acc: 0.8816 

epoch: 345 avg_loss: 0.2985, acc: 0.8807 

epoch: 346 avg_loss: 0.3001, acc: 0.8753 

epoch: 347 avg_loss: 0.3041, acc: 0.8790 

epoch: 348 avg_loss: 0.3056, acc: 0.8793 

epoch: 349 avg_loss: 0.3054, acc: 0.8793 

epoch: 350 avg_loss: 0.3008, acc: 0.8775 
