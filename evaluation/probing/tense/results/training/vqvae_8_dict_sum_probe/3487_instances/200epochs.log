
number of params: 3078 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/tense/results/training/vqvae_8_dict_sum_probe/3487_instances/200epochs.png', log_path='evaluation/probing/tense/results/training/vqvae_8_dict_sum_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7f042d3b7090>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_8_dict_sum_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (linear): Linear(in_features=512, out_features=6, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/tense/results/training/vqvae_8_dict_sum_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (linear_9): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/tense/results/training/vqvae_8_dict_sum_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2tense', trndata='evaluation/probing/tense/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/tense/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([512, 512]): False
linear_2.bias, torch.Size([512]): False
linear_3.weight, torch.Size([512, 512]): False
linear_3.bias, torch.Size([512]): False
linear_4.weight, torch.Size([512, 512]): False
linear_4.bias, torch.Size([512]): False
linear_5.weight, torch.Size([512, 512]): False
linear_5.bias, torch.Size([512]): False
linear_6.weight, torch.Size([512, 512]): False
linear_6.bias, torch.Size([512]): False
linear_7.weight, torch.Size([512, 512]): False
linear_7.bias, torch.Size([512]): False
linear_8.weight, torch.Size([512, 512]): False
linear_8.bias, torch.Size([512]): False
vq_layer_2.embedding.weight, torch.Size([32, 512]): False
vq_layer_3.embedding.weight, torch.Size([32, 512]): False
vq_layer_4.embedding.weight, torch.Size([32, 512]): False
vq_layer_5.embedding.weight, torch.Size([32, 512]): False
vq_layer_6.embedding.weight, torch.Size([32, 512]): False
vq_layer_7.embedding.weight, torch.Size([32, 512]): False
vq_layer_8.embedding.weight, torch.Size([32, 512]): False
linear.weight, torch.Size([6, 512]): True
linear.bias, torch.Size([6]): True
epoch: 0 avg_loss: 1.5649, acc: 0.3390 

epoch: 1 avg_loss: 1.3389, acc: 0.4087 

epoch: 2 avg_loss: 1.2889, acc: 0.4491 

epoch: 3 avg_loss: 1.3011, acc: 0.4348 

epoch: 4 avg_loss: 1.2889, acc: 0.4439 

epoch: 5 avg_loss: 1.2833, acc: 0.4339 

epoch: 6 avg_loss: 1.2720, acc: 0.4184 

epoch: 7 avg_loss: 1.2730, acc: 0.4342 

epoch: 8 avg_loss: 1.2666, acc: 0.4425 

epoch: 9 avg_loss: 1.2590, acc: 0.4299 

epoch: 10 avg_loss: 1.2764, acc: 0.4087 

epoch: 11 avg_loss: 1.2712, acc: 0.4264 

epoch: 12 avg_loss: 1.2756, acc: 0.4465 

epoch: 13 avg_loss: 1.2911, acc: 0.3980 

epoch: 14 avg_loss: 1.2619, acc: 0.4167 

epoch: 15 avg_loss: 1.2753, acc: 0.4262 

epoch: 16 avg_loss: 1.2794, acc: 0.4230 

epoch: 17 avg_loss: 1.2568, acc: 0.4408 

epoch: 18 avg_loss: 1.2630, acc: 0.4259 

epoch: 19 avg_loss: 1.2680, acc: 0.4193 

epoch: 20 avg_loss: 1.2965, acc: 0.3909 

epoch: 21 avg_loss: 1.2888, acc: 0.4305 

epoch: 22 avg_loss: 1.2679, acc: 0.4101 

epoch: 23 avg_loss: 1.2692, acc: 0.4494 

epoch: 24 avg_loss: 1.2704, acc: 0.4273 

epoch: 25 avg_loss: 1.2742, acc: 0.3963 

epoch: 26 avg_loss: 1.2838, acc: 0.4474 

epoch: 27 avg_loss: 1.2916, acc: 0.4127 

epoch: 28 avg_loss: 1.2830, acc: 0.3740 

epoch: 29 avg_loss: 1.2820, acc: 0.4348 

epoch: 30 avg_loss: 1.2887, acc: 0.3783 

epoch: 31 avg_loss: 1.2624, acc: 0.4336 

epoch: 32 avg_loss: 1.2566, acc: 0.4310 

epoch: 33 avg_loss: 1.2775, acc: 0.4236 

epoch: 34 avg_loss: 1.2676, acc: 0.4284 

epoch: 35 avg_loss: 1.2813, acc: 0.4118 

epoch: 36 avg_loss: 1.2919, acc: 0.3966 

epoch: 37 avg_loss: 1.2560, acc: 0.4098 

epoch: 38 avg_loss: 1.2646, acc: 0.4459 

epoch: 39 avg_loss: 1.2519, acc: 0.4296 

epoch: 40 avg_loss: 1.2508, acc: 0.4388 

epoch: 41 avg_loss: 1.2561, acc: 0.4279 

epoch: 42 avg_loss: 1.2690, acc: 0.4256 

epoch: 43 avg_loss: 1.2602, acc: 0.4279 

epoch: 44 avg_loss: 1.2660, acc: 0.4459 

epoch: 45 avg_loss: 1.2659, acc: 0.4276 

epoch: 46 avg_loss: 1.2578, acc: 0.4273 

epoch: 47 avg_loss: 1.2569, acc: 0.4316 

epoch: 48 avg_loss: 1.2568, acc: 0.4471 

epoch: 49 avg_loss: 1.2661, acc: 0.4224 

epoch: 50 avg_loss: 1.2725, acc: 0.3940 

epoch: 51 avg_loss: 1.2753, acc: 0.4213 

epoch: 52 avg_loss: 1.2631, acc: 0.4448 

epoch: 53 avg_loss: 1.2623, acc: 0.4442 

epoch: 54 avg_loss: 1.2553, acc: 0.4379 

epoch: 55 avg_loss: 1.2793, acc: 0.4158 

epoch: 56 avg_loss: 1.2639, acc: 0.4422 

epoch: 57 avg_loss: 1.2643, acc: 0.4345 

epoch: 58 avg_loss: 1.2642, acc: 0.4089 

epoch: 59 avg_loss: 1.2548, acc: 0.4138 

epoch: 60 avg_loss: 1.2882, acc: 0.4345 

epoch: 61 avg_loss: 1.2836, acc: 0.4150 

epoch: 62 avg_loss: 1.2559, acc: 0.4230 

epoch: 63 avg_loss: 1.2631, acc: 0.4084 

epoch: 64 avg_loss: 1.2569, acc: 0.4459 

epoch: 65 avg_loss: 1.2557, acc: 0.4330 

epoch: 66 avg_loss: 1.2858, acc: 0.4121 

epoch: 67 avg_loss: 1.2544, acc: 0.4333 

epoch: 68 avg_loss: 1.2652, acc: 0.4284 

epoch: 69 avg_loss: 1.2745, acc: 0.4058 

epoch: 70 avg_loss: 1.2592, acc: 0.4267 

epoch: 71 avg_loss: 1.2755, acc: 0.4132 

epoch: 72 avg_loss: 1.2537, acc: 0.4253 

epoch: 73 avg_loss: 1.2842, acc: 0.4259 

epoch: 74 avg_loss: 1.2738, acc: 0.3972 

epoch: 75 avg_loss: 1.2499, acc: 0.4273 

epoch: 76 avg_loss: 1.2587, acc: 0.4330 

epoch: 77 avg_loss: 1.2444, acc: 0.4477 

epoch: 78 avg_loss: 1.2702, acc: 0.4009 

epoch: 79 avg_loss: 1.2736, acc: 0.4270 

epoch: 80 avg_loss: 1.2710, acc: 0.4107 

epoch: 81 avg_loss: 1.2628, acc: 0.4287 

epoch: 82 avg_loss: 1.2694, acc: 0.4207 

epoch: 83 avg_loss: 1.2963, acc: 0.4190 

epoch: 84 avg_loss: 1.2660, acc: 0.4371 

epoch: 85 avg_loss: 1.2544, acc: 0.4158 

epoch: 86 avg_loss: 1.2626, acc: 0.4482 

epoch: 87 avg_loss: 1.2750, acc: 0.4092 

epoch: 88 avg_loss: 1.2757, acc: 0.4299 

epoch: 89 avg_loss: 1.2740, acc: 0.4227 

epoch: 90 avg_loss: 1.2628, acc: 0.4178 

epoch: 91 avg_loss: 1.2833, acc: 0.3980 

epoch: 92 avg_loss: 1.2760, acc: 0.4127 

epoch: 93 avg_loss: 1.2645, acc: 0.4213 

epoch: 94 avg_loss: 1.2541, acc: 0.4408 

epoch: 95 avg_loss: 1.2681, acc: 0.4411 

epoch: 96 avg_loss: 1.2592, acc: 0.4256 

epoch: 97 avg_loss: 1.2504, acc: 0.4173 

epoch: 98 avg_loss: 1.2553, acc: 0.4307 

epoch: 99 avg_loss: 1.2638, acc: 0.4316 

epoch: 100 avg_loss: 1.2611, acc: 0.4253 

epoch: 101 avg_loss: 1.2824, acc: 0.4029 

epoch: 102 avg_loss: 1.2589, acc: 0.4356 

epoch: 103 avg_loss: 1.2818, acc: 0.4342 

epoch: 104 avg_loss: 1.2612, acc: 0.4276 

epoch: 105 avg_loss: 1.2743, acc: 0.4190 

epoch: 106 avg_loss: 1.2696, acc: 0.4227 

epoch: 107 avg_loss: 1.2545, acc: 0.4328 

epoch: 108 avg_loss: 1.2726, acc: 0.4270 

epoch: 109 avg_loss: 1.2687, acc: 0.3983 

epoch: 110 avg_loss: 1.2783, acc: 0.4313 

epoch: 111 avg_loss: 1.2826, acc: 0.3834 

epoch: 112 avg_loss: 1.2667, acc: 0.4319 

epoch: 113 avg_loss: 1.2677, acc: 0.4170 

epoch: 114 avg_loss: 1.2576, acc: 0.4333 

epoch: 115 avg_loss: 1.2739, acc: 0.4474 

epoch: 116 avg_loss: 1.2637, acc: 0.4382 

epoch: 117 avg_loss: 1.2729, acc: 0.4081 

epoch: 118 avg_loss: 1.2624, acc: 0.4178 

epoch: 119 avg_loss: 1.2600, acc: 0.4353 

epoch: 120 avg_loss: 1.2655, acc: 0.4537 

epoch: 121 avg_loss: 1.2592, acc: 0.4147 

epoch: 122 avg_loss: 1.2650, acc: 0.4181 

epoch: 123 avg_loss: 1.2442, acc: 0.4284 

epoch: 124 avg_loss: 1.2541, acc: 0.4153 

epoch: 125 avg_loss: 1.2688, acc: 0.4416 

epoch: 126 avg_loss: 1.2606, acc: 0.4462 

epoch: 127 avg_loss: 1.2696, acc: 0.4213 

epoch: 128 avg_loss: 1.2846, acc: 0.4147 

epoch: 129 avg_loss: 1.2637, acc: 0.4184 

epoch: 130 avg_loss: 1.2601, acc: 0.4353 

epoch: 131 avg_loss: 1.2798, acc: 0.4391 

epoch: 132 avg_loss: 1.2603, acc: 0.4072 

epoch: 133 avg_loss: 1.2592, acc: 0.4282 

epoch: 134 avg_loss: 1.2928, acc: 0.4468 

epoch: 135 avg_loss: 1.2683, acc: 0.4227 

epoch: 136 avg_loss: 1.2710, acc: 0.4110 

epoch: 137 avg_loss: 1.2573, acc: 0.4425 

epoch: 138 avg_loss: 1.2687, acc: 0.4436 

epoch: 139 avg_loss: 1.2531, acc: 0.4259 

epoch: 140 avg_loss: 1.2492, acc: 0.4316 

epoch: 141 avg_loss: 1.2644, acc: 0.4411 

epoch: 142 avg_loss: 1.2677, acc: 0.3935 

epoch: 143 avg_loss: 1.2749, acc: 0.4425 

epoch: 144 avg_loss: 1.2532, acc: 0.4213 

epoch: 145 avg_loss: 1.2650, acc: 0.4204 

epoch: 146 avg_loss: 1.2728, acc: 0.4055 

epoch: 147 avg_loss: 1.2822, acc: 0.4405 

epoch: 148 avg_loss: 1.2850, acc: 0.3986 

epoch: 149 avg_loss: 1.2868, acc: 0.4006 

epoch: 150 avg_loss: 1.2825, acc: 0.4198 

epoch: 151 avg_loss: 1.2483, acc: 0.4230 

epoch: 152 avg_loss: 1.2536, acc: 0.4350 

epoch: 153 avg_loss: 1.2549, acc: 0.4299 

epoch: 154 avg_loss: 1.2574, acc: 0.4264 

epoch: 155 avg_loss: 1.2486, acc: 0.4373 

epoch: 156 avg_loss: 1.2792, acc: 0.4436 

epoch: 157 avg_loss: 1.2777, acc: 0.4247 

epoch: 158 avg_loss: 1.2632, acc: 0.4095 

epoch: 159 avg_loss: 1.2702, acc: 0.4267 

epoch: 160 avg_loss: 1.2960, acc: 0.3843 

epoch: 161 avg_loss: 1.2454, acc: 0.4534 

epoch: 162 avg_loss: 1.2693, acc: 0.3849 

epoch: 163 avg_loss: 1.2491, acc: 0.4319 

epoch: 164 avg_loss: 1.2847, acc: 0.4505 

epoch: 165 avg_loss: 1.2763, acc: 0.3960 

epoch: 166 avg_loss: 1.2657, acc: 0.4339 

epoch: 167 avg_loss: 1.2683, acc: 0.4150 

epoch: 168 avg_loss: 1.2608, acc: 0.4193 

epoch: 169 avg_loss: 1.2884, acc: 0.4239 

epoch: 170 avg_loss: 1.2576, acc: 0.4345 

epoch: 171 avg_loss: 1.2618, acc: 0.4408 

epoch: 172 avg_loss: 1.2597, acc: 0.4305 

epoch: 173 avg_loss: 1.2512, acc: 0.4448 

epoch: 174 avg_loss: 1.2542, acc: 0.4362 

epoch: 175 avg_loss: 1.2654, acc: 0.4425 

epoch: 176 avg_loss: 1.2522, acc: 0.4408 

epoch: 177 avg_loss: 1.2559, acc: 0.4279 

epoch: 178 avg_loss: 1.2530, acc: 0.4442 

epoch: 179 avg_loss: 1.2751, acc: 0.4046 

epoch: 180 avg_loss: 1.2742, acc: 0.4273 

epoch: 181 avg_loss: 1.2624, acc: 0.4026 

epoch: 182 avg_loss: 1.2590, acc: 0.4362 

epoch: 183 avg_loss: 1.2543, acc: 0.4373 

epoch: 184 avg_loss: 1.2589, acc: 0.4284 

epoch: 185 avg_loss: 1.2602, acc: 0.4201 

epoch: 186 avg_loss: 1.2475, acc: 0.4345 

epoch: 187 avg_loss: 1.2708, acc: 0.4112 

epoch: 188 avg_loss: 1.2615, acc: 0.4382 

epoch: 189 avg_loss: 1.2617, acc: 0.4319 

epoch: 190 avg_loss: 1.2632, acc: 0.4316 

epoch: 191 avg_loss: 1.2789, acc: 0.4319 

epoch: 192 avg_loss: 1.2851, acc: 0.4227 

epoch: 193 avg_loss: 1.2506, acc: 0.4494 

epoch: 194 avg_loss: 1.2645, acc: 0.4224 

epoch: 195 avg_loss: 1.2612, acc: 0.4439 

epoch: 196 avg_loss: 1.2422, acc: 0.4474 

epoch: 197 avg_loss: 1.2540, acc: 0.4431 

epoch: 198 avg_loss: 1.2692, acc: 0.4110 

epoch: 199 avg_loss: 1.3071, acc: 0.4110 
