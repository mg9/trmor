
number of params: 3078 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=100, fig_path='evaluation/probing/tense/results/training/vqvae_7_2_probe/8145_instances/100epochs.png', log_path='evaluation/probing/tense/results/training/vqvae_7_2_probe/8145_instances/100epochs.log', logger=<common.utils.Logger object at 0x7fd5af428ed0>, lr=0.001, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_7_2_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(35, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(2000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (linear): Linear(in_features=512, out_features=6, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/tense/results/training/vqvae_7_2_probe/8145_instances/', nh=512, ni=256, num_dicts=7, nz=512, opt='Adam', orddict_emb_num=100, pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(35, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=320, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(2000, 320)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(35, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=35, bias=False)
    (loss): CrossEntropyLoss()
  )
), rootdict_emb_dim=320, rootdict_emb_num=2000, save_path='evaluation/probing/tense/results/training/vqvae_7_2_probe/8145_instances/100epochs.pt', seq_to_no_pad='surface', task='surf2tense', trndata='model/vqvae/data/trmor2018.uniquesurfs.verbs.uniquerooted.trn.txt', trnsize=8145, tstdata='model/vqvae/data/trmor2018.uniquesurfs.verbs.seenroots.val.txt', tstsize=2412, valdata='model/vqvae/data/trmor2018.uniquesurfs.verbs.seenroots.val.txt', valsize=2412)

encoder.embed.weight, torch.Size([35, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_root.weight, torch.Size([320, 512]): False
linear_root.bias, torch.Size([320]): False
vq_layer_root.embedding.weight, torch.Size([2000, 320]): False
ord_linears.0.weight, torch.Size([32, 512]): False
ord_linears.0.bias, torch.Size([32]): False
ord_linears.1.weight, torch.Size([32, 512]): False
ord_linears.1.bias, torch.Size([32]): False
ord_linears.2.weight, torch.Size([32, 512]): False
ord_linears.2.bias, torch.Size([32]): False
ord_linears.3.weight, torch.Size([32, 512]): False
ord_linears.3.bias, torch.Size([32]): False
ord_linears.4.weight, torch.Size([32, 512]): False
ord_linears.4.bias, torch.Size([32]): False
ord_linears.5.weight, torch.Size([32, 512]): False
ord_linears.5.bias, torch.Size([32]): False
ord_vq_layers.0.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.1.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.2.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.3.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.4.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.5.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([6, 512]): True
linear.bias, torch.Size([6]): True
epoch: 0 avg_loss: 0.9203, acc: 0.6814 
val --- avg_loss: 0.7095, acc: 0.7857  
update best loss 

epoch: 1 avg_loss: 0.4928, acc: 0.8451 
val --- avg_loss: 0.5158, acc: 0.8308  
update best loss 

epoch: 2 avg_loss: 0.4089, acc: 0.8708 
val --- avg_loss: 0.4908, acc: 0.8354  
update best loss 

epoch: 3 avg_loss: 0.3702, acc: 0.8769 
val --- avg_loss: 0.4535, acc: 0.8512  
update best loss 

epoch: 4 avg_loss: 0.3459, acc: 0.8800 
val --- avg_loss: 0.4351, acc: 0.8557  
update best loss 

epoch: 5 avg_loss: 0.3280, acc: 0.8841 
val --- avg_loss: 0.4485, acc: 0.8536  

epoch: 6 avg_loss: 0.3189, acc: 0.8880 
val --- avg_loss: 0.4332, acc: 0.8586  
update best loss 

epoch: 7 avg_loss: 0.3088, acc: 0.8884 
val --- avg_loss: 0.4275, acc: 0.8541  
update best loss 

epoch: 8 avg_loss: 0.3049, acc: 0.8904 
val --- avg_loss: 0.4226, acc: 0.8557  
update best loss 

epoch: 9 avg_loss: 0.2977, acc: 0.8924 
val --- avg_loss: 0.4115, acc: 0.8611  
update best loss 

epoch: 10 avg_loss: 0.2951, acc: 0.8936 
val --- avg_loss: 0.4612, acc: 0.8541  

epoch: 11 avg_loss: 0.2895, acc: 0.8937 
val --- avg_loss: 0.4606, acc: 0.8607  

epoch: 12 avg_loss: 0.2835, acc: 0.8955 
val --- avg_loss: 0.4020, acc: 0.8648  
update best loss 

epoch: 13 avg_loss: 0.2807, acc: 0.8955 
val --- avg_loss: 0.4385, acc: 0.8624  

epoch: 14 avg_loss: 0.2784, acc: 0.8954 
val --- avg_loss: 0.4406, acc: 0.8669  

epoch: 15 avg_loss: 0.2775, acc: 0.8966 
val --- avg_loss: 0.4374, acc: 0.8648  

epoch: 16 avg_loss: 0.2768, acc: 0.8963 
val --- avg_loss: 0.4332, acc: 0.8694  

epoch: 17 avg_loss: 0.2716, acc: 0.8965 
val --- avg_loss: 0.4071, acc: 0.8744  

epoch: 18 avg_loss: 0.2694, acc: 0.8991 
val --- avg_loss: 0.4391, acc: 0.8673  

epoch: 19 avg_loss: 0.2689, acc: 0.8979 
val --- avg_loss: 0.4130, acc: 0.8731  

epoch: 20 avg_loss: 0.2686, acc: 0.8999 
val --- avg_loss: 0.4197, acc: 0.8690  

epoch: 21 avg_loss: 0.2651, acc: 0.8994 
val --- avg_loss: 0.4437, acc: 0.8669  

epoch: 22 avg_loss: 0.2653, acc: 0.8991 
val --- avg_loss: 0.4263, acc: 0.8731  

epoch: 23 avg_loss: 0.2650, acc: 0.9017 
val --- avg_loss: 0.4628, acc: 0.8628  

epoch: 24 avg_loss: 0.2643, acc: 0.9014 
val --- avg_loss: 0.4218, acc: 0.8690  

epoch: 25 avg_loss: 0.2628, acc: 0.9007 
val --- avg_loss: 0.4333, acc: 0.8661  

epoch: 26 avg_loss: 0.2608, acc: 0.9025 
val --- avg_loss: 0.4132, acc: 0.8715  

epoch: 27 avg_loss: 0.2573, acc: 0.9017 
val --- avg_loss: 0.4667, acc: 0.8648  

epoch: 28 avg_loss: 0.2588, acc: 0.9004 
val --- avg_loss: 0.4275, acc: 0.8706  

epoch: 29 avg_loss: 0.2595, acc: 0.9044 
val --- avg_loss: 0.4470, acc: 0.8686  

epoch: 30 avg_loss: 0.2584, acc: 0.9017 
val --- avg_loss: 0.4353, acc: 0.8694  

epoch: 31 avg_loss: 0.2565, acc: 0.9028 
val --- avg_loss: 0.4426, acc: 0.8702  

epoch: 32 avg_loss: 0.2557, acc: 0.8996 
val --- avg_loss: 0.4209, acc: 0.8735  

epoch: 33 avg_loss: 0.2563, acc: 0.9025 
val --- avg_loss: 0.4434, acc: 0.8686  

epoch: 34 avg_loss: 0.2563, acc: 0.9010 
val --- avg_loss: 0.4277, acc: 0.8735  

epoch: 35 avg_loss: 0.2518, acc: 0.9040 
val --- avg_loss: 0.4468, acc: 0.8673  

epoch: 36 avg_loss: 0.2548, acc: 0.9048 
val --- avg_loss: 0.4440, acc: 0.8727  

epoch: 37 avg_loss: 0.2504, acc: 0.9037 
val --- avg_loss: 0.4116, acc: 0.8735  

epoch: 38 avg_loss: 0.2531, acc: 0.9021 
val --- avg_loss: 0.4561, acc: 0.8702  

epoch: 39 avg_loss: 0.2556, acc: 0.9004 
val --- avg_loss: 0.4343, acc: 0.8644  

epoch: 40 avg_loss: 0.2507, acc: 0.9048 
val --- avg_loss: 0.4288, acc: 0.8648  

epoch: 41 avg_loss: 0.2505, acc: 0.9035 
val --- avg_loss: 0.4510, acc: 0.8715  

epoch: 42 avg_loss: 0.2493, acc: 0.9036 
val --- avg_loss: 0.4408, acc: 0.8690  

epoch: 43 avg_loss: 0.2492, acc: 0.9055 
val --- avg_loss: 0.4104, acc: 0.8686  

epoch: 44 avg_loss: 0.2487, acc: 0.9060 
val --- avg_loss: 0.4515, acc: 0.8723  

epoch: 45 avg_loss: 0.2482, acc: 0.9050 
val --- avg_loss: 0.4164, acc: 0.8706  

epoch: 46 avg_loss: 0.2483, acc: 0.9057 
val --- avg_loss: 0.4374, acc: 0.8702  

epoch: 47 avg_loss: 0.2486, acc: 0.9061 
val --- avg_loss: 0.4336, acc: 0.8690  

epoch: 48 avg_loss: 0.2457, acc: 0.9057 
val --- avg_loss: 0.4437, acc: 0.8677  

epoch: 49 avg_loss: 0.2457, acc: 0.9053 
val --- avg_loss: 0.4269, acc: 0.8765  

epoch: 50 avg_loss: 0.2472, acc: 0.9047 
val --- avg_loss: 0.4346, acc: 0.8711  

epoch: 51 avg_loss: 0.2454, acc: 0.9037 
val --- avg_loss: 0.4374, acc: 0.8665  

epoch: 52 avg_loss: 0.2437, acc: 0.9066 
val --- avg_loss: 0.4281, acc: 0.8752  

epoch: 53 avg_loss: 0.2427, acc: 0.9083 
val --- avg_loss: 0.4398, acc: 0.8735  

epoch: 54 avg_loss: 0.2457, acc: 0.9063 
val --- avg_loss: 0.4469, acc: 0.8702  

epoch: 55 avg_loss: 0.2433, acc: 0.9077 
val --- avg_loss: 0.4307, acc: 0.8731  

epoch: 56 avg_loss: 0.2436, acc: 0.9077 
val --- avg_loss: 0.4198, acc: 0.8706  

epoch: 57 avg_loss: 0.2442, acc: 0.9055 
val --- avg_loss: 0.4600, acc: 0.8702  

epoch: 58 avg_loss: 0.2384, acc: 0.9101 
val --- avg_loss: 0.4662, acc: 0.8690  

epoch: 59 avg_loss: 0.2425, acc: 0.9074 
val --- avg_loss: 0.4487, acc: 0.8731  

epoch: 60 avg_loss: 0.2409, acc: 0.9071 
val --- avg_loss: 0.4632, acc: 0.8731  

epoch: 61 avg_loss: 0.2394, acc: 0.9080 
val --- avg_loss: 0.4355, acc: 0.8711  

epoch: 62 avg_loss: 0.2429, acc: 0.9058 
val --- avg_loss: 0.4477, acc: 0.8669  

epoch: 63 avg_loss: 0.2397, acc: 0.9076 
val --- avg_loss: 0.4396, acc: 0.8727  

epoch: 64 avg_loss: 0.2405, acc: 0.9077 
val --- avg_loss: 0.4563, acc: 0.8735  

epoch: 65 avg_loss: 0.2394, acc: 0.9091 
val --- avg_loss: 0.4577, acc: 0.8727  

epoch: 66 avg_loss: 0.2405, acc: 0.9074 
val --- avg_loss: 0.4362, acc: 0.8711  

epoch: 67 avg_loss: 0.2397, acc: 0.9101 
val --- avg_loss: 0.4606, acc: 0.8719  

epoch: 68 avg_loss: 0.2366, acc: 0.9094 
val --- avg_loss: 0.4663, acc: 0.8719  

epoch: 69 avg_loss: 0.2397, acc: 0.9088 
val --- avg_loss: 0.4360, acc: 0.8752  

epoch: 70 avg_loss: 0.2361, acc: 0.9077 
val --- avg_loss: 0.4547, acc: 0.8711  

epoch: 71 avg_loss: 0.2373, acc: 0.9066 
val --- avg_loss: 0.4683, acc: 0.8648  

epoch: 72 avg_loss: 0.2348, acc: 0.9084 
val --- avg_loss: 0.4257, acc: 0.8727  

epoch: 73 avg_loss: 0.2368, acc: 0.9101 
val --- avg_loss: 0.4427, acc: 0.8756  

epoch: 74 avg_loss: 0.2364, acc: 0.9082 
val --- avg_loss: 0.4392, acc: 0.8731  

epoch: 75 avg_loss: 0.2385, acc: 0.9068 
val --- avg_loss: 0.4417, acc: 0.8706  

epoch: 76 avg_loss: 0.2376, acc: 0.9079 
val --- avg_loss: 0.4387, acc: 0.8752  

epoch: 77 avg_loss: 0.2338, acc: 0.9095 
val --- avg_loss: 0.4934, acc: 0.8657  

epoch: 78 avg_loss: 0.2400, acc: 0.9087 
val --- avg_loss: 0.4452, acc: 0.8740  

epoch: 79 avg_loss: 0.2354, acc: 0.9071 
val --- avg_loss: 0.4354, acc: 0.8706  

epoch: 80 avg_loss: 0.2338, acc: 0.9110 
val --- avg_loss: 0.4339, acc: 0.8752  

epoch: 81 avg_loss: 0.2327, acc: 0.9094 
val --- avg_loss: 0.4559, acc: 0.8752  

epoch: 82 avg_loss: 0.2336, acc: 0.9091 
val --- avg_loss: 0.4208, acc: 0.8744  

epoch: 83 avg_loss: 0.2340, acc: 0.9107 
val --- avg_loss: 0.4392, acc: 0.8719  

epoch: 84 avg_loss: 0.2362, acc: 0.9096 
val --- avg_loss: 0.4301, acc: 0.8731  

epoch: 85 avg_loss: 0.2323, acc: 0.9100 
val --- avg_loss: 0.4647, acc: 0.8727  

epoch: 86 avg_loss: 0.2313, acc: 0.9107 
val --- avg_loss: 0.4445, acc: 0.8748  

epoch: 87 avg_loss: 0.2333, acc: 0.9109 
val --- avg_loss: 0.4518, acc: 0.8719  

epoch: 88 avg_loss: 0.2319, acc: 0.9100 
val --- avg_loss: 0.4395, acc: 0.8735  

epoch: 89 avg_loss: 0.2327, acc: 0.9101 
val --- avg_loss: 0.4582, acc: 0.8711  

epoch: 90 avg_loss: 0.2326, acc: 0.9105 
val --- avg_loss: 0.4810, acc: 0.8702  

epoch: 91 avg_loss: 0.2353, acc: 0.9101 
val --- avg_loss: 0.5073, acc: 0.8657  

epoch: 92 avg_loss: 0.2307, acc: 0.9095 
val --- avg_loss: 0.4665, acc: 0.8706  

epoch: 93 avg_loss: 0.2323, acc: 0.9106 
val --- avg_loss: 0.4579, acc: 0.8702  

epoch: 94 avg_loss: 0.2314, acc: 0.9085 
val --- avg_loss: 0.4709, acc: 0.8657  

epoch: 95 avg_loss: 0.2334, acc: 0.9105 
val --- avg_loss: 0.4651, acc: 0.8661  

epoch: 96 avg_loss: 0.2308, acc: 0.9103 
val --- avg_loss: 0.4342, acc: 0.8702  

epoch: 97 avg_loss: 0.2305, acc: 0.9091 
val --- avg_loss: 0.4819, acc: 0.8702  

epoch: 98 avg_loss: 0.2299, acc: 0.9132 
val --- avg_loss: 0.4572, acc: 0.8756  

epoch: 99 avg_loss: 0.2329, acc: 0.9103 
val --- avg_loss: 0.4727, acc: 0.8723  
