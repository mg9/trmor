
number of params: 248512 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/root_concept/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/200epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7f4daf6c4550>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_7_dict_enlarged_root_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=320, bias=True)
  (linear_3): Linear(in_features=512, out_features=32, bias=True)
  (linear_4): Linear(in_features=512, out_features=32, bias=True)
  (linear_5): Linear(in_features=512, out_features=32, bias=True)
  (linear_6): Linear(in_features=512, out_features=32, bias=True)
  (linear_7): Linear(in_features=512, out_features=32, bias=True)
  (linear_8): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (linear): Linear(in_features=352, out_features=704, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=320, bias=True)
  (linear_3): Linear(in_features=512, out_features=32, bias=True)
  (linear_4): Linear(in_features=512, out_features=32, bias=True)
  (linear_5): Linear(in_features=512, out_features=32, bias=True)
  (linear_6): Linear(in_features=512, out_features=32, bias=True)
  (linear_7): Linear(in_features=512, out_features=32, bias=True)
  (linear_8): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(1000, 320)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/root_concept/results/training/vqvae_7_dict_enlarged_root_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([320, 512]): False
linear_2.bias, torch.Size([320]): False
linear_3.weight, torch.Size([32, 512]): False
linear_3.bias, torch.Size([32]): False
linear_4.weight, torch.Size([32, 512]): False
linear_4.bias, torch.Size([32]): False
linear_5.weight, torch.Size([32, 512]): False
linear_5.bias, torch.Size([32]): False
linear_6.weight, torch.Size([32, 512]): False
linear_6.bias, torch.Size([32]): False
linear_7.weight, torch.Size([32, 512]): False
linear_7.bias, torch.Size([32]): False
linear_8.weight, torch.Size([32, 512]): False
linear_8.bias, torch.Size([32]): False
vq_layer_2.embedding.weight, torch.Size([1000, 320]): False
vq_layer_3.embedding.weight, torch.Size([100, 32]): False
vq_layer_4.embedding.weight, torch.Size([100, 32]): False
vq_layer_5.embedding.weight, torch.Size([100, 32]): False
vq_layer_6.embedding.weight, torch.Size([100, 32]): False
vq_layer_7.embedding.weight, torch.Size([100, 32]): False
vq_layer_8.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([704, 352]): True
linear.bias, torch.Size([704]): True
epoch: 0 avg_loss: 3.6093, acc: 0.4927 

epoch: 1 avg_loss: 1.2399, acc: 0.6934 

epoch: 2 avg_loss: 1.3204, acc: 0.7227 

epoch: 3 avg_loss: 1.1372, acc: 0.7336 

epoch: 4 avg_loss: 1.0498, acc: 0.7545 

epoch: 5 avg_loss: 0.9866, acc: 0.7651 

epoch: 6 avg_loss: 1.0297, acc: 0.7651 

epoch: 7 avg_loss: 0.9703, acc: 0.7818 

epoch: 8 avg_loss: 0.9277, acc: 0.7749 

epoch: 9 avg_loss: 0.9139, acc: 0.7921 

epoch: 10 avg_loss: 0.7797, acc: 0.7967 

epoch: 11 avg_loss: 0.8802, acc: 0.7935 

epoch: 12 avg_loss: 0.8374, acc: 0.8013 

epoch: 13 avg_loss: 0.8038, acc: 0.8038 

epoch: 14 avg_loss: 0.8058, acc: 0.8079 

epoch: 15 avg_loss: 0.8349, acc: 0.8036 

epoch: 16 avg_loss: 0.7594, acc: 0.8145 

epoch: 17 avg_loss: 0.6857, acc: 0.8308 

epoch: 18 avg_loss: 0.7435, acc: 0.8225 

epoch: 19 avg_loss: 0.8063, acc: 0.8165 

epoch: 20 avg_loss: 0.6978, acc: 0.8325 

epoch: 21 avg_loss: 0.7292, acc: 0.8222 

epoch: 22 avg_loss: 0.6559, acc: 0.8400 

epoch: 23 avg_loss: 0.6726, acc: 0.8428 

epoch: 24 avg_loss: 0.6381, acc: 0.8374 

epoch: 25 avg_loss: 0.7136, acc: 0.8380 

epoch: 26 avg_loss: 0.7884, acc: 0.8322 

epoch: 27 avg_loss: 0.7158, acc: 0.8377 

epoch: 28 avg_loss: 0.6785, acc: 0.8437 

epoch: 29 avg_loss: 0.6525, acc: 0.8460 

epoch: 30 avg_loss: 0.6491, acc: 0.8506 

epoch: 31 avg_loss: 0.6176, acc: 0.8535 

epoch: 32 avg_loss: 0.7060, acc: 0.8457 

epoch: 33 avg_loss: 0.6404, acc: 0.8560 

epoch: 34 avg_loss: 0.6109, acc: 0.8566 

epoch: 35 avg_loss: 0.5848, acc: 0.8635 

epoch: 36 avg_loss: 0.5776, acc: 0.8612 

epoch: 37 avg_loss: 0.6150, acc: 0.8560 

epoch: 38 avg_loss: 0.6946, acc: 0.8489 

epoch: 39 avg_loss: 0.6936, acc: 0.8512 

epoch: 40 avg_loss: 0.6867, acc: 0.8569 

epoch: 41 avg_loss: 0.5664, acc: 0.8735 

epoch: 42 avg_loss: 0.5786, acc: 0.8730 

epoch: 43 avg_loss: 0.6280, acc: 0.8641 

epoch: 44 avg_loss: 0.6302, acc: 0.8606 

epoch: 45 avg_loss: 0.5903, acc: 0.8672 

epoch: 46 avg_loss: 0.5588, acc: 0.8598 

epoch: 47 avg_loss: 0.5642, acc: 0.8675 

epoch: 48 avg_loss: 0.5851, acc: 0.8669 

epoch: 49 avg_loss: 0.5557, acc: 0.8709 

epoch: 50 avg_loss: 0.6061, acc: 0.8664 

epoch: 51 avg_loss: 0.5543, acc: 0.8684 

epoch: 52 avg_loss: 0.5573, acc: 0.8687 

epoch: 53 avg_loss: 0.6205, acc: 0.8603 

epoch: 54 avg_loss: 0.5779, acc: 0.8718 

epoch: 55 avg_loss: 0.6273, acc: 0.8681 

epoch: 56 avg_loss: 0.6330, acc: 0.8666 

epoch: 57 avg_loss: 0.5952, acc: 0.8701 

epoch: 58 avg_loss: 0.6400, acc: 0.8641 

epoch: 59 avg_loss: 0.5903, acc: 0.8707 

epoch: 60 avg_loss: 0.6220, acc: 0.8715 

epoch: 61 avg_loss: 0.6440, acc: 0.8669 

epoch: 62 avg_loss: 0.6204, acc: 0.8715 

epoch: 63 avg_loss: 0.5885, acc: 0.8764 

epoch: 64 avg_loss: 0.5720, acc: 0.8735 

epoch: 65 avg_loss: 0.5938, acc: 0.8747 

epoch: 66 avg_loss: 0.5559, acc: 0.8775 

epoch: 67 avg_loss: 0.5615, acc: 0.8730 

epoch: 68 avg_loss: 0.6446, acc: 0.8692 

epoch: 69 avg_loss: 0.6665, acc: 0.8632 

epoch: 70 avg_loss: 0.7474, acc: 0.8649 

epoch: 71 avg_loss: 0.6518, acc: 0.8618 

epoch: 72 avg_loss: 0.6492, acc: 0.8684 

epoch: 73 avg_loss: 0.6571, acc: 0.8644 

epoch: 74 avg_loss: 0.6802, acc: 0.8629 

epoch: 75 avg_loss: 0.5972, acc: 0.8744 

epoch: 76 avg_loss: 0.6042, acc: 0.8738 

epoch: 77 avg_loss: 0.6700, acc: 0.8701 

epoch: 78 avg_loss: 0.6074, acc: 0.8744 

epoch: 79 avg_loss: 0.5037, acc: 0.8836 

epoch: 80 avg_loss: 0.5036, acc: 0.8807 

epoch: 81 avg_loss: 0.5099, acc: 0.8816 

epoch: 82 avg_loss: 0.6392, acc: 0.8764 

epoch: 83 avg_loss: 0.5756, acc: 0.8755 

epoch: 84 avg_loss: 0.6070, acc: 0.8824 

epoch: 85 avg_loss: 0.5328, acc: 0.8847 

epoch: 86 avg_loss: 0.6072, acc: 0.8775 

epoch: 87 avg_loss: 0.5490, acc: 0.8796 

epoch: 88 avg_loss: 0.5230, acc: 0.8910 

epoch: 89 avg_loss: 0.5486, acc: 0.8767 

epoch: 90 avg_loss: 0.5646, acc: 0.8790 

epoch: 91 avg_loss: 0.5242, acc: 0.8818 

epoch: 92 avg_loss: 0.5188, acc: 0.8853 

epoch: 93 avg_loss: 0.5721, acc: 0.8839 

epoch: 94 avg_loss: 0.5797, acc: 0.8850 

epoch: 95 avg_loss: 0.5415, acc: 0.8796 

epoch: 96 avg_loss: 0.5666, acc: 0.8870 

epoch: 97 avg_loss: 0.5381, acc: 0.8859 

epoch: 98 avg_loss: 0.6231, acc: 0.8856 

epoch: 99 avg_loss: 0.6036, acc: 0.8841 

epoch: 100 avg_loss: 0.5613, acc: 0.8836 

epoch: 101 avg_loss: 0.5857, acc: 0.8775 

epoch: 102 avg_loss: 0.5536, acc: 0.8864 

epoch: 103 avg_loss: 0.5723, acc: 0.8813 

epoch: 104 avg_loss: 0.5218, acc: 0.8970 

epoch: 105 avg_loss: 0.5752, acc: 0.8821 

epoch: 106 avg_loss: 0.6453, acc: 0.8810 

epoch: 107 avg_loss: 0.6268, acc: 0.8793 

epoch: 108 avg_loss: 0.6304, acc: 0.8796 

epoch: 109 avg_loss: 0.6297, acc: 0.8781 

epoch: 110 avg_loss: 0.6243, acc: 0.8781 

epoch: 111 avg_loss: 0.6241, acc: 0.8801 

epoch: 112 avg_loss: 0.5663, acc: 0.8879 

epoch: 113 avg_loss: 0.5970, acc: 0.8804 

epoch: 114 avg_loss: 0.5114, acc: 0.8913 

epoch: 115 avg_loss: 0.5185, acc: 0.8882 

epoch: 116 avg_loss: 0.5845, acc: 0.8796 

epoch: 117 avg_loss: 0.6013, acc: 0.8867 

epoch: 118 avg_loss: 0.6051, acc: 0.8876 

epoch: 119 avg_loss: 0.6445, acc: 0.8856 

epoch: 120 avg_loss: 0.5907, acc: 0.8824 

epoch: 121 avg_loss: 0.5726, acc: 0.8907 

epoch: 122 avg_loss: 0.5540, acc: 0.8847 

epoch: 123 avg_loss: 0.5991, acc: 0.8850 

epoch: 124 avg_loss: 0.5804, acc: 0.8859 

epoch: 125 avg_loss: 0.5514, acc: 0.8945 

epoch: 126 avg_loss: 0.6315, acc: 0.8784 

epoch: 127 avg_loss: 0.5464, acc: 0.8887 

epoch: 128 avg_loss: 0.5893, acc: 0.8930 

epoch: 129 avg_loss: 0.5802, acc: 0.8830 

epoch: 130 avg_loss: 0.5702, acc: 0.8861 

epoch: 131 avg_loss: 0.6252, acc: 0.8836 

epoch: 132 avg_loss: 0.6478, acc: 0.8767 

epoch: 133 avg_loss: 0.5902, acc: 0.8836 

epoch: 134 avg_loss: 0.5350, acc: 0.8864 

epoch: 135 avg_loss: 0.5492, acc: 0.8873 

epoch: 136 avg_loss: 0.5811, acc: 0.8902 

epoch: 137 avg_loss: 0.5828, acc: 0.8833 

epoch: 138 avg_loss: 0.5479, acc: 0.8853 

epoch: 139 avg_loss: 0.5806, acc: 0.8902 

epoch: 140 avg_loss: 0.5848, acc: 0.8867 

epoch: 141 avg_loss: 0.5986, acc: 0.8767 

epoch: 142 avg_loss: 0.6783, acc: 0.8821 

epoch: 143 avg_loss: 0.5254, acc: 0.8933 

epoch: 144 avg_loss: 0.5735, acc: 0.8945 

epoch: 145 avg_loss: 0.5402, acc: 0.8882 

epoch: 146 avg_loss: 0.5758, acc: 0.9031 

epoch: 147 avg_loss: 0.5252, acc: 0.8896 

epoch: 148 avg_loss: 0.5643, acc: 0.8905 

epoch: 149 avg_loss: 0.5827, acc: 0.8933 

epoch: 150 avg_loss: 0.5398, acc: 0.8988 

epoch: 151 avg_loss: 0.5742, acc: 0.8884 

epoch: 152 avg_loss: 0.5610, acc: 0.8930 

epoch: 153 avg_loss: 0.5985, acc: 0.8824 

epoch: 154 avg_loss: 0.5531, acc: 0.8927 

epoch: 155 avg_loss: 0.5243, acc: 0.8859 

epoch: 156 avg_loss: 0.5704, acc: 0.8905 

epoch: 157 avg_loss: 0.5681, acc: 0.8913 

epoch: 158 avg_loss: 0.5233, acc: 0.8925 

epoch: 159 avg_loss: 0.5256, acc: 0.8939 

epoch: 160 avg_loss: 0.5402, acc: 0.8925 

epoch: 161 avg_loss: 0.5238, acc: 0.8950 

epoch: 162 avg_loss: 0.5531, acc: 0.8965 

epoch: 163 avg_loss: 0.5317, acc: 0.8970 

epoch: 164 avg_loss: 0.5018, acc: 0.9016 

epoch: 165 avg_loss: 0.4890, acc: 0.8973 

epoch: 166 avg_loss: 0.5217, acc: 0.8950 

epoch: 167 avg_loss: 0.4990, acc: 0.8968 

epoch: 168 avg_loss: 0.5389, acc: 0.8948 

epoch: 169 avg_loss: 0.5489, acc: 0.9008 

epoch: 170 avg_loss: 0.5710, acc: 0.8999 

epoch: 171 avg_loss: 0.5185, acc: 0.8905 

epoch: 172 avg_loss: 0.4982, acc: 0.8970 

epoch: 173 avg_loss: 0.5151, acc: 0.9028 

epoch: 174 avg_loss: 0.5642, acc: 0.8899 

epoch: 175 avg_loss: 0.6618, acc: 0.8905 

epoch: 176 avg_loss: 0.5475, acc: 0.8999 

epoch: 177 avg_loss: 0.5954, acc: 0.8970 

epoch: 178 avg_loss: 0.5364, acc: 0.8939 

epoch: 179 avg_loss: 0.5561, acc: 0.8930 

epoch: 180 avg_loss: 0.5964, acc: 0.8939 

epoch: 181 avg_loss: 0.4996, acc: 0.8979 

epoch: 182 avg_loss: 0.5339, acc: 0.8973 

epoch: 183 avg_loss: 0.5243, acc: 0.8916 

epoch: 184 avg_loss: 0.5457, acc: 0.8985 

epoch: 185 avg_loss: 0.4812, acc: 0.8991 

epoch: 186 avg_loss: 0.5108, acc: 0.8942 

epoch: 187 avg_loss: 0.5079, acc: 0.8970 

epoch: 188 avg_loss: 0.5161, acc: 0.8976 

epoch: 189 avg_loss: 0.5008, acc: 0.9022 

epoch: 190 avg_loss: 0.5291, acc: 0.8993 

epoch: 191 avg_loss: 0.4757, acc: 0.8996 

epoch: 192 avg_loss: 0.4705, acc: 0.9039 

epoch: 193 avg_loss: 0.4747, acc: 0.9056 

epoch: 194 avg_loss: 0.5305, acc: 0.8956 

epoch: 195 avg_loss: 0.5131, acc: 0.8948 

epoch: 196 avg_loss: 0.5209, acc: 0.9008 

epoch: 197 avg_loss: 0.5662, acc: 0.8956 

epoch: 198 avg_loss: 0.6608, acc: 0.8933 

epoch: 199 avg_loss: 0.6196, acc: 0.8847 
