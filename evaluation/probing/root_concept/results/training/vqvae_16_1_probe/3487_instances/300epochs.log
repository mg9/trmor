
number of params: 23232 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=300, fig_path='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/300epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/300epochs.log', logger=<common.utils.Logger object at 0x7f8b64fb84d0>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_16_1_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
    (6): Linear(in_features=512, out_features=32, bias=True)
    (7): Linear(in_features=512, out_features=32, bias=True)
    (8): Linear(in_features=512, out_features=32, bias=True)
    (9): Linear(in_features=512, out_features=32, bias=True)
    (10): Linear(in_features=512, out_features=32, bias=True)
    (11): Linear(in_features=512, out_features=32, bias=True)
    (12): Linear(in_features=512, out_features=32, bias=True)
    (13): Linear(in_features=512, out_features=32, bias=True)
    (14): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (6): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (7): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (8): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (9): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (10): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (11): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (12): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (13): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (14): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (linear): Linear(in_features=32, out_features=704, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/', nh=512, ni=256, num_dicts=16, nz=512, opt='Adam', orddict_emb_num=100, pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
    (6): Linear(in_features=512, out_features=32, bias=True)
    (7): Linear(in_features=512, out_features=32, bias=True)
    (8): Linear(in_features=512, out_features=32, bias=True)
    (9): Linear(in_features=512, out_features=32, bias=True)
    (10): Linear(in_features=512, out_features=32, bias=True)
    (11): Linear(in_features=512, out_features=32, bias=True)
    (12): Linear(in_features=512, out_features=32, bias=True)
    (13): Linear(in_features=512, out_features=32, bias=True)
    (14): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (6): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (7): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (8): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (9): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (10): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (11): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (12): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (13): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (14): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), rootdict_emb_dim=32, rootdict_emb_num=100, save_path='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/300epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_root.weight, torch.Size([32, 512]): False
linear_root.bias, torch.Size([32]): False
vq_layer_root.embedding.weight, torch.Size([100, 32]): False
ord_linears.0.weight, torch.Size([32, 512]): False
ord_linears.0.bias, torch.Size([32]): False
ord_linears.1.weight, torch.Size([32, 512]): False
ord_linears.1.bias, torch.Size([32]): False
ord_linears.2.weight, torch.Size([32, 512]): False
ord_linears.2.bias, torch.Size([32]): False
ord_linears.3.weight, torch.Size([32, 512]): False
ord_linears.3.bias, torch.Size([32]): False
ord_linears.4.weight, torch.Size([32, 512]): False
ord_linears.4.bias, torch.Size([32]): False
ord_linears.5.weight, torch.Size([32, 512]): False
ord_linears.5.bias, torch.Size([32]): False
ord_linears.6.weight, torch.Size([32, 512]): False
ord_linears.6.bias, torch.Size([32]): False
ord_linears.7.weight, torch.Size([32, 512]): False
ord_linears.7.bias, torch.Size([32]): False
ord_linears.8.weight, torch.Size([32, 512]): False
ord_linears.8.bias, torch.Size([32]): False
ord_linears.9.weight, torch.Size([32, 512]): False
ord_linears.9.bias, torch.Size([32]): False
ord_linears.10.weight, torch.Size([32, 512]): False
ord_linears.10.bias, torch.Size([32]): False
ord_linears.11.weight, torch.Size([32, 512]): False
ord_linears.11.bias, torch.Size([32]): False
ord_linears.12.weight, torch.Size([32, 512]): False
ord_linears.12.bias, torch.Size([32]): False
ord_linears.13.weight, torch.Size([32, 512]): False
ord_linears.13.bias, torch.Size([32]): False
ord_linears.14.weight, torch.Size([32, 512]): False
ord_linears.14.bias, torch.Size([32]): False
ord_vq_layers.0.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.1.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.2.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.3.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.4.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.5.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.6.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.7.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.8.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.9.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.10.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.11.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.12.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.13.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.14.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([704, 32]): True
linear.bias, torch.Size([704]): True
epoch: 0 avg_loss: 6.0827, acc: 0.0393 
val --- avg_loss: 5.9132, acc: 0.0383  
update best loss 

epoch: 1 avg_loss: 5.2202, acc: 0.0757 
val --- avg_loss: 5.6048, acc: 0.0574  
update best loss 

epoch: 2 avg_loss: 4.8099, acc: 0.0958 
val --- avg_loss: 5.4530, acc: 0.0622  
update best loss 

epoch: 3 avg_loss: 4.5581, acc: 0.1044 
val --- avg_loss: 5.3126, acc: 0.0622  
update best loss 

epoch: 4 avg_loss: 4.3525, acc: 0.1101 
val --- avg_loss: 5.1612, acc: 0.0718  
update best loss 

epoch: 5 avg_loss: 4.2053, acc: 0.1104 
val --- avg_loss: 5.1272, acc: 0.0670  
update best loss 

epoch: 6 avg_loss: 4.0824, acc: 0.1167 
val --- avg_loss: 5.0626, acc: 0.0622  
update best loss 

epoch: 7 avg_loss: 3.9734, acc: 0.1204 
val --- avg_loss: 4.9914, acc: 0.0478  
update best loss 

epoch: 8 avg_loss: 3.8975, acc: 0.1167 
val --- avg_loss: 4.9453, acc: 0.0526  
update best loss 

epoch: 9 avg_loss: 3.8308, acc: 0.1207 
val --- avg_loss: 4.8676, acc: 0.0622  
update best loss 

epoch: 10 avg_loss: 3.7687, acc: 0.1153 
val --- avg_loss: 4.8611, acc: 0.0622  
update best loss 

epoch: 11 avg_loss: 3.7187, acc: 0.1210 
val --- avg_loss: 4.8370, acc: 0.0526  
update best loss 

epoch: 12 avg_loss: 3.6728, acc: 0.1202 
val --- avg_loss: 4.8112, acc: 0.0526  
update best loss 

epoch: 13 avg_loss: 3.6365, acc: 0.1196 
val --- avg_loss: 4.8064, acc: 0.0478  
update best loss 

epoch: 14 avg_loss: 3.6110, acc: 0.1239 
val --- avg_loss: 4.8130, acc: 0.0574  

epoch: 15 avg_loss: 3.5821, acc: 0.1187 
val --- avg_loss: 4.7818, acc: 0.0574  
update best loss 

epoch: 16 avg_loss: 3.5485, acc: 0.1259 
val --- avg_loss: 4.7901, acc: 0.0526  

epoch: 17 avg_loss: 3.5329, acc: 0.1204 
val --- avg_loss: 4.8112, acc: 0.0526  

epoch: 18 avg_loss: 3.5110, acc: 0.1227 
val --- avg_loss: 4.7514, acc: 0.0526  
update best loss 

epoch: 19 avg_loss: 3.4872, acc: 0.1227 
val --- avg_loss: 4.7649, acc: 0.0478  

epoch: 20 avg_loss: 3.4738, acc: 0.1161 
val --- avg_loss: 4.7728, acc: 0.0574  

epoch: 21 avg_loss: 3.4459, acc: 0.1233 
val --- avg_loss: 4.7490, acc: 0.0526  
update best loss 

epoch: 22 avg_loss: 3.4489, acc: 0.1184 
val --- avg_loss: 4.7591, acc: 0.0478  

epoch: 23 avg_loss: 3.4317, acc: 0.1207 
val --- avg_loss: 4.7813, acc: 0.0574  

epoch: 24 avg_loss: 3.4107, acc: 0.1230 
val --- avg_loss: 4.7721, acc: 0.0526  

epoch: 25 avg_loss: 3.4034, acc: 0.1219 
val --- avg_loss: 4.7565, acc: 0.0526  

epoch: 26 avg_loss: 3.3978, acc: 0.1227 
val --- avg_loss: 4.7489, acc: 0.0478  
update best loss 

epoch: 27 avg_loss: 3.3870, acc: 0.1164 
val --- avg_loss: 4.7560, acc: 0.0478  

epoch: 28 avg_loss: 3.3708, acc: 0.1210 
val --- avg_loss: 4.7730, acc: 0.0526  

epoch: 29 avg_loss: 3.3655, acc: 0.1256 
val --- avg_loss: 4.7541, acc: 0.0478  

epoch: 30 avg_loss: 3.3618, acc: 0.1136 
val --- avg_loss: 4.7754, acc: 0.0431  

epoch: 31 avg_loss: 3.3460, acc: 0.1167 
val --- avg_loss: 4.7846, acc: 0.0574  

epoch: 32 avg_loss: 3.3293, acc: 0.1219 
val --- avg_loss: 4.8170, acc: 0.0526  

epoch: 33 avg_loss: 3.3409, acc: 0.1190 
val --- avg_loss: 4.7555, acc: 0.0574  

epoch: 34 avg_loss: 3.3240, acc: 0.1239 
val --- avg_loss: 4.7701, acc: 0.0526  

epoch: 35 avg_loss: 3.3285, acc: 0.1233 
val --- avg_loss: 4.8116, acc: 0.0478  

epoch: 36 avg_loss: 3.3150, acc: 0.1222 
val --- avg_loss: 4.7823, acc: 0.0622  

epoch: 37 avg_loss: 3.3082, acc: 0.1182 
val --- avg_loss: 4.7748, acc: 0.0478  

epoch: 38 avg_loss: 3.3179, acc: 0.1213 
val --- avg_loss: 4.7924, acc: 0.0478  

epoch: 39 avg_loss: 3.3125, acc: 0.1173 
val --- avg_loss: 4.7836, acc: 0.0526  

epoch: 40 avg_loss: 3.2991, acc: 0.1210 
val --- avg_loss: 4.8217, acc: 0.0478  

epoch: 41 avg_loss: 3.2976, acc: 0.1139 
val --- avg_loss: 4.7668, acc: 0.0478  

epoch: 42 avg_loss: 3.2939, acc: 0.1199 
val --- avg_loss: 4.7763, acc: 0.0526  

epoch: 43 avg_loss: 3.2871, acc: 0.1236 
val --- avg_loss: 4.7943, acc: 0.0526  

epoch: 44 avg_loss: 3.2780, acc: 0.1233 
val --- avg_loss: 4.8013, acc: 0.0526  

epoch: 45 avg_loss: 3.2693, acc: 0.1199 
val --- avg_loss: 4.8201, acc: 0.0526  

epoch: 46 avg_loss: 3.2739, acc: 0.1182 
val --- avg_loss: 4.8288, acc: 0.0574  

epoch: 47 avg_loss: 3.2678, acc: 0.1207 
val --- avg_loss: 4.7991, acc: 0.0526  

epoch: 48 avg_loss: 3.2719, acc: 0.1207 
val --- avg_loss: 4.8446, acc: 0.0431  

epoch: 49 avg_loss: 3.2568, acc: 0.1268 
val --- avg_loss: 4.8118, acc: 0.0431  

epoch: 50 avg_loss: 3.2542, acc: 0.1182 
val --- avg_loss: 4.8165, acc: 0.0526  

epoch: 51 avg_loss: 3.2519, acc: 0.1273 
val --- avg_loss: 4.8466, acc: 0.0478  

epoch: 52 avg_loss: 3.2540, acc: 0.1199 
val --- avg_loss: 4.8467, acc: 0.0431  

epoch: 53 avg_loss: 3.2536, acc: 0.1207 
val --- avg_loss: 4.8133, acc: 0.0526  

epoch: 54 avg_loss: 3.2380, acc: 0.1225 
val --- avg_loss: 4.8610, acc: 0.0526  

epoch: 55 avg_loss: 3.2467, acc: 0.1173 
val --- avg_loss: 4.8392, acc: 0.0526  

epoch: 56 avg_loss: 3.2405, acc: 0.1182 
val --- avg_loss: 4.8250, acc: 0.0574  

epoch: 57 avg_loss: 3.2399, acc: 0.1182 
val --- avg_loss: 4.8348, acc: 0.0526  

epoch: 58 avg_loss: 3.2294, acc: 0.1233 
val --- avg_loss: 4.8649, acc: 0.0622  

epoch: 59 avg_loss: 3.2438, acc: 0.1247 
val --- avg_loss: 4.8563, acc: 0.0574  

epoch: 60 avg_loss: 3.2225, acc: 0.1225 
val --- avg_loss: 4.8718, acc: 0.0478  

epoch: 61 avg_loss: 3.2275, acc: 0.1230 
val --- avg_loss: 4.8335, acc: 0.0478  

epoch: 62 avg_loss: 3.2312, acc: 0.1190 
val --- avg_loss: 4.8558, acc: 0.0574  

epoch: 63 avg_loss: 3.2384, acc: 0.1219 
val --- avg_loss: 4.8689, acc: 0.0478  

epoch: 64 avg_loss: 3.2309, acc: 0.1219 
val --- avg_loss: 4.8337, acc: 0.0574  

epoch: 65 avg_loss: 3.2150, acc: 0.1227 
val --- avg_loss: 4.8506, acc: 0.0478  

epoch: 66 avg_loss: 3.2201, acc: 0.1222 
val --- avg_loss: 4.8476, acc: 0.0526  

epoch: 67 avg_loss: 3.2217, acc: 0.1196 
val --- avg_loss: 4.8484, acc: 0.0526  

epoch: 68 avg_loss: 3.2171, acc: 0.1216 
val --- avg_loss: 4.8766, acc: 0.0526  

epoch: 69 avg_loss: 3.2134, acc: 0.1219 
val --- avg_loss: 4.8955, acc: 0.0526  

epoch: 70 avg_loss: 3.2037, acc: 0.1202 
val --- avg_loss: 4.8765, acc: 0.0431  

epoch: 71 avg_loss: 3.2176, acc: 0.1213 
val --- avg_loss: 4.8529, acc: 0.0526  

epoch: 72 avg_loss: 3.2087, acc: 0.1190 
val --- avg_loss: 4.8766, acc: 0.0574  

epoch: 73 avg_loss: 3.2063, acc: 0.1225 
val --- avg_loss: 4.8893, acc: 0.0478  

epoch: 74 avg_loss: 3.2017, acc: 0.1182 
val --- avg_loss: 4.8795, acc: 0.0526  

epoch: 75 avg_loss: 3.2019, acc: 0.1202 
val --- avg_loss: 4.8685, acc: 0.0478  

epoch: 76 avg_loss: 3.2011, acc: 0.1233 
val --- avg_loss: 4.8598, acc: 0.0431  

epoch: 77 avg_loss: 3.1920, acc: 0.1184 
val --- avg_loss: 4.8705, acc: 0.0526  

epoch: 78 avg_loss: 3.1968, acc: 0.1233 
val --- avg_loss: 4.8988, acc: 0.0622  

epoch: 79 avg_loss: 3.2033, acc: 0.1213 
val --- avg_loss: 4.8809, acc: 0.0478  

epoch: 80 avg_loss: 3.1940, acc: 0.1173 
val --- avg_loss: 4.8852, acc: 0.0574  

epoch: 81 avg_loss: 3.1957, acc: 0.1187 
val --- avg_loss: 4.9038, acc: 0.0478  

epoch: 82 avg_loss: 3.1909, acc: 0.1204 
val --- avg_loss: 4.9264, acc: 0.0526  

epoch: 83 avg_loss: 3.1922, acc: 0.1222 
val --- avg_loss: 4.9023, acc: 0.0574  

epoch: 84 avg_loss: 3.1873, acc: 0.1236 
val --- avg_loss: 4.8896, acc: 0.0478  

epoch: 85 avg_loss: 3.1844, acc: 0.1187 
val --- avg_loss: 4.8792, acc: 0.0574  

epoch: 86 avg_loss: 3.1842, acc: 0.1216 
val --- avg_loss: 4.9193, acc: 0.0526  

epoch: 87 avg_loss: 3.1864, acc: 0.1176 
val --- avg_loss: 4.9001, acc: 0.0478  

epoch: 88 avg_loss: 3.1794, acc: 0.1239 
val --- avg_loss: 4.8980, acc: 0.0478  

epoch: 89 avg_loss: 3.1888, acc: 0.1233 
val --- avg_loss: 4.9191, acc: 0.0574  

epoch: 90 avg_loss: 3.1773, acc: 0.1159 
val --- avg_loss: 4.9394, acc: 0.0574  

epoch: 91 avg_loss: 3.1780, acc: 0.1216 
val --- avg_loss: 4.9355, acc: 0.0478  

epoch: 92 avg_loss: 3.1724, acc: 0.1207 
val --- avg_loss: 4.9615, acc: 0.0622  

epoch: 93 avg_loss: 3.1795, acc: 0.1236 
val --- avg_loss: 4.9399, acc: 0.0574  

epoch: 94 avg_loss: 3.1987, acc: 0.1144 
val --- avg_loss: 4.9204, acc: 0.0478  

epoch: 95 avg_loss: 3.1848, acc: 0.1196 
val --- avg_loss: 4.9122, acc: 0.0478  

epoch: 96 avg_loss: 3.1714, acc: 0.1196 
val --- avg_loss: 4.9051, acc: 0.0431  

epoch: 97 avg_loss: 3.1779, acc: 0.1170 
val --- avg_loss: 4.9257, acc: 0.0431  

epoch: 98 avg_loss: 3.1678, acc: 0.1239 
val --- avg_loss: 4.9355, acc: 0.0431  

epoch: 99 avg_loss: 3.1883, acc: 0.1164 
val --- avg_loss: 4.9254, acc: 0.0431  

epoch: 100 avg_loss: 3.1714, acc: 0.1225 
val --- avg_loss: 4.9398, acc: 0.0478  

epoch: 101 avg_loss: 3.1651, acc: 0.1225 
val --- avg_loss: 4.9304, acc: 0.0574  

epoch: 102 avg_loss: 3.1717, acc: 0.1173 
val --- avg_loss: 4.9483, acc: 0.0526  

epoch: 103 avg_loss: 3.1756, acc: 0.1225 
val --- avg_loss: 4.9591, acc: 0.0622  

epoch: 104 avg_loss: 3.1700, acc: 0.1164 
val --- avg_loss: 4.9206, acc: 0.0574  

epoch: 105 avg_loss: 3.1693, acc: 0.1187 
val --- avg_loss: 4.9322, acc: 0.0431  

epoch: 106 avg_loss: 3.1658, acc: 0.1204 
val --- avg_loss: 4.9787, acc: 0.0574  

epoch: 107 avg_loss: 3.1653, acc: 0.1230 
val --- avg_loss: 4.9554, acc: 0.0526  

epoch: 108 avg_loss: 3.1758, acc: 0.1121 
val --- avg_loss: 4.9342, acc: 0.0383  

epoch: 109 avg_loss: 3.1633, acc: 0.1245 
val --- avg_loss: 4.9734, acc: 0.0526  

epoch: 110 avg_loss: 3.1571, acc: 0.1219 
val --- avg_loss: 4.9469, acc: 0.0574  

epoch: 111 avg_loss: 3.1592, acc: 0.1156 
val --- avg_loss: 4.9497, acc: 0.0526  

epoch: 112 avg_loss: 3.1580, acc: 0.1219 
val --- avg_loss: 4.9528, acc: 0.0431  

epoch: 113 avg_loss: 3.1560, acc: 0.1202 
val --- avg_loss: 4.9606, acc: 0.0574  

epoch: 114 avg_loss: 3.1648, acc: 0.1216 
val --- avg_loss: 5.0038, acc: 0.0526  

epoch: 115 avg_loss: 3.1531, acc: 0.1236 
val --- avg_loss: 4.9675, acc: 0.0478  

epoch: 116 avg_loss: 3.1672, acc: 0.1219 
val --- avg_loss: 4.9651, acc: 0.0670  

epoch: 117 avg_loss: 3.1532, acc: 0.1173 
val --- avg_loss: 4.9458, acc: 0.0431  

epoch: 118 avg_loss: 3.1717, acc: 0.1245 
val --- avg_loss: 4.9644, acc: 0.0478  

epoch: 119 avg_loss: 3.1561, acc: 0.1210 
val --- avg_loss: 4.9744, acc: 0.0478  

epoch: 120 avg_loss: 3.1612, acc: 0.1216 
val --- avg_loss: 4.9828, acc: 0.0478  

epoch: 121 avg_loss: 3.1495, acc: 0.1164 
val --- avg_loss: 4.9492, acc: 0.0574  

epoch: 122 avg_loss: 3.1599, acc: 0.1262 
val --- avg_loss: 4.9854, acc: 0.0431  

epoch: 123 avg_loss: 3.1554, acc: 0.1182 
val --- avg_loss: 5.0088, acc: 0.0574  

epoch: 124 avg_loss: 3.1530, acc: 0.1170 
val --- avg_loss: 4.9529, acc: 0.0526  

epoch: 125 avg_loss: 3.1512, acc: 0.1242 
val --- avg_loss: 4.9863, acc: 0.0431  

epoch: 126 avg_loss: 3.1487, acc: 0.1187 
val --- avg_loss: 4.9900, acc: 0.0431  

epoch: 127 avg_loss: 3.1444, acc: 0.1182 
val --- avg_loss: 4.9511, acc: 0.0478  

epoch: 128 avg_loss: 3.1509, acc: 0.1259 
val --- avg_loss: 4.9977, acc: 0.0478  

epoch: 129 avg_loss: 3.1562, acc: 0.1179 
val --- avg_loss: 4.9710, acc: 0.0431  

epoch: 130 avg_loss: 3.1432, acc: 0.1247 
val --- avg_loss: 4.9963, acc: 0.0574  

epoch: 131 avg_loss: 3.1503, acc: 0.1196 
val --- avg_loss: 4.9750, acc: 0.0383  

epoch: 132 avg_loss: 3.1522, acc: 0.1202 
val --- avg_loss: 5.0124, acc: 0.0431  

epoch: 133 avg_loss: 3.1467, acc: 0.1236 
val --- avg_loss: 5.0047, acc: 0.0526  

epoch: 134 avg_loss: 3.1361, acc: 0.1219 
val --- avg_loss: 4.9881, acc: 0.0478  

epoch: 135 avg_loss: 3.1467, acc: 0.1187 
val --- avg_loss: 4.9950, acc: 0.0431  

epoch: 136 avg_loss: 3.1473, acc: 0.1259 
val --- avg_loss: 4.9968, acc: 0.0478  

epoch: 137 avg_loss: 3.1623, acc: 0.1210 
val --- avg_loss: 5.0175, acc: 0.0574  

epoch: 138 avg_loss: 3.1489, acc: 0.1182 
val --- avg_loss: 4.9952, acc: 0.0622  

epoch: 139 avg_loss: 3.1421, acc: 0.1213 
val --- avg_loss: 5.0269, acc: 0.0431  

epoch: 140 avg_loss: 3.1467, acc: 0.1222 
val --- avg_loss: 4.9981, acc: 0.0526  

epoch: 141 avg_loss: 3.1481, acc: 0.1190 
val --- avg_loss: 5.0175, acc: 0.0526  

epoch: 142 avg_loss: 3.1398, acc: 0.1173 
val --- avg_loss: 4.9875, acc: 0.0622  

epoch: 143 avg_loss: 3.1414, acc: 0.1199 
val --- avg_loss: 5.0008, acc: 0.0574  

epoch: 144 avg_loss: 3.1391, acc: 0.1184 
val --- avg_loss: 5.0368, acc: 0.0574  

epoch: 145 avg_loss: 3.1353, acc: 0.1245 
val --- avg_loss: 5.0436, acc: 0.0431  

epoch: 146 avg_loss: 3.1343, acc: 0.1225 
val --- avg_loss: 5.0520, acc: 0.0526  

epoch: 147 avg_loss: 3.1366, acc: 0.1216 
val --- avg_loss: 5.0034, acc: 0.0431  

epoch: 148 avg_loss: 3.1422, acc: 0.1176 
val --- avg_loss: 4.9962, acc: 0.0478  

epoch: 149 avg_loss: 3.1335, acc: 0.1259 
val --- avg_loss: 5.0125, acc: 0.0622  

epoch: 150 avg_loss: 3.1417, acc: 0.1182 
val --- avg_loss: 5.0095, acc: 0.0478  

epoch: 151 avg_loss: 3.1322, acc: 0.1202 
val --- avg_loss: 5.0094, acc: 0.0478  

epoch: 152 avg_loss: 3.1362, acc: 0.1233 
val --- avg_loss: 5.0313, acc: 0.0526  

epoch: 153 avg_loss: 3.1364, acc: 0.1161 
val --- avg_loss: 5.0378, acc: 0.0574  

epoch: 154 avg_loss: 3.1400, acc: 0.1182 
val --- avg_loss: 5.0379, acc: 0.0622  

epoch: 155 avg_loss: 3.1303, acc: 0.1210 
val --- avg_loss: 5.0073, acc: 0.0622  

epoch: 156 avg_loss: 3.1303, acc: 0.1219 
val --- avg_loss: 5.0293, acc: 0.0431  

epoch: 157 avg_loss: 3.1371, acc: 0.1216 
val --- avg_loss: 5.0465, acc: 0.0526  

epoch: 158 avg_loss: 3.1421, acc: 0.1193 
val --- avg_loss: 5.0545, acc: 0.0622  

epoch: 159 avg_loss: 3.1270, acc: 0.1193 
val --- avg_loss: 5.0404, acc: 0.0574  

epoch: 160 avg_loss: 3.1421, acc: 0.1210 
val --- avg_loss: 5.0497, acc: 0.0574  

epoch: 161 avg_loss: 3.1281, acc: 0.1193 
val --- avg_loss: 5.0717, acc: 0.0574  

epoch: 162 avg_loss: 3.1288, acc: 0.1219 
val --- avg_loss: 5.0016, acc: 0.0622  

epoch: 163 avg_loss: 3.1381, acc: 0.1179 
val --- avg_loss: 5.0223, acc: 0.0431  

epoch: 164 avg_loss: 3.1406, acc: 0.1202 
val --- avg_loss: 5.0477, acc: 0.0574  

epoch: 165 avg_loss: 3.1238, acc: 0.1207 
val --- avg_loss: 5.0236, acc: 0.0574  

epoch: 166 avg_loss: 3.1236, acc: 0.1216 
val --- avg_loss: 5.0594, acc: 0.0574  

epoch: 167 avg_loss: 3.1349, acc: 0.1190 
val --- avg_loss: 5.0303, acc: 0.0526  

epoch: 168 avg_loss: 3.1292, acc: 0.1227 
val --- avg_loss: 5.0525, acc: 0.0478  

epoch: 169 avg_loss: 3.1280, acc: 0.1245 
val --- avg_loss: 5.0476, acc: 0.0478  

epoch: 170 avg_loss: 3.1304, acc: 0.1227 
val --- avg_loss: 5.0720, acc: 0.0478  

epoch: 171 avg_loss: 3.1281, acc: 0.1219 
val --- avg_loss: 5.0251, acc: 0.0431  

epoch: 172 avg_loss: 3.1193, acc: 0.1219 
val --- avg_loss: 5.0497, acc: 0.0622  

epoch: 173 avg_loss: 3.1309, acc: 0.1156 
val --- avg_loss: 5.0434, acc: 0.0526  

epoch: 174 avg_loss: 3.1256, acc: 0.1193 
val --- avg_loss: 5.0700, acc: 0.0478  

epoch: 175 avg_loss: 3.1221, acc: 0.1270 
val --- avg_loss: 5.0962, acc: 0.0478  

epoch: 176 avg_loss: 3.1332, acc: 0.1219 
val --- avg_loss: 5.0827, acc: 0.0478  

epoch: 177 avg_loss: 3.1211, acc: 0.1265 
val --- avg_loss: 5.0488, acc: 0.0622  

epoch: 178 avg_loss: 3.1291, acc: 0.1153 
val --- avg_loss: 5.0821, acc: 0.0478  

epoch: 179 avg_loss: 3.1300, acc: 0.1182 
val --- avg_loss: 5.0888, acc: 0.0622  

epoch: 180 avg_loss: 3.1247, acc: 0.1159 
val --- avg_loss: 5.0449, acc: 0.0478  

epoch: 181 avg_loss: 3.1182, acc: 0.1182 
val --- avg_loss: 5.0742, acc: 0.0478  

epoch: 182 avg_loss: 3.1344, acc: 0.1202 
val --- avg_loss: 5.0553, acc: 0.0478  

epoch: 183 avg_loss: 3.1246, acc: 0.1230 
val --- avg_loss: 5.0875, acc: 0.0574  

epoch: 184 avg_loss: 3.1164, acc: 0.1222 
val --- avg_loss: 5.0625, acc: 0.0478  

epoch: 185 avg_loss: 3.1285, acc: 0.1199 
val --- avg_loss: 5.0529, acc: 0.0478  

epoch: 186 avg_loss: 3.1147, acc: 0.1216 
val --- avg_loss: 5.0821, acc: 0.0478  

epoch: 187 avg_loss: 3.1245, acc: 0.1196 
val --- avg_loss: 5.0563, acc: 0.0574  

epoch: 188 avg_loss: 3.1156, acc: 0.1236 
val --- avg_loss: 5.0791, acc: 0.0431  

epoch: 189 avg_loss: 3.1298, acc: 0.1156 
val --- avg_loss: 5.0845, acc: 0.0718  

epoch: 190 avg_loss: 3.1165, acc: 0.1213 
val --- avg_loss: 5.0708, acc: 0.0431  

epoch: 191 avg_loss: 3.1217, acc: 0.1153 
val --- avg_loss: 5.0843, acc: 0.0574  

epoch: 192 avg_loss: 3.1135, acc: 0.1210 
val --- avg_loss: 5.0846, acc: 0.0526  

epoch: 193 avg_loss: 3.1171, acc: 0.1190 
val --- avg_loss: 5.0774, acc: 0.0622  

epoch: 194 avg_loss: 3.1236, acc: 0.1230 
val --- avg_loss: 5.0772, acc: 0.0526  

epoch: 195 avg_loss: 3.1242, acc: 0.1210 
val --- avg_loss: 5.0720, acc: 0.0478  

epoch: 196 avg_loss: 3.1255, acc: 0.1199 
val --- avg_loss: 5.1167, acc: 0.0431  

epoch: 197 avg_loss: 3.1221, acc: 0.1207 
val --- avg_loss: 5.0713, acc: 0.0478  

epoch: 198 avg_loss: 3.1148, acc: 0.1259 
val --- avg_loss: 5.1038, acc: 0.0574  

epoch: 199 avg_loss: 3.1159, acc: 0.1199 
val --- avg_loss: 5.0937, acc: 0.0574  

epoch: 200 avg_loss: 3.1187, acc: 0.1170 
val --- avg_loss: 5.0657, acc: 0.0478  

epoch: 201 avg_loss: 3.1234, acc: 0.1184 
val --- avg_loss: 5.0776, acc: 0.0574  

epoch: 202 avg_loss: 3.1120, acc: 0.1230 
val --- avg_loss: 5.0626, acc: 0.0574  

epoch: 203 avg_loss: 3.1160, acc: 0.1213 
val --- avg_loss: 5.0837, acc: 0.0478  

epoch: 204 avg_loss: 3.1223, acc: 0.1227 
val --- avg_loss: 5.1119, acc: 0.0526  

epoch: 205 avg_loss: 3.1075, acc: 0.1199 
val --- avg_loss: 5.0926, acc: 0.0622  

epoch: 206 avg_loss: 3.1167, acc: 0.1236 
val --- avg_loss: 5.0795, acc: 0.0431  

epoch: 207 avg_loss: 3.1194, acc: 0.1210 
val --- avg_loss: 5.0976, acc: 0.0574  

epoch: 208 avg_loss: 3.1202, acc: 0.1184 
val --- avg_loss: 5.1056, acc: 0.0431  

epoch: 209 avg_loss: 3.1144, acc: 0.1276 
val --- avg_loss: 5.1338, acc: 0.0526  

epoch: 210 avg_loss: 3.1183, acc: 0.1193 
val --- avg_loss: 5.0680, acc: 0.0478  

epoch: 211 avg_loss: 3.1187, acc: 0.1167 
val --- avg_loss: 5.0970, acc: 0.0574  

epoch: 212 avg_loss: 3.1055, acc: 0.1219 
val --- avg_loss: 5.1051, acc: 0.0526  

epoch: 213 avg_loss: 3.1122, acc: 0.1239 
val --- avg_loss: 5.1045, acc: 0.0526  

epoch: 214 avg_loss: 3.1151, acc: 0.1196 
val --- avg_loss: 5.0842, acc: 0.0526  

epoch: 215 avg_loss: 3.1151, acc: 0.1242 
val --- avg_loss: 5.0823, acc: 0.0478  

epoch: 216 avg_loss: 3.1111, acc: 0.1182 
val --- avg_loss: 5.1203, acc: 0.0526  

epoch: 217 avg_loss: 3.1158, acc: 0.1242 
val --- avg_loss: 5.1245, acc: 0.0526  

epoch: 218 avg_loss: 3.1145, acc: 0.1222 
val --- avg_loss: 5.1014, acc: 0.0383  

epoch: 219 avg_loss: 3.1158, acc: 0.1233 
val --- avg_loss: 5.1194, acc: 0.0574  

epoch: 220 avg_loss: 3.1156, acc: 0.1253 
val --- avg_loss: 5.1037, acc: 0.0622  

epoch: 221 avg_loss: 3.1039, acc: 0.1167 
val --- avg_loss: 5.1053, acc: 0.0431  

epoch: 222 avg_loss: 3.1058, acc: 0.1187 
val --- avg_loss: 5.0982, acc: 0.0574  

epoch: 223 avg_loss: 3.1206, acc: 0.1225 
val --- avg_loss: 5.1294, acc: 0.0526  

epoch: 224 avg_loss: 3.1107, acc: 0.1196 
val --- avg_loss: 5.1005, acc: 0.0526  

epoch: 225 avg_loss: 3.1081, acc: 0.1242 
val --- avg_loss: 5.1336, acc: 0.0574  

epoch: 226 avg_loss: 3.1248, acc: 0.1196 
val --- avg_loss: 5.1174, acc: 0.0574  

epoch: 227 avg_loss: 3.1065, acc: 0.1190 
val --- avg_loss: 5.1007, acc: 0.0526  

epoch: 228 avg_loss: 3.1015, acc: 0.1239 
val --- avg_loss: 5.1207, acc: 0.0574  

epoch: 229 avg_loss: 3.1074, acc: 0.1242 
val --- avg_loss: 5.1172, acc: 0.0431  

epoch: 230 avg_loss: 3.1119, acc: 0.1233 
val --- avg_loss: 5.1397, acc: 0.0526  

epoch: 231 avg_loss: 3.1053, acc: 0.1182 
val --- avg_loss: 5.1186, acc: 0.0574  

epoch: 232 avg_loss: 3.1036, acc: 0.1204 
val --- avg_loss: 5.1411, acc: 0.0574  

epoch: 233 avg_loss: 3.1020, acc: 0.1173 
val --- avg_loss: 5.1268, acc: 0.0526  

epoch: 234 avg_loss: 3.1207, acc: 0.1190 
val --- avg_loss: 5.1355, acc: 0.0574  

epoch: 235 avg_loss: 3.1047, acc: 0.1247 
val --- avg_loss: 5.1115, acc: 0.0574  

epoch: 236 avg_loss: 3.1095, acc: 0.1247 
val --- avg_loss: 5.1121, acc: 0.0478  

epoch: 237 avg_loss: 3.1085, acc: 0.1207 
val --- avg_loss: 5.1294, acc: 0.0478  

epoch: 238 avg_loss: 3.1114, acc: 0.1161 
val --- avg_loss: 5.1544, acc: 0.0622  

epoch: 239 avg_loss: 3.1017, acc: 0.1242 
val --- avg_loss: 5.1109, acc: 0.0526  

epoch: 240 avg_loss: 3.1097, acc: 0.1204 
val --- avg_loss: 5.1257, acc: 0.0478  

epoch: 241 avg_loss: 3.1019, acc: 0.1236 
val --- avg_loss: 5.0953, acc: 0.0526  

epoch: 242 avg_loss: 3.1182, acc: 0.1216 
val --- avg_loss: 5.1669, acc: 0.0478  

epoch: 243 avg_loss: 3.0971, acc: 0.1207 
val --- avg_loss: 5.1238, acc: 0.0526  

epoch: 244 avg_loss: 3.1004, acc: 0.1245 
val --- avg_loss: 5.1580, acc: 0.0574  

epoch: 245 avg_loss: 3.1008, acc: 0.1225 
val --- avg_loss: 5.1454, acc: 0.0478  

epoch: 246 avg_loss: 3.0997, acc: 0.1219 
val --- avg_loss: 5.1332, acc: 0.0574  

epoch: 247 avg_loss: 3.1281, acc: 0.1161 
val --- avg_loss: 5.1406, acc: 0.0526  

epoch: 248 avg_loss: 3.1086, acc: 0.1196 
val --- avg_loss: 5.1566, acc: 0.0622  

epoch: 249 avg_loss: 3.1061, acc: 0.1225 
val --- avg_loss: 5.1491, acc: 0.0526  

epoch: 250 avg_loss: 3.1050, acc: 0.1176 
val --- avg_loss: 5.1532, acc: 0.0622  

epoch: 251 avg_loss: 3.1046, acc: 0.1245 
val --- avg_loss: 5.1282, acc: 0.0383  

epoch: 252 avg_loss: 3.1019, acc: 0.1207 
val --- avg_loss: 5.1515, acc: 0.0574  

epoch: 253 avg_loss: 3.0961, acc: 0.1213 
val --- avg_loss: 5.1568, acc: 0.0478  

epoch: 254 avg_loss: 3.0982, acc: 0.1247 
val --- avg_loss: 5.1667, acc: 0.0574  

epoch: 255 avg_loss: 3.1077, acc: 0.1222 
val --- avg_loss: 5.1433, acc: 0.0574  

epoch: 256 avg_loss: 3.1002, acc: 0.1227 
val --- avg_loss: 5.1505, acc: 0.0526  

epoch: 257 avg_loss: 3.1059, acc: 0.1242 
val --- avg_loss: 5.1453, acc: 0.0526  

epoch: 258 avg_loss: 3.1002, acc: 0.1239 
val --- avg_loss: 5.1534, acc: 0.0478  

epoch: 259 avg_loss: 3.0929, acc: 0.1187 
val --- avg_loss: 5.1458, acc: 0.0574  

epoch: 260 avg_loss: 3.1056, acc: 0.1247 
val --- avg_loss: 5.1934, acc: 0.0622  

epoch: 261 avg_loss: 3.0994, acc: 0.1213 
val --- avg_loss: 5.1605, acc: 0.0622  

epoch: 262 avg_loss: 3.1064, acc: 0.1176 
val --- avg_loss: 5.1333, acc: 0.0574  

epoch: 263 avg_loss: 3.0992, acc: 0.1196 
val --- avg_loss: 5.1867, acc: 0.0574  

epoch: 264 avg_loss: 3.1084, acc: 0.1233 
val --- avg_loss: 5.1798, acc: 0.0526  

epoch: 265 avg_loss: 3.0966, acc: 0.1256 
val --- avg_loss: 5.1801, acc: 0.0526  

epoch: 266 avg_loss: 3.0932, acc: 0.1222 
val --- avg_loss: 5.1474, acc: 0.0526  

epoch: 267 avg_loss: 3.0915, acc: 0.1259 
val --- avg_loss: 5.1628, acc: 0.0478  

epoch: 268 avg_loss: 3.0966, acc: 0.1202 
val --- avg_loss: 5.1534, acc: 0.0526  

epoch: 269 avg_loss: 3.1023, acc: 0.1210 
val --- avg_loss: 5.1542, acc: 0.0478  

epoch: 270 avg_loss: 3.1128, acc: 0.1216 
val --- avg_loss: 5.1893, acc: 0.0574  

epoch: 271 avg_loss: 3.0864, acc: 0.1262 
val --- avg_loss: 5.1383, acc: 0.0478  

epoch: 272 avg_loss: 3.1010, acc: 0.1207 
val --- avg_loss: 5.1696, acc: 0.0478  

epoch: 273 avg_loss: 3.1097, acc: 0.1190 
val --- avg_loss: 5.1760, acc: 0.0431  

epoch: 274 avg_loss: 3.0914, acc: 0.1219 
val --- avg_loss: 5.1695, acc: 0.0574  

epoch: 275 avg_loss: 3.0975, acc: 0.1225 
val --- avg_loss: 5.1626, acc: 0.0526  

epoch: 276 avg_loss: 3.0934, acc: 0.1190 
val --- avg_loss: 5.1543, acc: 0.0478  

epoch: 277 avg_loss: 3.1034, acc: 0.1204 
val --- avg_loss: 5.1732, acc: 0.0526  

epoch: 278 avg_loss: 3.1133, acc: 0.1161 
val --- avg_loss: 5.1420, acc: 0.0526  

epoch: 279 avg_loss: 3.0972, acc: 0.1247 
val --- avg_loss: 5.2026, acc: 0.0478  

epoch: 280 avg_loss: 3.0972, acc: 0.1199 
val --- avg_loss: 5.1667, acc: 0.0526  

epoch: 281 avg_loss: 3.0978, acc: 0.1239 
val --- avg_loss: 5.1620, acc: 0.0526  

epoch: 282 avg_loss: 3.1050, acc: 0.1190 
val --- avg_loss: 5.1717, acc: 0.0526  

epoch: 283 avg_loss: 3.0940, acc: 0.1164 
val --- avg_loss: 5.1552, acc: 0.0478  

epoch: 284 avg_loss: 3.1010, acc: 0.1196 
val --- avg_loss: 5.2005, acc: 0.0574  

epoch: 285 avg_loss: 3.0971, acc: 0.1184 
val --- avg_loss: 5.1575, acc: 0.0431  

epoch: 286 avg_loss: 3.0936, acc: 0.1156 
val --- avg_loss: 5.2012, acc: 0.0574  

epoch: 287 avg_loss: 3.0875, acc: 0.1196 
val --- avg_loss: 5.1734, acc: 0.0431  

epoch: 288 avg_loss: 3.0926, acc: 0.1270 
val --- avg_loss: 5.1840, acc: 0.0526  

epoch: 289 avg_loss: 3.0932, acc: 0.1219 
val --- avg_loss: 5.1767, acc: 0.0526  

epoch: 290 avg_loss: 3.0941, acc: 0.1213 
val --- avg_loss: 5.1835, acc: 0.0478  

epoch: 291 avg_loss: 3.1056, acc: 0.1182 
val --- avg_loss: 5.1936, acc: 0.0526  

epoch: 292 avg_loss: 3.0983, acc: 0.1179 
val --- avg_loss: 5.1620, acc: 0.0431  

epoch: 293 avg_loss: 3.1082, acc: 0.1161 
val --- avg_loss: 5.2349, acc: 0.0574  

epoch: 294 avg_loss: 3.0916, acc: 0.1233 
val --- avg_loss: 5.1864, acc: 0.0574  

epoch: 295 avg_loss: 3.0903, acc: 0.1242 
val --- avg_loss: 5.1804, acc: 0.0526  

epoch: 296 avg_loss: 3.0900, acc: 0.1161 
val --- avg_loss: 5.1537, acc: 0.0478  

epoch: 297 avg_loss: 3.1002, acc: 0.1222 
val --- avg_loss: 5.1994, acc: 0.0431  

epoch: 298 avg_loss: 3.0953, acc: 0.1262 
val --- avg_loss: 5.1835, acc: 0.0670  

epoch: 299 avg_loss: 3.0862, acc: 0.1242 
val --- avg_loss: 5.1840, acc: 0.0622  
