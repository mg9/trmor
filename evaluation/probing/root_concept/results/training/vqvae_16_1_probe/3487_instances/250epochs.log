
number of params: 23232 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=250, fig_path='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/250epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/250epochs.log', logger=<common.utils.Logger object at 0x7f374b0fd250>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_16_1_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
    (6): Linear(in_features=512, out_features=32, bias=True)
    (7): Linear(in_features=512, out_features=32, bias=True)
    (8): Linear(in_features=512, out_features=32, bias=True)
    (9): Linear(in_features=512, out_features=32, bias=True)
    (10): Linear(in_features=512, out_features=32, bias=True)
    (11): Linear(in_features=512, out_features=32, bias=True)
    (12): Linear(in_features=512, out_features=32, bias=True)
    (13): Linear(in_features=512, out_features=32, bias=True)
    (14): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (6): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (7): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (8): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (9): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (10): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (11): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (12): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (13): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (14): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (linear): Linear(in_features=32, out_features=704, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/', nh=512, ni=256, num_dicts=16, nz=512, opt='Adam', orddict_emb_num=100, pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_root): Linear(in_features=512, out_features=32, bias=True)
  (vq_layer_root): VectorQuantizer(
    (embedding): Embedding(100, 32)
  )
  (ord_linears): ModuleList(
    (0): Linear(in_features=512, out_features=32, bias=True)
    (1): Linear(in_features=512, out_features=32, bias=True)
    (2): Linear(in_features=512, out_features=32, bias=True)
    (3): Linear(in_features=512, out_features=32, bias=True)
    (4): Linear(in_features=512, out_features=32, bias=True)
    (5): Linear(in_features=512, out_features=32, bias=True)
    (6): Linear(in_features=512, out_features=32, bias=True)
    (7): Linear(in_features=512, out_features=32, bias=True)
    (8): Linear(in_features=512, out_features=32, bias=True)
    (9): Linear(in_features=512, out_features=32, bias=True)
    (10): Linear(in_features=512, out_features=32, bias=True)
    (11): Linear(in_features=512, out_features=32, bias=True)
    (12): Linear(in_features=512, out_features=32, bias=True)
    (13): Linear(in_features=512, out_features=32, bias=True)
    (14): Linear(in_features=512, out_features=32, bias=True)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (6): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (7): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (8): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (9): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (10): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (11): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (12): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (13): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
    (14): VectorQuantizer(
      (embedding): Embedding(100, 32)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), rootdict_emb_dim=32, rootdict_emb_num=100, save_path='evaluation/probing/root_concept/results/training/vqvae_16_1_probe/3487_instances/250epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_root.weight, torch.Size([32, 512]): False
linear_root.bias, torch.Size([32]): False
vq_layer_root.embedding.weight, torch.Size([100, 32]): False
ord_linears.0.weight, torch.Size([32, 512]): False
ord_linears.0.bias, torch.Size([32]): False
ord_linears.1.weight, torch.Size([32, 512]): False
ord_linears.1.bias, torch.Size([32]): False
ord_linears.2.weight, torch.Size([32, 512]): False
ord_linears.2.bias, torch.Size([32]): False
ord_linears.3.weight, torch.Size([32, 512]): False
ord_linears.3.bias, torch.Size([32]): False
ord_linears.4.weight, torch.Size([32, 512]): False
ord_linears.4.bias, torch.Size([32]): False
ord_linears.5.weight, torch.Size([32, 512]): False
ord_linears.5.bias, torch.Size([32]): False
ord_linears.6.weight, torch.Size([32, 512]): False
ord_linears.6.bias, torch.Size([32]): False
ord_linears.7.weight, torch.Size([32, 512]): False
ord_linears.7.bias, torch.Size([32]): False
ord_linears.8.weight, torch.Size([32, 512]): False
ord_linears.8.bias, torch.Size([32]): False
ord_linears.9.weight, torch.Size([32, 512]): False
ord_linears.9.bias, torch.Size([32]): False
ord_linears.10.weight, torch.Size([32, 512]): False
ord_linears.10.bias, torch.Size([32]): False
ord_linears.11.weight, torch.Size([32, 512]): False
ord_linears.11.bias, torch.Size([32]): False
ord_linears.12.weight, torch.Size([32, 512]): False
ord_linears.12.bias, torch.Size([32]): False
ord_linears.13.weight, torch.Size([32, 512]): False
ord_linears.13.bias, torch.Size([32]): False
ord_linears.14.weight, torch.Size([32, 512]): False
ord_linears.14.bias, torch.Size([32]): False
ord_vq_layers.0.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.1.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.2.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.3.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.4.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.5.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.6.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.7.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.8.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.9.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.10.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.11.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.12.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.13.embedding.weight, torch.Size([100, 32]): False
ord_vq_layers.14.embedding.weight, torch.Size([100, 32]): False
linear.weight, torch.Size([704, 32]): True
linear.bias, torch.Size([704]): True
epoch: 0 avg_loss: 6.0541, acc: 0.0445 
val --- avg_loss: 5.7854, acc: 0.0144  
update best loss 

epoch: 1 avg_loss: 5.1650, acc: 0.0946 
val --- avg_loss: 5.4371, acc: 0.0287  
update best loss 

epoch: 2 avg_loss: 4.7644, acc: 0.1124 
val --- avg_loss: 5.2910, acc: 0.0478  
update best loss 

epoch: 3 avg_loss: 4.5096, acc: 0.1187 
val --- avg_loss: 5.0617, acc: 0.0670  
update best loss 

epoch: 4 avg_loss: 4.2937, acc: 0.1245 
val --- avg_loss: 4.9904, acc: 0.0574  
update best loss 

epoch: 5 avg_loss: 4.1350, acc: 0.1285 
val --- avg_loss: 4.8846, acc: 0.0622  
update best loss 

epoch: 6 avg_loss: 4.0053, acc: 0.1296 
val --- avg_loss: 4.8664, acc: 0.0622  
update best loss 

epoch: 7 avg_loss: 3.9026, acc: 0.1296 
val --- avg_loss: 4.7912, acc: 0.0670  
update best loss 

epoch: 8 avg_loss: 3.8011, acc: 0.1362 
val --- avg_loss: 4.7766, acc: 0.0813  
update best loss 

epoch: 9 avg_loss: 3.7388, acc: 0.1293 
val --- avg_loss: 4.7417, acc: 0.0670  
update best loss 

epoch: 10 avg_loss: 3.6826, acc: 0.1313 
val --- avg_loss: 4.7508, acc: 0.0766  

epoch: 11 avg_loss: 3.6150, acc: 0.1328 
val --- avg_loss: 4.7021, acc: 0.0718  
update best loss 

epoch: 12 avg_loss: 3.5640, acc: 0.1302 
val --- avg_loss: 4.7001, acc: 0.0670  
update best loss 

epoch: 13 avg_loss: 3.5332, acc: 0.1311 
val --- avg_loss: 4.7118, acc: 0.0813  

epoch: 14 avg_loss: 3.5042, acc: 0.1282 
val --- avg_loss: 4.6936, acc: 0.0861  
update best loss 

epoch: 15 avg_loss: 3.4745, acc: 0.1328 
val --- avg_loss: 4.6904, acc: 0.0622  
update best loss 

epoch: 16 avg_loss: 3.4519, acc: 0.1288 
val --- avg_loss: 4.6770, acc: 0.0670  
update best loss 

epoch: 17 avg_loss: 3.4201, acc: 0.1345 
val --- avg_loss: 4.6931, acc: 0.0718  

epoch: 18 avg_loss: 3.3852, acc: 0.1368 
val --- avg_loss: 4.6973, acc: 0.0718  

epoch: 19 avg_loss: 3.3732, acc: 0.1382 
val --- avg_loss: 4.6789, acc: 0.0622  

epoch: 20 avg_loss: 3.3470, acc: 0.1288 
val --- avg_loss: 4.6822, acc: 0.0766  

epoch: 21 avg_loss: 3.3196, acc: 0.1371 
val --- avg_loss: 4.6912, acc: 0.0526  

epoch: 22 avg_loss: 3.3170, acc: 0.1291 
val --- avg_loss: 4.6994, acc: 0.0813  

epoch: 23 avg_loss: 3.3109, acc: 0.1351 
val --- avg_loss: 4.7081, acc: 0.0670  

epoch: 24 avg_loss: 3.2876, acc: 0.1359 
val --- avg_loss: 4.6875, acc: 0.0670  

epoch: 25 avg_loss: 3.2693, acc: 0.1425 
val --- avg_loss: 4.7058, acc: 0.0766  

epoch: 26 avg_loss: 3.2612, acc: 0.1313 
val --- avg_loss: 4.7082, acc: 0.0670  

epoch: 27 avg_loss: 3.2547, acc: 0.1339 
val --- avg_loss: 4.7094, acc: 0.0718  

epoch: 28 avg_loss: 3.2435, acc: 0.1342 
val --- avg_loss: 4.7070, acc: 0.0478  

epoch: 29 avg_loss: 3.2351, acc: 0.1362 
val --- avg_loss: 4.7327, acc: 0.0718  

epoch: 30 avg_loss: 3.2293, acc: 0.1391 
val --- avg_loss: 4.7417, acc: 0.0670  

epoch: 31 avg_loss: 3.2156, acc: 0.1313 
val --- avg_loss: 4.7067, acc: 0.0670  

epoch: 32 avg_loss: 3.2080, acc: 0.1348 
val --- avg_loss: 4.7260, acc: 0.0622  

epoch: 33 avg_loss: 3.2123, acc: 0.1262 
val --- avg_loss: 4.7439, acc: 0.0622  

epoch: 34 avg_loss: 3.1903, acc: 0.1334 
val --- avg_loss: 4.7285, acc: 0.0526  

epoch: 35 avg_loss: 3.1912, acc: 0.1316 
val --- avg_loss: 4.7507, acc: 0.0670  

epoch: 36 avg_loss: 3.1800, acc: 0.1342 
val --- avg_loss: 4.7432, acc: 0.0670  

epoch: 37 avg_loss: 3.1731, acc: 0.1293 
val --- avg_loss: 4.7410, acc: 0.0574  

epoch: 38 avg_loss: 3.1745, acc: 0.1319 
val --- avg_loss: 4.7727, acc: 0.0718  

epoch: 39 avg_loss: 3.1582, acc: 0.1331 
val --- avg_loss: 4.7508, acc: 0.0574  

epoch: 40 avg_loss: 3.1561, acc: 0.1365 
val --- avg_loss: 4.7462, acc: 0.0574  

epoch: 41 avg_loss: 3.1586, acc: 0.1328 
val --- avg_loss: 4.7597, acc: 0.0670  

epoch: 42 avg_loss: 3.1537, acc: 0.1293 
val --- avg_loss: 4.7726, acc: 0.0718  

epoch: 43 avg_loss: 3.1614, acc: 0.1336 
val --- avg_loss: 4.8027, acc: 0.0622  

epoch: 44 avg_loss: 3.1404, acc: 0.1385 
val --- avg_loss: 4.7735, acc: 0.0718  

epoch: 45 avg_loss: 3.1362, acc: 0.1374 
val --- avg_loss: 4.7969, acc: 0.0622  

epoch: 46 avg_loss: 3.1361, acc: 0.1325 
val --- avg_loss: 4.7903, acc: 0.0574  

epoch: 47 avg_loss: 3.1393, acc: 0.1356 
val --- avg_loss: 4.7937, acc: 0.0718  

epoch: 48 avg_loss: 3.1255, acc: 0.1334 
val --- avg_loss: 4.8112, acc: 0.0718  

epoch: 49 avg_loss: 3.1305, acc: 0.1348 
val --- avg_loss: 4.7996, acc: 0.0909  

epoch: 50 avg_loss: 3.1216, acc: 0.1319 
val --- avg_loss: 4.8168, acc: 0.0622  

epoch: 51 avg_loss: 3.1153, acc: 0.1374 
val --- avg_loss: 4.8217, acc: 0.0766  

epoch: 52 avg_loss: 3.1222, acc: 0.1345 
val --- avg_loss: 4.8252, acc: 0.0622  

epoch: 53 avg_loss: 3.1096, acc: 0.1362 
val --- avg_loss: 4.8610, acc: 0.0718  

epoch: 54 avg_loss: 3.1152, acc: 0.1316 
val --- avg_loss: 4.8250, acc: 0.0813  

epoch: 55 avg_loss: 3.1048, acc: 0.1336 
val --- avg_loss: 4.8494, acc: 0.0574  

epoch: 56 avg_loss: 3.1015, acc: 0.1356 
val --- avg_loss: 4.8283, acc: 0.0622  

epoch: 57 avg_loss: 3.0995, acc: 0.1351 
val --- avg_loss: 4.8596, acc: 0.0670  

epoch: 58 avg_loss: 3.0962, acc: 0.1339 
val --- avg_loss: 4.8510, acc: 0.0718  

epoch: 59 avg_loss: 3.0951, acc: 0.1402 
val --- avg_loss: 4.8755, acc: 0.0478  

epoch: 60 avg_loss: 3.0884, acc: 0.1431 
val --- avg_loss: 4.8602, acc: 0.0766  

epoch: 61 avg_loss: 3.1038, acc: 0.1336 
val --- avg_loss: 4.8608, acc: 0.0526  

epoch: 62 avg_loss: 3.0959, acc: 0.1345 
val --- avg_loss: 4.8873, acc: 0.0670  

epoch: 63 avg_loss: 3.0812, acc: 0.1348 
val --- avg_loss: 4.8814, acc: 0.0526  

epoch: 64 avg_loss: 3.0904, acc: 0.1362 
val --- avg_loss: 4.8607, acc: 0.0718  

epoch: 65 avg_loss: 3.0986, acc: 0.1356 
val --- avg_loss: 4.9184, acc: 0.0526  

epoch: 66 avg_loss: 3.0889, acc: 0.1391 
val --- avg_loss: 4.8585, acc: 0.0670  

epoch: 67 avg_loss: 3.0782, acc: 0.1422 
val --- avg_loss: 4.8871, acc: 0.0813  

epoch: 68 avg_loss: 3.0683, acc: 0.1362 
val --- avg_loss: 4.8741, acc: 0.0574  

epoch: 69 avg_loss: 3.0917, acc: 0.1336 
val --- avg_loss: 4.9069, acc: 0.0574  

epoch: 70 avg_loss: 3.0737, acc: 0.1368 
val --- avg_loss: 4.8798, acc: 0.0861  

epoch: 71 avg_loss: 3.0797, acc: 0.1313 
val --- avg_loss: 4.9262, acc: 0.0526  

epoch: 72 avg_loss: 3.0757, acc: 0.1328 
val --- avg_loss: 4.9145, acc: 0.0526  

epoch: 73 avg_loss: 3.0664, acc: 0.1402 
val --- avg_loss: 4.9053, acc: 0.0622  

epoch: 74 avg_loss: 3.0680, acc: 0.1377 
val --- avg_loss: 4.9173, acc: 0.0670  

epoch: 75 avg_loss: 3.0639, acc: 0.1368 
val --- avg_loss: 4.9477, acc: 0.0718  

epoch: 76 avg_loss: 3.0622, acc: 0.1397 
val --- avg_loss: 4.9092, acc: 0.0718  

epoch: 77 avg_loss: 3.0608, acc: 0.1345 
val --- avg_loss: 4.9226, acc: 0.0622  

epoch: 78 avg_loss: 3.0535, acc: 0.1382 
val --- avg_loss: 4.9353, acc: 0.0718  

epoch: 79 avg_loss: 3.0533, acc: 0.1336 
val --- avg_loss: 4.9498, acc: 0.0574  

epoch: 80 avg_loss: 3.0531, acc: 0.1362 
val --- avg_loss: 4.9277, acc: 0.0670  

epoch: 81 avg_loss: 3.0590, acc: 0.1345 
val --- avg_loss: 4.9398, acc: 0.0526  

epoch: 82 avg_loss: 3.0579, acc: 0.1331 
val --- avg_loss: 4.9376, acc: 0.0813  

epoch: 83 avg_loss: 3.0597, acc: 0.1379 
val --- avg_loss: 4.9732, acc: 0.0622  

epoch: 84 avg_loss: 3.0432, acc: 0.1368 
val --- avg_loss: 4.9303, acc: 0.0622  

epoch: 85 avg_loss: 3.0469, acc: 0.1354 
val --- avg_loss: 4.9816, acc: 0.0670  

epoch: 86 avg_loss: 3.0513, acc: 0.1368 
val --- avg_loss: 4.9491, acc: 0.0861  

epoch: 87 avg_loss: 3.0431, acc: 0.1339 
val --- avg_loss: 4.9820, acc: 0.0526  

epoch: 88 avg_loss: 3.0459, acc: 0.1339 
val --- avg_loss: 4.9593, acc: 0.0622  

epoch: 89 avg_loss: 3.0444, acc: 0.1362 
val --- avg_loss: 5.0002, acc: 0.0718  

epoch: 90 avg_loss: 3.0456, acc: 0.1397 
val --- avg_loss: 4.9781, acc: 0.0718  

epoch: 91 avg_loss: 3.0397, acc: 0.1374 
val --- avg_loss: 4.9822, acc: 0.0622  

epoch: 92 avg_loss: 3.0392, acc: 0.1385 
val --- avg_loss: 4.9816, acc: 0.0670  

epoch: 93 avg_loss: 3.0437, acc: 0.1351 
val --- avg_loss: 5.0050, acc: 0.0574  

epoch: 94 avg_loss: 3.0402, acc: 0.1334 
val --- avg_loss: 4.9819, acc: 0.0574  

epoch: 95 avg_loss: 3.0433, acc: 0.1331 
val --- avg_loss: 5.0236, acc: 0.0622  

epoch: 96 avg_loss: 3.0338, acc: 0.1308 
val --- avg_loss: 5.0249, acc: 0.0718  

epoch: 97 avg_loss: 3.0380, acc: 0.1382 
val --- avg_loss: 5.0206, acc: 0.0718  

epoch: 98 avg_loss: 3.0287, acc: 0.1331 
val --- avg_loss: 4.9944, acc: 0.0622  

epoch: 99 avg_loss: 3.0411, acc: 0.1348 
val --- avg_loss: 5.0239, acc: 0.0478  

epoch: 100 avg_loss: 3.0219, acc: 0.1394 
val --- avg_loss: 4.9971, acc: 0.0909  

epoch: 101 avg_loss: 3.0284, acc: 0.1328 
val --- avg_loss: 5.0316, acc: 0.0478  

epoch: 102 avg_loss: 3.0237, acc: 0.1356 
val --- avg_loss: 5.0119, acc: 0.0861  

epoch: 103 avg_loss: 3.0338, acc: 0.1374 
val --- avg_loss: 5.0070, acc: 0.0574  

epoch: 104 avg_loss: 3.0256, acc: 0.1308 
val --- avg_loss: 5.0176, acc: 0.0622  

epoch: 105 avg_loss: 3.0296, acc: 0.1325 
val --- avg_loss: 5.0363, acc: 0.0813  

epoch: 106 avg_loss: 3.0310, acc: 0.1362 
val --- avg_loss: 5.0102, acc: 0.0670  

epoch: 107 avg_loss: 3.0246, acc: 0.1334 
val --- avg_loss: 5.0479, acc: 0.0526  

epoch: 108 avg_loss: 3.0258, acc: 0.1348 
val --- avg_loss: 5.0422, acc: 0.0813  

epoch: 109 avg_loss: 3.0266, acc: 0.1385 
val --- avg_loss: 5.0538, acc: 0.0574  

epoch: 110 avg_loss: 3.0250, acc: 0.1313 
val --- avg_loss: 5.0151, acc: 0.0622  

epoch: 111 avg_loss: 3.0268, acc: 0.1391 
val --- avg_loss: 5.0827, acc: 0.0478  

epoch: 112 avg_loss: 3.0189, acc: 0.1331 
val --- avg_loss: 5.0254, acc: 0.0670  

epoch: 113 avg_loss: 3.0187, acc: 0.1382 
val --- avg_loss: 5.0618, acc: 0.0574  

epoch: 114 avg_loss: 3.0195, acc: 0.1336 
val --- avg_loss: 5.0784, acc: 0.0431  

epoch: 115 avg_loss: 3.0177, acc: 0.1313 
val --- avg_loss: 5.0677, acc: 0.0431  

epoch: 116 avg_loss: 3.0169, acc: 0.1411 
val --- avg_loss: 5.0573, acc: 0.0718  

epoch: 117 avg_loss: 3.0177, acc: 0.1325 
val --- avg_loss: 5.0770, acc: 0.0574  

epoch: 118 avg_loss: 3.0176, acc: 0.1371 
val --- avg_loss: 5.0924, acc: 0.0622  

epoch: 119 avg_loss: 3.0191, acc: 0.1348 
val --- avg_loss: 5.1054, acc: 0.0478  

epoch: 120 avg_loss: 3.0115, acc: 0.1342 
val --- avg_loss: 5.0598, acc: 0.0909  

epoch: 121 avg_loss: 3.0090, acc: 0.1414 
val --- avg_loss: 5.0636, acc: 0.0766  

epoch: 122 avg_loss: 3.0247, acc: 0.1311 
val --- avg_loss: 5.0852, acc: 0.0431  

epoch: 123 avg_loss: 3.0125, acc: 0.1328 
val --- avg_loss: 5.0977, acc: 0.0622  

epoch: 124 avg_loss: 3.0214, acc: 0.1342 
val --- avg_loss: 5.0694, acc: 0.0622  

epoch: 125 avg_loss: 3.0306, acc: 0.1270 
val --- avg_loss: 5.0827, acc: 0.0622  

epoch: 126 avg_loss: 3.0132, acc: 0.1336 
val --- avg_loss: 5.0904, acc: 0.0718  

epoch: 127 avg_loss: 3.0052, acc: 0.1348 
val --- avg_loss: 5.1223, acc: 0.0574  

epoch: 128 avg_loss: 3.0061, acc: 0.1342 
val --- avg_loss: 5.0939, acc: 0.0718  

epoch: 129 avg_loss: 3.0063, acc: 0.1345 
val --- avg_loss: 5.1504, acc: 0.0526  

epoch: 130 avg_loss: 3.0058, acc: 0.1328 
val --- avg_loss: 5.0700, acc: 0.0813  

epoch: 131 avg_loss: 3.0073, acc: 0.1325 
val --- avg_loss: 5.1352, acc: 0.0478  

epoch: 132 avg_loss: 3.0090, acc: 0.1348 
val --- avg_loss: 5.1074, acc: 0.0813  

epoch: 133 avg_loss: 3.0131, acc: 0.1336 
val --- avg_loss: 5.1262, acc: 0.0526  

epoch: 134 avg_loss: 3.0110, acc: 0.1391 
val --- avg_loss: 5.1060, acc: 0.0718  

epoch: 135 avg_loss: 3.0110, acc: 0.1411 
val --- avg_loss: 5.1156, acc: 0.0670  

epoch: 136 avg_loss: 3.0112, acc: 0.1359 
val --- avg_loss: 5.1113, acc: 0.0813  

epoch: 137 avg_loss: 3.0052, acc: 0.1342 
val --- avg_loss: 5.1325, acc: 0.0574  

epoch: 138 avg_loss: 2.9987, acc: 0.1371 
val --- avg_loss: 5.1322, acc: 0.0813  

epoch: 139 avg_loss: 2.9977, acc: 0.1311 
val --- avg_loss: 5.1061, acc: 0.0622  

epoch: 140 avg_loss: 3.0031, acc: 0.1374 
val --- avg_loss: 5.1251, acc: 0.0718  

epoch: 141 avg_loss: 3.0052, acc: 0.1328 
val --- avg_loss: 5.1117, acc: 0.0813  

epoch: 142 avg_loss: 2.9996, acc: 0.1356 
val --- avg_loss: 5.1359, acc: 0.0431  

epoch: 143 avg_loss: 2.9989, acc: 0.1348 
val --- avg_loss: 5.1272, acc: 0.0718  

epoch: 144 avg_loss: 3.0026, acc: 0.1391 
val --- avg_loss: 5.1613, acc: 0.0670  

epoch: 145 avg_loss: 3.0072, acc: 0.1288 
val --- avg_loss: 5.1489, acc: 0.0813  

epoch: 146 avg_loss: 3.0038, acc: 0.1313 
val --- avg_loss: 5.1505, acc: 0.0718  

epoch: 147 avg_loss: 2.9907, acc: 0.1377 
val --- avg_loss: 5.1429, acc: 0.0718  

epoch: 148 avg_loss: 2.9886, acc: 0.1368 
val --- avg_loss: 5.1424, acc: 0.0622  

epoch: 149 avg_loss: 2.9932, acc: 0.1339 
val --- avg_loss: 5.1479, acc: 0.0766  

epoch: 150 avg_loss: 2.9937, acc: 0.1299 
val --- avg_loss: 5.1664, acc: 0.0718  

epoch: 151 avg_loss: 3.0036, acc: 0.1399 
val --- avg_loss: 5.1728, acc: 0.0670  

epoch: 152 avg_loss: 3.0006, acc: 0.1316 
val --- avg_loss: 5.1567, acc: 0.0574  

epoch: 153 avg_loss: 2.9888, acc: 0.1334 
val --- avg_loss: 5.1412, acc: 0.0766  

epoch: 154 avg_loss: 2.9978, acc: 0.1302 
val --- avg_loss: 5.1710, acc: 0.0574  

epoch: 155 avg_loss: 2.9956, acc: 0.1399 
val --- avg_loss: 5.1562, acc: 0.0766  

epoch: 156 avg_loss: 2.9856, acc: 0.1354 
val --- avg_loss: 5.1908, acc: 0.0622  

epoch: 157 avg_loss: 2.9984, acc: 0.1291 
val --- avg_loss: 5.1719, acc: 0.0718  

epoch: 158 avg_loss: 3.0027, acc: 0.1368 
val --- avg_loss: 5.1554, acc: 0.0861  

epoch: 159 avg_loss: 2.9867, acc: 0.1345 
val --- avg_loss: 5.1692, acc: 0.0766  

epoch: 160 avg_loss: 2.9846, acc: 0.1371 
val --- avg_loss: 5.1834, acc: 0.0718  

epoch: 161 avg_loss: 2.9849, acc: 0.1351 
val --- avg_loss: 5.1746, acc: 0.0526  

epoch: 162 avg_loss: 2.9837, acc: 0.1339 
val --- avg_loss: 5.1667, acc: 0.0766  

epoch: 163 avg_loss: 3.0004, acc: 0.1348 
val --- avg_loss: 5.1772, acc: 0.0813  

epoch: 164 avg_loss: 2.9891, acc: 0.1371 
val --- avg_loss: 5.1797, acc: 0.0813  

epoch: 165 avg_loss: 2.9835, acc: 0.1339 
val --- avg_loss: 5.2097, acc: 0.0526  

epoch: 166 avg_loss: 2.9914, acc: 0.1313 
val --- avg_loss: 5.1916, acc: 0.0766  

epoch: 167 avg_loss: 2.9861, acc: 0.1351 
val --- avg_loss: 5.1940, acc: 0.0574  

epoch: 168 avg_loss: 2.9945, acc: 0.1302 
val --- avg_loss: 5.1807, acc: 0.0766  

epoch: 169 avg_loss: 2.9823, acc: 0.1354 
val --- avg_loss: 5.2028, acc: 0.0718  

epoch: 170 avg_loss: 2.9891, acc: 0.1405 
val --- avg_loss: 5.1852, acc: 0.0766  

epoch: 171 avg_loss: 2.9912, acc: 0.1311 
val --- avg_loss: 5.1887, acc: 0.0766  

epoch: 172 avg_loss: 2.9836, acc: 0.1420 
val --- avg_loss: 5.2295, acc: 0.0813  

epoch: 173 avg_loss: 2.9816, acc: 0.1331 
val --- avg_loss: 5.2064, acc: 0.0622  

epoch: 174 avg_loss: 2.9878, acc: 0.1319 
val --- avg_loss: 5.2013, acc: 0.0861  

epoch: 175 avg_loss: 2.9768, acc: 0.1339 
val --- avg_loss: 5.2099, acc: 0.0526  

epoch: 176 avg_loss: 2.9797, acc: 0.1388 
val --- avg_loss: 5.2174, acc: 0.0861  

epoch: 177 avg_loss: 2.9815, acc: 0.1342 
val --- avg_loss: 5.2328, acc: 0.0622  

epoch: 178 avg_loss: 2.9986, acc: 0.1259 
val --- avg_loss: 5.2051, acc: 0.0813  

epoch: 179 avg_loss: 2.9873, acc: 0.1311 
val --- avg_loss: 5.2485, acc: 0.0622  

epoch: 180 avg_loss: 2.9769, acc: 0.1339 
val --- avg_loss: 5.2243, acc: 0.0670  

epoch: 181 avg_loss: 2.9882, acc: 0.1374 
val --- avg_loss: 5.2481, acc: 0.0526  

epoch: 182 avg_loss: 2.9823, acc: 0.1342 
val --- avg_loss: 5.2244, acc: 0.0622  

epoch: 183 avg_loss: 2.9816, acc: 0.1316 
val --- avg_loss: 5.2241, acc: 0.0861  

epoch: 184 avg_loss: 2.9905, acc: 0.1319 
val --- avg_loss: 5.2328, acc: 0.0813  

epoch: 185 avg_loss: 2.9799, acc: 0.1388 
val --- avg_loss: 5.2349, acc: 0.0622  

epoch: 186 avg_loss: 2.9804, acc: 0.1348 
val --- avg_loss: 5.2321, acc: 0.0431  

epoch: 187 avg_loss: 2.9844, acc: 0.1296 
val --- avg_loss: 5.2424, acc: 0.0670  

epoch: 188 avg_loss: 2.9753, acc: 0.1365 
val --- avg_loss: 5.2463, acc: 0.0766  

epoch: 189 avg_loss: 2.9781, acc: 0.1374 
val --- avg_loss: 5.2496, acc: 0.0526  

epoch: 190 avg_loss: 2.9930, acc: 0.1293 
val --- avg_loss: 5.2340, acc: 0.0909  

epoch: 191 avg_loss: 2.9734, acc: 0.1331 
val --- avg_loss: 5.2496, acc: 0.0622  

epoch: 192 avg_loss: 2.9890, acc: 0.1379 
val --- avg_loss: 5.2660, acc: 0.0574  

epoch: 193 avg_loss: 2.9699, acc: 0.1322 
val --- avg_loss: 5.2456, acc: 0.0861  

epoch: 194 avg_loss: 2.9681, acc: 0.1354 
val --- avg_loss: 5.2379, acc: 0.0622  

epoch: 195 avg_loss: 2.9715, acc: 0.1374 
val --- avg_loss: 5.2460, acc: 0.0718  

epoch: 196 avg_loss: 2.9764, acc: 0.1345 
val --- avg_loss: 5.2413, acc: 0.0670  

epoch: 197 avg_loss: 2.9683, acc: 0.1379 
val --- avg_loss: 5.2381, acc: 0.0766  

epoch: 198 avg_loss: 2.9751, acc: 0.1348 
val --- avg_loss: 5.2701, acc: 0.0670  

epoch: 199 avg_loss: 2.9795, acc: 0.1342 
val --- avg_loss: 5.2554, acc: 0.0813  

epoch: 200 avg_loss: 2.9792, acc: 0.1368 
val --- avg_loss: 5.2641, acc: 0.0574  

epoch: 201 avg_loss: 2.9723, acc: 0.1356 
val --- avg_loss: 5.2613, acc: 0.0718  

epoch: 202 avg_loss: 2.9780, acc: 0.1411 
val --- avg_loss: 5.2650, acc: 0.0813  

epoch: 203 avg_loss: 2.9787, acc: 0.1399 
val --- avg_loss: 5.2622, acc: 0.0766  

epoch: 204 avg_loss: 2.9774, acc: 0.1359 
val --- avg_loss: 5.2855, acc: 0.0574  

epoch: 205 avg_loss: 2.9708, acc: 0.1325 
val --- avg_loss: 5.2460, acc: 0.0861  

epoch: 206 avg_loss: 2.9772, acc: 0.1391 
val --- avg_loss: 5.3007, acc: 0.0526  

epoch: 207 avg_loss: 2.9864, acc: 0.1322 
val --- avg_loss: 5.2673, acc: 0.0861  

epoch: 208 avg_loss: 2.9901, acc: 0.1354 
val --- avg_loss: 5.3383, acc: 0.0766  

epoch: 209 avg_loss: 2.9803, acc: 0.1379 
val --- avg_loss: 5.2692, acc: 0.0718  

epoch: 210 avg_loss: 2.9763, acc: 0.1377 
val --- avg_loss: 5.2728, acc: 0.0670  

epoch: 211 avg_loss: 2.9719, acc: 0.1348 
val --- avg_loss: 5.2838, acc: 0.0766  

epoch: 212 avg_loss: 2.9840, acc: 0.1345 
val --- avg_loss: 5.3028, acc: 0.0574  

epoch: 213 avg_loss: 2.9707, acc: 0.1313 
val --- avg_loss: 5.2814, acc: 0.0766  

epoch: 214 avg_loss: 2.9805, acc: 0.1365 
val --- avg_loss: 5.3017, acc: 0.0622  

epoch: 215 avg_loss: 2.9823, acc: 0.1342 
val --- avg_loss: 5.2698, acc: 0.0909  

epoch: 216 avg_loss: 2.9725, acc: 0.1417 
val --- avg_loss: 5.3193, acc: 0.0526  

epoch: 217 avg_loss: 2.9752, acc: 0.1328 
val --- avg_loss: 5.2800, acc: 0.0718  

epoch: 218 avg_loss: 2.9719, acc: 0.1296 
val --- avg_loss: 5.3428, acc: 0.0622  

epoch: 219 avg_loss: 2.9723, acc: 0.1359 
val --- avg_loss: 5.3057, acc: 0.0526  

epoch: 220 avg_loss: 2.9805, acc: 0.1313 
val --- avg_loss: 5.3155, acc: 0.0622  

epoch: 221 avg_loss: 2.9712, acc: 0.1345 
val --- avg_loss: 5.2995, acc: 0.0670  

epoch: 222 avg_loss: 2.9638, acc: 0.1313 
val --- avg_loss: 5.2864, acc: 0.0766  

epoch: 223 avg_loss: 2.9766, acc: 0.1316 
val --- avg_loss: 5.2935, acc: 0.0574  

epoch: 224 avg_loss: 2.9672, acc: 0.1339 
val --- avg_loss: 5.3142, acc: 0.0718  

epoch: 225 avg_loss: 2.9582, acc: 0.1362 
val --- avg_loss: 5.2972, acc: 0.0718  

epoch: 226 avg_loss: 2.9722, acc: 0.1325 
val --- avg_loss: 5.3213, acc: 0.0909  

epoch: 227 avg_loss: 2.9733, acc: 0.1328 
val --- avg_loss: 5.3176, acc: 0.0622  

epoch: 228 avg_loss: 2.9659, acc: 0.1362 
val --- avg_loss: 5.3136, acc: 0.0718  

epoch: 229 avg_loss: 2.9680, acc: 0.1368 
val --- avg_loss: 5.3377, acc: 0.0622  

epoch: 230 avg_loss: 2.9630, acc: 0.1356 
val --- avg_loss: 5.2890, acc: 0.0813  

epoch: 231 avg_loss: 2.9695, acc: 0.1359 
val --- avg_loss: 5.3526, acc: 0.0670  

epoch: 232 avg_loss: 2.9683, acc: 0.1342 
val --- avg_loss: 5.3193, acc: 0.0622  

epoch: 233 avg_loss: 2.9647, acc: 0.1382 
val --- avg_loss: 5.3159, acc: 0.0574  

epoch: 234 avg_loss: 2.9746, acc: 0.1313 
val --- avg_loss: 5.3012, acc: 0.0766  

epoch: 235 avg_loss: 2.9652, acc: 0.1351 
val --- avg_loss: 5.3248, acc: 0.0622  

epoch: 236 avg_loss: 2.9623, acc: 0.1379 
val --- avg_loss: 5.3001, acc: 0.0670  

epoch: 237 avg_loss: 2.9644, acc: 0.1336 
val --- avg_loss: 5.3516, acc: 0.0766  

epoch: 238 avg_loss: 2.9615, acc: 0.1302 
val --- avg_loss: 5.3219, acc: 0.0766  

epoch: 239 avg_loss: 2.9617, acc: 0.1397 
val --- avg_loss: 5.3565, acc: 0.0574  

epoch: 240 avg_loss: 2.9630, acc: 0.1331 
val --- avg_loss: 5.3210, acc: 0.0718  

epoch: 241 avg_loss: 2.9729, acc: 0.1339 
val --- avg_loss: 5.3430, acc: 0.0909  

epoch: 242 avg_loss: 2.9605, acc: 0.1348 
val --- avg_loss: 5.3244, acc: 0.0574  

epoch: 243 avg_loss: 2.9648, acc: 0.1371 
val --- avg_loss: 5.3144, acc: 0.0813  

epoch: 244 avg_loss: 2.9522, acc: 0.1425 
val --- avg_loss: 5.3550, acc: 0.0718  

epoch: 245 avg_loss: 2.9534, acc: 0.1394 
val --- avg_loss: 5.3501, acc: 0.0622  

epoch: 246 avg_loss: 2.9670, acc: 0.1308 
val --- avg_loss: 5.3365, acc: 0.0622  

epoch: 247 avg_loss: 2.9663, acc: 0.1397 
val --- avg_loss: 5.3319, acc: 0.0861  

epoch: 248 avg_loss: 2.9629, acc: 0.1382 
val --- avg_loss: 5.3643, acc: 0.0478  

epoch: 249 avg_loss: 2.9601, acc: 0.1356 
val --- avg_loss: 5.3864, acc: 0.0718  
