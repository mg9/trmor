
number of params: 361152 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/root_concept/results/training/vqvae_8_dict_sum_probe/3487_instances/200epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_8_dict_sum_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7f79d0fbfc90>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_8_dict_sum_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (linear): Linear(in_features=512, out_features=704, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_8_dict_sum_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=512, bias=True)
  (linear_3): Linear(in_features=512, out_features=512, bias=True)
  (linear_4): Linear(in_features=512, out_features=512, bias=True)
  (linear_5): Linear(in_features=512, out_features=512, bias=True)
  (linear_6): Linear(in_features=512, out_features=512, bias=True)
  (linear_7): Linear(in_features=512, out_features=512, bias=True)
  (linear_8): Linear(in_features=512, out_features=512, bias=True)
  (linear_9): Linear(in_features=512, out_features=512, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(32, 512)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/root_concept/results/training/vqvae_8_dict_sum_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([512, 512]): False
linear_2.bias, torch.Size([512]): False
linear_3.weight, torch.Size([512, 512]): False
linear_3.bias, torch.Size([512]): False
linear_4.weight, torch.Size([512, 512]): False
linear_4.bias, torch.Size([512]): False
linear_5.weight, torch.Size([512, 512]): False
linear_5.bias, torch.Size([512]): False
linear_6.weight, torch.Size([512, 512]): False
linear_6.bias, torch.Size([512]): False
linear_7.weight, torch.Size([512, 512]): False
linear_7.bias, torch.Size([512]): False
linear_8.weight, torch.Size([512, 512]): False
linear_8.bias, torch.Size([512]): False
vq_layer_2.embedding.weight, torch.Size([32, 512]): False
vq_layer_3.embedding.weight, torch.Size([32, 512]): False
vq_layer_4.embedding.weight, torch.Size([32, 512]): False
vq_layer_5.embedding.weight, torch.Size([32, 512]): False
vq_layer_6.embedding.weight, torch.Size([32, 512]): False
vq_layer_7.embedding.weight, torch.Size([32, 512]): False
vq_layer_8.embedding.weight, torch.Size([32, 512]): False
linear.weight, torch.Size([704, 512]): True
linear.bias, torch.Size([704]): True
epoch: 0 avg_loss: 8.3615, acc: 0.0361 

epoch: 1 avg_loss: 6.8654, acc: 0.0356 

epoch: 2 avg_loss: 7.5456, acc: 0.0416 

epoch: 3 avg_loss: 7.0434, acc: 0.0376 

epoch: 4 avg_loss: 6.9635, acc: 0.0399 

epoch: 5 avg_loss: 7.0717, acc: 0.0384 

epoch: 6 avg_loss: 6.8225, acc: 0.0447 

epoch: 7 avg_loss: 6.9145, acc: 0.0384 

epoch: 8 avg_loss: 6.9172, acc: 0.0456 

epoch: 9 avg_loss: 7.3179, acc: 0.0364 

epoch: 10 avg_loss: 7.4316, acc: 0.0436 

epoch: 11 avg_loss: 7.2780, acc: 0.0433 

epoch: 12 avg_loss: 6.9089, acc: 0.0370 

epoch: 13 avg_loss: 6.7571, acc: 0.0419 

epoch: 14 avg_loss: 7.0175, acc: 0.0404 

epoch: 15 avg_loss: 7.6090, acc: 0.0341 

epoch: 16 avg_loss: 7.1010, acc: 0.0387 

epoch: 17 avg_loss: 7.1723, acc: 0.0436 

epoch: 18 avg_loss: 7.0631, acc: 0.0393 

epoch: 19 avg_loss: 7.1591, acc: 0.0427 

epoch: 20 avg_loss: 7.5139, acc: 0.0407 

epoch: 21 avg_loss: 7.3581, acc: 0.0384 

epoch: 22 avg_loss: 7.2556, acc: 0.0430 

epoch: 23 avg_loss: 7.4892, acc: 0.0410 

epoch: 24 avg_loss: 7.4062, acc: 0.0376 

epoch: 25 avg_loss: 7.4902, acc: 0.0490 

epoch: 26 avg_loss: 7.1336, acc: 0.0327 

epoch: 27 avg_loss: 7.0438, acc: 0.0422 

epoch: 28 avg_loss: 7.3800, acc: 0.0410 

epoch: 29 avg_loss: 7.2874, acc: 0.0410 

epoch: 30 avg_loss: 7.4784, acc: 0.0376 

epoch: 31 avg_loss: 6.9937, acc: 0.0399 

epoch: 32 avg_loss: 7.0533, acc: 0.0373 

epoch: 33 avg_loss: 7.0137, acc: 0.0467 

epoch: 34 avg_loss: 7.2967, acc: 0.0396 

epoch: 35 avg_loss: 7.4936, acc: 0.0356 

epoch: 36 avg_loss: 7.5108, acc: 0.0373 

epoch: 37 avg_loss: 7.2920, acc: 0.0327 

epoch: 38 avg_loss: 7.1768, acc: 0.0396 

epoch: 39 avg_loss: 7.1941, acc: 0.0367 

epoch: 40 avg_loss: 6.9681, acc: 0.0399 

epoch: 41 avg_loss: 7.1687, acc: 0.0436 

epoch: 42 avg_loss: 7.1325, acc: 0.0404 

epoch: 43 avg_loss: 7.3250, acc: 0.0422 

epoch: 44 avg_loss: 7.1935, acc: 0.0387 

epoch: 45 avg_loss: 7.4461, acc: 0.0364 

epoch: 46 avg_loss: 7.4519, acc: 0.0381 

epoch: 47 avg_loss: 7.4458, acc: 0.0401 

epoch: 48 avg_loss: 7.3636, acc: 0.0430 

epoch: 49 avg_loss: 7.2291, acc: 0.0379 

epoch: 50 avg_loss: 7.4104, acc: 0.0467 

epoch: 51 avg_loss: 7.6045, acc: 0.0327 

epoch: 52 avg_loss: 7.1791, acc: 0.0387 

epoch: 53 avg_loss: 7.6185, acc: 0.0407 

epoch: 54 avg_loss: 7.2480, acc: 0.0407 

epoch: 55 avg_loss: 7.3276, acc: 0.0407 

epoch: 56 avg_loss: 7.3202, acc: 0.0384 

epoch: 57 avg_loss: 7.3205, acc: 0.0347 

epoch: 58 avg_loss: 7.3481, acc: 0.0436 

epoch: 59 avg_loss: 7.3552, acc: 0.0295 

epoch: 60 avg_loss: 7.2480, acc: 0.0330 

epoch: 61 avg_loss: 7.3818, acc: 0.0344 

epoch: 62 avg_loss: 7.0410, acc: 0.0447 

epoch: 63 avg_loss: 7.2926, acc: 0.0356 

epoch: 64 avg_loss: 6.9004, acc: 0.0407 

epoch: 65 avg_loss: 7.2380, acc: 0.0384 

epoch: 66 avg_loss: 7.2814, acc: 0.0347 

epoch: 67 avg_loss: 7.1703, acc: 0.0433 

epoch: 68 avg_loss: 7.0792, acc: 0.0439 

epoch: 69 avg_loss: 6.9382, acc: 0.0324 

epoch: 70 avg_loss: 7.5769, acc: 0.0404 

epoch: 71 avg_loss: 7.0416, acc: 0.0401 

epoch: 72 avg_loss: 7.3487, acc: 0.0419 

epoch: 73 avg_loss: 7.5397, acc: 0.0318 

epoch: 74 avg_loss: 7.1934, acc: 0.0430 

epoch: 75 avg_loss: 7.5546, acc: 0.0370 

epoch: 76 avg_loss: 7.3028, acc: 0.0373 

epoch: 77 avg_loss: 7.1338, acc: 0.0416 

epoch: 78 avg_loss: 7.3450, acc: 0.0358 

epoch: 79 avg_loss: 7.2091, acc: 0.0358 

epoch: 80 avg_loss: 7.3810, acc: 0.0387 

epoch: 81 avg_loss: 7.4350, acc: 0.0333 

epoch: 82 avg_loss: 7.0859, acc: 0.0367 

epoch: 83 avg_loss: 7.3605, acc: 0.0347 

epoch: 84 avg_loss: 7.1031, acc: 0.0390 

epoch: 85 avg_loss: 7.1661, acc: 0.0367 

epoch: 86 avg_loss: 7.1511, acc: 0.0361 

epoch: 87 avg_loss: 7.5332, acc: 0.0387 

epoch: 88 avg_loss: 7.1506, acc: 0.0427 

epoch: 89 avg_loss: 7.1760, acc: 0.0379 

epoch: 90 avg_loss: 7.4398, acc: 0.0379 

epoch: 91 avg_loss: 7.3475, acc: 0.0367 

epoch: 92 avg_loss: 7.8501, acc: 0.0393 

epoch: 93 avg_loss: 7.4542, acc: 0.0356 

epoch: 94 avg_loss: 7.2892, acc: 0.0370 

epoch: 95 avg_loss: 7.5700, acc: 0.0401 

epoch: 96 avg_loss: 7.4459, acc: 0.0370 

epoch: 97 avg_loss: 7.3989, acc: 0.0399 

epoch: 98 avg_loss: 7.4702, acc: 0.0401 

epoch: 99 avg_loss: 7.2567, acc: 0.0473 

epoch: 100 avg_loss: 7.4009, acc: 0.0384 

epoch: 101 avg_loss: 7.6722, acc: 0.0376 

epoch: 102 avg_loss: 7.2860, acc: 0.0404 

epoch: 103 avg_loss: 7.1894, acc: 0.0427 

epoch: 104 avg_loss: 7.2019, acc: 0.0358 

epoch: 105 avg_loss: 7.4379, acc: 0.0379 

epoch: 106 avg_loss: 6.9487, acc: 0.0430 

epoch: 107 avg_loss: 7.0215, acc: 0.0364 

epoch: 108 avg_loss: 7.3044, acc: 0.0404 

epoch: 109 avg_loss: 7.4284, acc: 0.0384 

epoch: 110 avg_loss: 7.2562, acc: 0.0422 

epoch: 111 avg_loss: 7.4616, acc: 0.0379 

epoch: 112 avg_loss: 7.6326, acc: 0.0416 

epoch: 113 avg_loss: 7.2799, acc: 0.0370 

epoch: 114 avg_loss: 7.4311, acc: 0.0353 

epoch: 115 avg_loss: 7.1801, acc: 0.0338 

epoch: 116 avg_loss: 7.2438, acc: 0.0275 

epoch: 117 avg_loss: 7.0454, acc: 0.0427 

epoch: 118 avg_loss: 7.4731, acc: 0.0407 

epoch: 119 avg_loss: 7.3515, acc: 0.0353 

epoch: 120 avg_loss: 7.3598, acc: 0.0410 

epoch: 121 avg_loss: 7.3614, acc: 0.0376 

epoch: 122 avg_loss: 7.4940, acc: 0.0416 

epoch: 123 avg_loss: 7.3777, acc: 0.0384 

epoch: 124 avg_loss: 7.1886, acc: 0.0379 

epoch: 125 avg_loss: 7.2890, acc: 0.0330 

epoch: 126 avg_loss: 7.5284, acc: 0.0373 

epoch: 127 avg_loss: 7.2732, acc: 0.0384 

epoch: 128 avg_loss: 7.2589, acc: 0.0381 

epoch: 129 avg_loss: 7.2535, acc: 0.0361 

epoch: 130 avg_loss: 7.2287, acc: 0.0430 

epoch: 131 avg_loss: 7.1740, acc: 0.0473 

epoch: 132 avg_loss: 7.2493, acc: 0.0393 

epoch: 133 avg_loss: 7.2939, acc: 0.0373 

epoch: 134 avg_loss: 7.2534, acc: 0.0321 

epoch: 135 avg_loss: 7.2179, acc: 0.0341 

epoch: 136 avg_loss: 7.6221, acc: 0.0376 

epoch: 137 avg_loss: 7.2649, acc: 0.0370 

epoch: 138 avg_loss: 6.9710, acc: 0.0416 

epoch: 139 avg_loss: 7.1907, acc: 0.0376 

epoch: 140 avg_loss: 7.4127, acc: 0.0390 

epoch: 141 avg_loss: 7.1708, acc: 0.0318 

epoch: 142 avg_loss: 7.3050, acc: 0.0436 

epoch: 143 avg_loss: 7.2587, acc: 0.0479 

epoch: 144 avg_loss: 7.2005, acc: 0.0373 

epoch: 145 avg_loss: 7.2807, acc: 0.0404 

epoch: 146 avg_loss: 7.5406, acc: 0.0350 

epoch: 147 avg_loss: 7.4967, acc: 0.0379 

epoch: 148 avg_loss: 7.0928, acc: 0.0436 

epoch: 149 avg_loss: 7.2309, acc: 0.0450 

epoch: 150 avg_loss: 7.4221, acc: 0.0387 

epoch: 151 avg_loss: 7.0231, acc: 0.0436 

epoch: 152 avg_loss: 7.1099, acc: 0.0384 

epoch: 153 avg_loss: 7.2195, acc: 0.0424 

epoch: 154 avg_loss: 7.1377, acc: 0.0384 

epoch: 155 avg_loss: 7.0833, acc: 0.0404 

epoch: 156 avg_loss: 7.3454, acc: 0.0381 

epoch: 157 avg_loss: 7.1649, acc: 0.0364 

epoch: 158 avg_loss: 7.2531, acc: 0.0356 

epoch: 159 avg_loss: 6.9996, acc: 0.0396 

epoch: 160 avg_loss: 6.9578, acc: 0.0358 

epoch: 161 avg_loss: 7.1499, acc: 0.0404 

epoch: 162 avg_loss: 7.0369, acc: 0.0390 

epoch: 163 avg_loss: 7.4911, acc: 0.0338 

epoch: 164 avg_loss: 7.0961, acc: 0.0413 

epoch: 165 avg_loss: 7.1455, acc: 0.0356 

epoch: 166 avg_loss: 7.3197, acc: 0.0361 

epoch: 167 avg_loss: 7.3164, acc: 0.0379 

epoch: 168 avg_loss: 7.0186, acc: 0.0413 

epoch: 169 avg_loss: 7.2884, acc: 0.0445 

epoch: 170 avg_loss: 7.0424, acc: 0.0427 

epoch: 171 avg_loss: 7.2924, acc: 0.0433 

epoch: 172 avg_loss: 7.5727, acc: 0.0373 

epoch: 173 avg_loss: 7.1493, acc: 0.0310 

epoch: 174 avg_loss: 7.3776, acc: 0.0416 

epoch: 175 avg_loss: 7.0617, acc: 0.0447 

epoch: 176 avg_loss: 7.8532, acc: 0.0482 

epoch: 177 avg_loss: 7.7219, acc: 0.0422 

epoch: 178 avg_loss: 7.3700, acc: 0.0384 

epoch: 179 avg_loss: 7.5702, acc: 0.0422 

epoch: 180 avg_loss: 7.2181, acc: 0.0399 

epoch: 181 avg_loss: 7.0567, acc: 0.0353 

epoch: 182 avg_loss: 7.1221, acc: 0.0396 

epoch: 183 avg_loss: 7.4889, acc: 0.0424 

epoch: 184 avg_loss: 7.4320, acc: 0.0436 

epoch: 185 avg_loss: 7.2810, acc: 0.0370 

epoch: 186 avg_loss: 7.5950, acc: 0.0410 

epoch: 187 avg_loss: 7.2753, acc: 0.0384 

epoch: 188 avg_loss: 7.3487, acc: 0.0453 

epoch: 189 avg_loss: 7.0443, acc: 0.0384 

epoch: 190 avg_loss: 7.2123, acc: 0.0358 

epoch: 191 avg_loss: 7.2671, acc: 0.0356 

epoch: 192 avg_loss: 7.6855, acc: 0.0427 

epoch: 193 avg_loss: 7.2689, acc: 0.0439 

epoch: 194 avg_loss: 7.4007, acc: 0.0427 

epoch: 195 avg_loss: 7.5953, acc: 0.0364 

epoch: 196 avg_loss: 7.4523, acc: 0.0364 

epoch: 197 avg_loss: 7.1130, acc: 0.0376 

epoch: 198 avg_loss: 7.7840, acc: 0.0396 

epoch: 199 avg_loss: 7.3933, acc: 0.0404 
