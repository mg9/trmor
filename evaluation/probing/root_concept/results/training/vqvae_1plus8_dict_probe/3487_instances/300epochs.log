
number of params: 360448 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=300, fig_path='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/300epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/300epochs.log', logger=<common.utils.Logger object at 0x7fcde7898150>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_1plus8_dict_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=64, bias=True)
  (linear_3): Linear(in_features=512, out_features=64, bias=True)
  (linear_4): Linear(in_features=512, out_features=64, bias=True)
  (linear_5): Linear(in_features=512, out_features=64, bias=True)
  (linear_6): Linear(in_features=512, out_features=64, bias=True)
  (linear_7): Linear(in_features=512, out_features=64, bias=True)
  (linear_8): Linear(in_features=512, out_features=64, bias=True)
  (linear_9): Linear(in_features=512, out_features=64, bias=True)
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (linear): Linear(in_features=512, out_features=704, bias=False)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (linear_2): Linear(in_features=512, out_features=64, bias=True)
  (linear_3): Linear(in_features=512, out_features=64, bias=True)
  (linear_4): Linear(in_features=512, out_features=64, bias=True)
  (linear_5): Linear(in_features=512, out_features=64, bias=True)
  (linear_6): Linear(in_features=512, out_features=64, bias=True)
  (linear_7): Linear(in_features=512, out_features=64, bias=True)
  (linear_8): Linear(in_features=512, out_features=64, bias=True)
  (linear_9): Linear(in_features=512, out_features=64, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/300epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([64, 512]): False
linear_2.bias, torch.Size([64]): False
linear_3.weight, torch.Size([64, 512]): False
linear_3.bias, torch.Size([64]): False
linear_4.weight, torch.Size([64, 512]): False
linear_4.bias, torch.Size([64]): False
linear_5.weight, torch.Size([64, 512]): False
linear_5.bias, torch.Size([64]): False
linear_6.weight, torch.Size([64, 512]): False
linear_6.bias, torch.Size([64]): False
linear_7.weight, torch.Size([64, 512]): False
linear_7.bias, torch.Size([64]): False
linear_8.weight, torch.Size([64, 512]): False
linear_8.bias, torch.Size([64]): False
linear_9.weight, torch.Size([64, 512]): False
linear_9.bias, torch.Size([64]): False
vq_layer.embedding.weight, torch.Size([704, 512]): False
vq_layer_2.embedding.weight, torch.Size([16, 64]): False
vq_layer_3.embedding.weight, torch.Size([16, 64]): False
vq_layer_4.embedding.weight, torch.Size([16, 64]): False
vq_layer_5.embedding.weight, torch.Size([16, 64]): False
vq_layer_6.embedding.weight, torch.Size([16, 64]): False
vq_layer_7.embedding.weight, torch.Size([16, 64]): False
vq_layer_8.embedding.weight, torch.Size([16, 64]): False
vq_layer_9.embedding.weight, torch.Size([16, 64]): False
linear.weight, torch.Size([704, 512]): True
epoch: 0 avg_loss: 6.8913, acc: 0.0155 

epoch: 1 avg_loss: 4.9932, acc: 0.0783 

epoch: 2 avg_loss: 4.2137, acc: 0.1695 

epoch: 3 avg_loss: 3.9444, acc: 0.2013 

epoch: 4 avg_loss: 3.7882, acc: 0.2260 

epoch: 5 avg_loss: 3.6426, acc: 0.2495 

epoch: 6 avg_loss: 3.5462, acc: 0.2656 

epoch: 7 avg_loss: 3.4658, acc: 0.2673 

epoch: 8 avg_loss: 3.3872, acc: 0.2810 

epoch: 9 avg_loss: 3.3176, acc: 0.2928 

epoch: 10 avg_loss: 3.2833, acc: 0.2988 

epoch: 11 avg_loss: 3.2285, acc: 0.3069 

epoch: 12 avg_loss: 3.1657, acc: 0.3086 

epoch: 13 avg_loss: 3.1790, acc: 0.3192 

epoch: 14 avg_loss: 3.1175, acc: 0.3178 

epoch: 15 avg_loss: 3.0704, acc: 0.3321 

epoch: 16 avg_loss: 3.1027, acc: 0.3189 

epoch: 17 avg_loss: 3.0703, acc: 0.3315 

epoch: 18 avg_loss: 3.0432, acc: 0.3324 

epoch: 19 avg_loss: 3.0989, acc: 0.3284 

epoch: 20 avg_loss: 3.0521, acc: 0.3332 

epoch: 21 avg_loss: 2.9505, acc: 0.3450 

epoch: 22 avg_loss: 2.9348, acc: 0.3398 

epoch: 23 avg_loss: 2.8983, acc: 0.3599 

epoch: 24 avg_loss: 2.9592, acc: 0.3395 

epoch: 25 avg_loss: 2.9264, acc: 0.3479 

epoch: 26 avg_loss: 2.9323, acc: 0.3525 

epoch: 27 avg_loss: 2.8721, acc: 0.3565 

epoch: 28 avg_loss: 2.8722, acc: 0.3438 

epoch: 29 avg_loss: 2.8650, acc: 0.3542 

epoch: 30 avg_loss: 2.8454, acc: 0.3714 

epoch: 31 avg_loss: 2.8320, acc: 0.3556 

epoch: 32 avg_loss: 2.7748, acc: 0.3708 

epoch: 33 avg_loss: 2.7855, acc: 0.3588 

epoch: 34 avg_loss: 2.7747, acc: 0.3608 

epoch: 35 avg_loss: 2.7667, acc: 0.3717 

epoch: 36 avg_loss: 2.8007, acc: 0.3679 

epoch: 37 avg_loss: 2.7783, acc: 0.3677 

epoch: 38 avg_loss: 2.7188, acc: 0.3806 

epoch: 39 avg_loss: 2.6872, acc: 0.3806 

epoch: 40 avg_loss: 2.7585, acc: 0.3625 

epoch: 41 avg_loss: 2.6796, acc: 0.3843 

epoch: 42 avg_loss: 2.6830, acc: 0.3783 

epoch: 43 avg_loss: 2.6958, acc: 0.3791 

epoch: 44 avg_loss: 2.6443, acc: 0.3843 

epoch: 45 avg_loss: 2.6555, acc: 0.3849 

epoch: 46 avg_loss: 2.6537, acc: 0.3837 

epoch: 47 avg_loss: 2.6358, acc: 0.3883 

epoch: 48 avg_loss: 2.6512, acc: 0.3800 

epoch: 49 avg_loss: 2.6044, acc: 0.3866 

epoch: 50 avg_loss: 2.6090, acc: 0.3980 

epoch: 51 avg_loss: 2.5878, acc: 0.3880 

epoch: 52 avg_loss: 2.6496, acc: 0.3946 

epoch: 53 avg_loss: 2.5657, acc: 0.4001 

epoch: 54 avg_loss: 2.5937, acc: 0.3894 

epoch: 55 avg_loss: 2.6090, acc: 0.3817 

epoch: 56 avg_loss: 2.5282, acc: 0.4081 

epoch: 57 avg_loss: 2.5229, acc: 0.3995 

epoch: 58 avg_loss: 2.5596, acc: 0.3960 

epoch: 59 avg_loss: 2.5265, acc: 0.3969 

epoch: 60 avg_loss: 2.5568, acc: 0.3995 

epoch: 61 avg_loss: 2.5041, acc: 0.4026 

epoch: 62 avg_loss: 2.5596, acc: 0.3903 

epoch: 63 avg_loss: 2.4976, acc: 0.3995 

epoch: 64 avg_loss: 2.5027, acc: 0.4155 

epoch: 65 avg_loss: 2.4938, acc: 0.4021 

epoch: 66 avg_loss: 2.4651, acc: 0.4161 

epoch: 67 avg_loss: 2.5387, acc: 0.4009 

epoch: 68 avg_loss: 2.5278, acc: 0.4015 

epoch: 69 avg_loss: 2.4795, acc: 0.4101 

epoch: 70 avg_loss: 2.4901, acc: 0.4069 

epoch: 71 avg_loss: 2.4383, acc: 0.4112 

epoch: 72 avg_loss: 2.4575, acc: 0.4072 

epoch: 73 avg_loss: 2.4288, acc: 0.4187 

epoch: 74 avg_loss: 2.4289, acc: 0.4124 

epoch: 75 avg_loss: 2.4156, acc: 0.4164 

epoch: 76 avg_loss: 2.4078, acc: 0.4204 

epoch: 77 avg_loss: 2.4311, acc: 0.4173 

epoch: 78 avg_loss: 2.4009, acc: 0.4107 

epoch: 79 avg_loss: 2.3765, acc: 0.4233 

epoch: 80 avg_loss: 2.3920, acc: 0.4150 

epoch: 81 avg_loss: 2.4180, acc: 0.4181 

epoch: 82 avg_loss: 2.3623, acc: 0.4170 

epoch: 83 avg_loss: 2.4186, acc: 0.4158 

epoch: 84 avg_loss: 2.3718, acc: 0.4279 

epoch: 85 avg_loss: 2.3466, acc: 0.4204 

epoch: 86 avg_loss: 2.3607, acc: 0.4224 

epoch: 87 avg_loss: 2.3520, acc: 0.4333 

epoch: 88 avg_loss: 2.3861, acc: 0.4264 

epoch: 89 avg_loss: 2.3069, acc: 0.4264 

epoch: 90 avg_loss: 2.3683, acc: 0.4164 

epoch: 91 avg_loss: 2.3811, acc: 0.4121 

epoch: 92 avg_loss: 2.3800, acc: 0.4207 

epoch: 93 avg_loss: 2.4684, acc: 0.3983 

epoch: 94 avg_loss: 2.4194, acc: 0.4009 

epoch: 95 avg_loss: 2.3564, acc: 0.4130 

epoch: 96 avg_loss: 2.3631, acc: 0.4216 

epoch: 97 avg_loss: 2.3471, acc: 0.4256 

epoch: 98 avg_loss: 2.3484, acc: 0.4170 

epoch: 99 avg_loss: 2.3233, acc: 0.4287 

epoch: 100 avg_loss: 2.3391, acc: 0.4181 

epoch: 101 avg_loss: 2.2838, acc: 0.4345 

epoch: 102 avg_loss: 2.3091, acc: 0.4287 

epoch: 103 avg_loss: 2.2757, acc: 0.4253 

epoch: 104 avg_loss: 2.2819, acc: 0.4365 

epoch: 105 avg_loss: 2.3334, acc: 0.4244 

epoch: 106 avg_loss: 2.2220, acc: 0.4477 

epoch: 107 avg_loss: 2.2206, acc: 0.4445 

epoch: 108 avg_loss: 2.2072, acc: 0.4451 

epoch: 109 avg_loss: 2.2092, acc: 0.4391 

epoch: 110 avg_loss: 2.2286, acc: 0.4465 

epoch: 111 avg_loss: 2.2136, acc: 0.4411 

epoch: 112 avg_loss: 2.2156, acc: 0.4485 

epoch: 113 avg_loss: 2.2309, acc: 0.4388 

epoch: 114 avg_loss: 2.1624, acc: 0.4531 

epoch: 115 avg_loss: 2.2254, acc: 0.4408 

epoch: 116 avg_loss: 2.2208, acc: 0.4371 

epoch: 117 avg_loss: 2.1593, acc: 0.4528 

epoch: 118 avg_loss: 2.1606, acc: 0.4540 

epoch: 119 avg_loss: 2.2609, acc: 0.4348 

epoch: 120 avg_loss: 2.1835, acc: 0.4497 

epoch: 121 avg_loss: 2.2031, acc: 0.4474 

epoch: 122 avg_loss: 2.1459, acc: 0.4523 

epoch: 123 avg_loss: 2.1854, acc: 0.4514 

epoch: 124 avg_loss: 2.1315, acc: 0.4471 

epoch: 125 avg_loss: 2.1800, acc: 0.4574 

epoch: 126 avg_loss: 2.1295, acc: 0.4643 

epoch: 127 avg_loss: 2.1824, acc: 0.4543 

epoch: 128 avg_loss: 2.1614, acc: 0.4442 

epoch: 129 avg_loss: 2.1175, acc: 0.4617 

epoch: 130 avg_loss: 2.2067, acc: 0.4382 

epoch: 131 avg_loss: 2.2002, acc: 0.4468 

epoch: 132 avg_loss: 2.1671, acc: 0.4436 

epoch: 133 avg_loss: 2.1576, acc: 0.4491 

epoch: 134 avg_loss: 2.2338, acc: 0.4511 

epoch: 135 avg_loss: 2.1612, acc: 0.4436 

epoch: 136 avg_loss: 2.1735, acc: 0.4454 

epoch: 137 avg_loss: 2.2936, acc: 0.4264 

epoch: 138 avg_loss: 2.1454, acc: 0.4505 

epoch: 139 avg_loss: 2.2041, acc: 0.4350 

epoch: 140 avg_loss: 2.1932, acc: 0.4425 

epoch: 141 avg_loss: 2.0901, acc: 0.4591 

epoch: 142 avg_loss: 2.1024, acc: 0.4606 

epoch: 143 avg_loss: 2.0614, acc: 0.4697 

epoch: 144 avg_loss: 2.0830, acc: 0.4657 

epoch: 145 avg_loss: 2.0190, acc: 0.4783 

epoch: 146 avg_loss: 2.0511, acc: 0.4672 

epoch: 147 avg_loss: 2.0926, acc: 0.4629 

epoch: 148 avg_loss: 2.0957, acc: 0.4631 

epoch: 149 avg_loss: 2.0709, acc: 0.4591 

epoch: 150 avg_loss: 2.0594, acc: 0.4672 

epoch: 151 avg_loss: 2.1099, acc: 0.4545 

epoch: 152 avg_loss: 2.0293, acc: 0.4738 

epoch: 153 avg_loss: 2.0145, acc: 0.4740 

epoch: 154 avg_loss: 2.0280, acc: 0.4740 

epoch: 155 avg_loss: 2.0757, acc: 0.4626 

epoch: 156 avg_loss: 2.0534, acc: 0.4692 

epoch: 157 avg_loss: 2.0723, acc: 0.4609 

epoch: 158 avg_loss: 2.0236, acc: 0.4861 

epoch: 159 avg_loss: 1.9597, acc: 0.4901 

epoch: 160 avg_loss: 1.9919, acc: 0.4801 

epoch: 161 avg_loss: 1.9843, acc: 0.4881 

epoch: 162 avg_loss: 2.0105, acc: 0.4738 

epoch: 163 avg_loss: 1.9984, acc: 0.4801 

epoch: 164 avg_loss: 2.0321, acc: 0.4683 

epoch: 165 avg_loss: 1.9858, acc: 0.4870 

epoch: 166 avg_loss: 1.9811, acc: 0.4838 

epoch: 167 avg_loss: 2.0272, acc: 0.4703 

epoch: 168 avg_loss: 2.0187, acc: 0.4703 

epoch: 169 avg_loss: 1.9913, acc: 0.4792 

epoch: 170 avg_loss: 1.9750, acc: 0.4918 

epoch: 171 avg_loss: 1.9853, acc: 0.4852 

epoch: 172 avg_loss: 2.0121, acc: 0.4749 

epoch: 173 avg_loss: 1.9960, acc: 0.4798 

epoch: 174 avg_loss: 1.9906, acc: 0.4809 

epoch: 175 avg_loss: 1.9881, acc: 0.4783 

epoch: 176 avg_loss: 2.0449, acc: 0.4649 

epoch: 177 avg_loss: 2.0370, acc: 0.4631 

epoch: 178 avg_loss: 2.0058, acc: 0.4858 

epoch: 179 avg_loss: 2.0042, acc: 0.4740 

epoch: 180 avg_loss: 2.0579, acc: 0.4643 

epoch: 181 avg_loss: 2.0466, acc: 0.4591 

epoch: 182 avg_loss: 2.0118, acc: 0.4735 

epoch: 183 avg_loss: 2.0166, acc: 0.4769 

epoch: 184 avg_loss: 2.0221, acc: 0.4669 

epoch: 185 avg_loss: 2.0200, acc: 0.4743 

epoch: 186 avg_loss: 2.1502, acc: 0.4457 

epoch: 187 avg_loss: 2.0465, acc: 0.4614 

epoch: 188 avg_loss: 2.0098, acc: 0.4775 

epoch: 189 avg_loss: 2.0615, acc: 0.4686 

epoch: 190 avg_loss: 2.1110, acc: 0.4520 

epoch: 191 avg_loss: 1.9896, acc: 0.4841 

epoch: 192 avg_loss: 1.9181, acc: 0.4884 

epoch: 193 avg_loss: 1.9191, acc: 0.4867 

epoch: 194 avg_loss: 1.9030, acc: 0.4806 

epoch: 195 avg_loss: 1.8912, acc: 0.5039 

epoch: 196 avg_loss: 1.8858, acc: 0.5001 

epoch: 197 avg_loss: 1.8916, acc: 0.5016 

epoch: 198 avg_loss: 1.8958, acc: 0.4938 

epoch: 199 avg_loss: 1.8836, acc: 0.4987 

epoch: 200 avg_loss: 1.9186, acc: 0.4872 

epoch: 201 avg_loss: 1.8930, acc: 0.4958 

epoch: 202 avg_loss: 1.8949, acc: 0.4918 

epoch: 203 avg_loss: 1.8683, acc: 0.5022 

epoch: 204 avg_loss: 1.8582, acc: 0.5099 

epoch: 205 avg_loss: 1.8383, acc: 0.5096 

epoch: 206 avg_loss: 1.8576, acc: 0.5136 

epoch: 207 avg_loss: 1.8785, acc: 0.4913 

epoch: 208 avg_loss: 1.9097, acc: 0.4918 

epoch: 209 avg_loss: 1.8510, acc: 0.5044 

epoch: 210 avg_loss: 1.8745, acc: 0.5085 

epoch: 211 avg_loss: 1.8592, acc: 0.5042 

epoch: 212 avg_loss: 1.8569, acc: 0.5004 

epoch: 213 avg_loss: 1.8664, acc: 0.5130 

epoch: 214 avg_loss: 1.8419, acc: 0.5076 

epoch: 215 avg_loss: 1.8841, acc: 0.4999 

epoch: 216 avg_loss: 1.8396, acc: 0.5073 

epoch: 217 avg_loss: 1.8425, acc: 0.5024 

epoch: 218 avg_loss: 1.8782, acc: 0.4884 

epoch: 219 avg_loss: 1.8588, acc: 0.5024 

epoch: 220 avg_loss: 1.8282, acc: 0.5062 

epoch: 221 avg_loss: 1.8236, acc: 0.5113 

epoch: 222 avg_loss: 1.8032, acc: 0.5148 

epoch: 223 avg_loss: 1.8244, acc: 0.5151 

epoch: 224 avg_loss: 1.8631, acc: 0.4910 

epoch: 225 avg_loss: 1.8110, acc: 0.5208 

epoch: 226 avg_loss: 1.7889, acc: 0.5162 

epoch: 227 avg_loss: 1.8570, acc: 0.4930 

epoch: 228 avg_loss: 1.8802, acc: 0.5019 

epoch: 229 avg_loss: 1.8754, acc: 0.4973 

epoch: 230 avg_loss: 1.8598, acc: 0.5096 

epoch: 231 avg_loss: 1.8631, acc: 0.4947 

epoch: 232 avg_loss: 1.8460, acc: 0.5001 

epoch: 233 avg_loss: 1.8397, acc: 0.5047 

epoch: 234 avg_loss: 1.8825, acc: 0.5010 

epoch: 235 avg_loss: 1.8479, acc: 0.5024 

epoch: 236 avg_loss: 1.8001, acc: 0.5093 

epoch: 237 avg_loss: 1.8199, acc: 0.5010 

epoch: 238 avg_loss: 1.8284, acc: 0.5085 

epoch: 239 avg_loss: 1.7867, acc: 0.5156 

epoch: 240 avg_loss: 1.8596, acc: 0.4967 

epoch: 241 avg_loss: 1.8251, acc: 0.5130 

epoch: 242 avg_loss: 1.8098, acc: 0.5128 

epoch: 243 avg_loss: 1.7895, acc: 0.5110 

epoch: 244 avg_loss: 1.7696, acc: 0.5130 

epoch: 245 avg_loss: 1.7853, acc: 0.5148 

epoch: 246 avg_loss: 1.7702, acc: 0.5182 

epoch: 247 avg_loss: 1.7768, acc: 0.5079 

epoch: 248 avg_loss: 1.7529, acc: 0.5217 

epoch: 249 avg_loss: 1.7760, acc: 0.5208 

epoch: 250 avg_loss: 1.7790, acc: 0.5156 

epoch: 251 avg_loss: 1.8144, acc: 0.5065 

epoch: 252 avg_loss: 1.8064, acc: 0.5099 

epoch: 253 avg_loss: 1.7364, acc: 0.5208 

epoch: 254 avg_loss: 1.7083, acc: 0.5377 

epoch: 255 avg_loss: 1.8137, acc: 0.5153 

epoch: 256 avg_loss: 1.7657, acc: 0.5174 

epoch: 257 avg_loss: 1.7569, acc: 0.5185 

epoch: 258 avg_loss: 1.7500, acc: 0.5245 

epoch: 259 avg_loss: 1.7627, acc: 0.5225 

epoch: 260 avg_loss: 1.7730, acc: 0.5217 

epoch: 261 avg_loss: 1.7648, acc: 0.5110 

epoch: 262 avg_loss: 1.7552, acc: 0.5176 

epoch: 263 avg_loss: 1.7892, acc: 0.5125 

epoch: 264 avg_loss: 1.7735, acc: 0.5159 

epoch: 265 avg_loss: 1.8077, acc: 0.5145 

epoch: 266 avg_loss: 1.7481, acc: 0.5271 

epoch: 267 avg_loss: 1.7983, acc: 0.5130 

epoch: 268 avg_loss: 1.7870, acc: 0.5139 

epoch: 269 avg_loss: 1.7712, acc: 0.5196 

epoch: 270 avg_loss: 1.7381, acc: 0.5340 

epoch: 271 avg_loss: 1.7459, acc: 0.5214 

epoch: 272 avg_loss: 1.7703, acc: 0.5268 

epoch: 273 avg_loss: 1.7183, acc: 0.5268 

epoch: 274 avg_loss: 1.7074, acc: 0.5300 

epoch: 275 avg_loss: 1.6998, acc: 0.5274 

epoch: 276 avg_loss: 1.7278, acc: 0.5317 

epoch: 277 avg_loss: 1.7121, acc: 0.5262 

epoch: 278 avg_loss: 1.6849, acc: 0.5377 

epoch: 279 avg_loss: 1.6654, acc: 0.5374 

epoch: 280 avg_loss: 1.6785, acc: 0.5420 

epoch: 281 avg_loss: 1.7138, acc: 0.5317 

epoch: 282 avg_loss: 1.6592, acc: 0.5369 

epoch: 283 avg_loss: 1.6436, acc: 0.5466 

epoch: 284 avg_loss: 1.6976, acc: 0.5377 

epoch: 285 avg_loss: 1.6669, acc: 0.5369 

epoch: 286 avg_loss: 1.6788, acc: 0.5371 

epoch: 287 avg_loss: 1.6709, acc: 0.5337 

epoch: 288 avg_loss: 1.6621, acc: 0.5386 

epoch: 289 avg_loss: 1.7099, acc: 0.5337 

epoch: 290 avg_loss: 1.6711, acc: 0.5432 

epoch: 291 avg_loss: 1.6920, acc: 0.5219 

epoch: 292 avg_loss: 1.6675, acc: 0.5389 

epoch: 293 avg_loss: 1.6595, acc: 0.5452 

epoch: 294 avg_loss: 1.6480, acc: 0.5455 

epoch: 295 avg_loss: 1.6683, acc: 0.5323 

epoch: 296 avg_loss: 1.6964, acc: 0.5340 

epoch: 297 avg_loss: 1.6576, acc: 0.5380 

epoch: 298 avg_loss: 1.6669, acc: 0.5394 

epoch: 299 avg_loss: 1.6764, acc: 0.5354 
