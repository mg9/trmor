
number of params: 360448 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=100, fig_path='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/100epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/100epochs.log', logger=<common.utils.Logger object at 0x7f7cbfdbea90>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_1plus8_dict_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=64, bias=True)
  (linear_3): Linear(in_features=512, out_features=64, bias=True)
  (linear_4): Linear(in_features=512, out_features=64, bias=True)
  (linear_5): Linear(in_features=512, out_features=64, bias=True)
  (linear_6): Linear(in_features=512, out_features=64, bias=True)
  (linear_7): Linear(in_features=512, out_features=64, bias=True)
  (linear_8): Linear(in_features=512, out_features=64, bias=True)
  (linear_9): Linear(in_features=512, out_features=64, bias=True)
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (linear): Linear(in_features=512, out_features=704, bias=False)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer): VectorQuantizer(
    (embedding): Embedding(704, 512)
  )
  (linear_2): Linear(in_features=512, out_features=64, bias=True)
  (linear_3): Linear(in_features=512, out_features=64, bias=True)
  (linear_4): Linear(in_features=512, out_features=64, bias=True)
  (linear_5): Linear(in_features=512, out_features=64, bias=True)
  (linear_6): Linear(in_features=512, out_features=64, bias=True)
  (linear_7): Linear(in_features=512, out_features=64, bias=True)
  (linear_8): Linear(in_features=512, out_features=64, bias=True)
  (linear_9): Linear(in_features=512, out_features=64, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/root_concept/results/training/vqvae_1plus8_dict_probe/3487_instances/100epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([64, 512]): False
linear_2.bias, torch.Size([64]): False
linear_3.weight, torch.Size([64, 512]): False
linear_3.bias, torch.Size([64]): False
linear_4.weight, torch.Size([64, 512]): False
linear_4.bias, torch.Size([64]): False
linear_5.weight, torch.Size([64, 512]): False
linear_5.bias, torch.Size([64]): False
linear_6.weight, torch.Size([64, 512]): False
linear_6.bias, torch.Size([64]): False
linear_7.weight, torch.Size([64, 512]): False
linear_7.bias, torch.Size([64]): False
linear_8.weight, torch.Size([64, 512]): False
linear_8.bias, torch.Size([64]): False
linear_9.weight, torch.Size([64, 512]): False
linear_9.bias, torch.Size([64]): False
vq_layer.embedding.weight, torch.Size([704, 512]): False
vq_layer_2.embedding.weight, torch.Size([16, 64]): False
vq_layer_3.embedding.weight, torch.Size([16, 64]): False
vq_layer_4.embedding.weight, torch.Size([16, 64]): False
vq_layer_5.embedding.weight, torch.Size([16, 64]): False
vq_layer_6.embedding.weight, torch.Size([16, 64]): False
vq_layer_7.embedding.weight, torch.Size([16, 64]): False
vq_layer_8.embedding.weight, torch.Size([16, 64]): False
vq_layer_9.embedding.weight, torch.Size([16, 64]): False
linear.weight, torch.Size([704, 512]): True
epoch: 0 avg_loss: 6.8035, acc: 0.0161 

epoch: 1 avg_loss: 4.9110, acc: 0.0857 

epoch: 2 avg_loss: 4.3284, acc: 0.1615 

epoch: 3 avg_loss: 4.1020, acc: 0.1818 

epoch: 4 avg_loss: 3.7333, acc: 0.2326 

epoch: 5 avg_loss: 3.6811, acc: 0.2415 

epoch: 6 avg_loss: 3.5213, acc: 0.2641 

epoch: 7 avg_loss: 3.4225, acc: 0.2739 

epoch: 8 avg_loss: 3.3951, acc: 0.2876 

epoch: 9 avg_loss: 3.3389, acc: 0.2896 

epoch: 10 avg_loss: 3.3064, acc: 0.2962 

epoch: 11 avg_loss: 3.2971, acc: 0.2945 

epoch: 12 avg_loss: 3.2161, acc: 0.3117 

epoch: 13 avg_loss: 3.1054, acc: 0.3149 

epoch: 14 avg_loss: 3.1212, acc: 0.3304 

epoch: 15 avg_loss: 3.0809, acc: 0.3198 

epoch: 16 avg_loss: 3.1429, acc: 0.3341 

epoch: 17 avg_loss: 3.0953, acc: 0.3229 

epoch: 18 avg_loss: 3.0519, acc: 0.3338 

epoch: 19 avg_loss: 3.0090, acc: 0.3387 

epoch: 20 avg_loss: 3.0190, acc: 0.3352 

epoch: 21 avg_loss: 3.0153, acc: 0.3355 

epoch: 22 avg_loss: 2.9635, acc: 0.3424 

epoch: 23 avg_loss: 2.9087, acc: 0.3513 

epoch: 24 avg_loss: 2.8968, acc: 0.3490 

epoch: 25 avg_loss: 2.9059, acc: 0.3390 

epoch: 26 avg_loss: 2.8994, acc: 0.3565 

epoch: 27 avg_loss: 2.9228, acc: 0.3461 

epoch: 28 avg_loss: 2.8839, acc: 0.3545 

epoch: 29 avg_loss: 2.8222, acc: 0.3645 

epoch: 30 avg_loss: 2.8646, acc: 0.3599 

epoch: 31 avg_loss: 2.8105, acc: 0.3636 

epoch: 32 avg_loss: 2.8169, acc: 0.3639 

epoch: 33 avg_loss: 2.8226, acc: 0.3570 

epoch: 34 avg_loss: 2.8209, acc: 0.3665 

epoch: 35 avg_loss: 2.7769, acc: 0.3659 

epoch: 36 avg_loss: 2.7306, acc: 0.3803 

epoch: 37 avg_loss: 2.7554, acc: 0.3677 

epoch: 38 avg_loss: 2.7521, acc: 0.3694 

epoch: 39 avg_loss: 2.7189, acc: 0.3751 

epoch: 40 avg_loss: 2.6924, acc: 0.3811 

epoch: 41 avg_loss: 2.6969, acc: 0.3820 

epoch: 42 avg_loss: 2.6943, acc: 0.3791 

epoch: 43 avg_loss: 2.6350, acc: 0.3892 

epoch: 44 avg_loss: 2.6745, acc: 0.3803 

epoch: 45 avg_loss: 2.6404, acc: 0.3874 

epoch: 46 avg_loss: 2.6530, acc: 0.3857 

epoch: 47 avg_loss: 2.6757, acc: 0.3745 

epoch: 48 avg_loss: 2.6402, acc: 0.3860 

epoch: 49 avg_loss: 2.5990, acc: 0.3874 

epoch: 50 avg_loss: 2.6342, acc: 0.3869 

epoch: 51 avg_loss: 2.5865, acc: 0.3923 

epoch: 52 avg_loss: 2.5855, acc: 0.3920 

epoch: 53 avg_loss: 2.5964, acc: 0.3935 

epoch: 54 avg_loss: 2.6411, acc: 0.3803 

epoch: 55 avg_loss: 2.6017, acc: 0.3866 

epoch: 56 avg_loss: 2.5669, acc: 0.3906 

epoch: 57 avg_loss: 2.6253, acc: 0.3780 

epoch: 58 avg_loss: 2.5285, acc: 0.4035 

epoch: 59 avg_loss: 2.5734, acc: 0.3975 

epoch: 60 avg_loss: 2.5869, acc: 0.3909 

epoch: 61 avg_loss: 2.5463, acc: 0.3969 

epoch: 62 avg_loss: 2.5810, acc: 0.3909 

epoch: 63 avg_loss: 2.5871, acc: 0.3894 

epoch: 64 avg_loss: 2.5292, acc: 0.3952 

epoch: 65 avg_loss: 2.5993, acc: 0.3923 

epoch: 66 avg_loss: 2.5778, acc: 0.3886 

epoch: 67 avg_loss: 2.4603, acc: 0.4084 

epoch: 68 avg_loss: 2.4988, acc: 0.4098 

epoch: 69 avg_loss: 2.4430, acc: 0.4121 

epoch: 70 avg_loss: 2.5038, acc: 0.3975 

epoch: 71 avg_loss: 2.4926, acc: 0.4084 

epoch: 72 avg_loss: 2.4857, acc: 0.4075 

epoch: 73 avg_loss: 2.4531, acc: 0.4115 

epoch: 74 avg_loss: 2.4877, acc: 0.4089 

epoch: 75 avg_loss: 2.4955, acc: 0.4032 

epoch: 76 avg_loss: 2.3788, acc: 0.4239 

epoch: 77 avg_loss: 2.4247, acc: 0.4067 

epoch: 78 avg_loss: 2.4457, acc: 0.4178 

epoch: 79 avg_loss: 2.4255, acc: 0.4170 

epoch: 80 avg_loss: 2.3752, acc: 0.4250 

epoch: 81 avg_loss: 2.4288, acc: 0.4104 

epoch: 82 avg_loss: 2.3835, acc: 0.4207 

epoch: 83 avg_loss: 2.4057, acc: 0.4161 

epoch: 84 avg_loss: 2.3731, acc: 0.4187 

epoch: 85 avg_loss: 2.4003, acc: 0.4184 

epoch: 86 avg_loss: 2.3286, acc: 0.4319 

epoch: 87 avg_loss: 2.3937, acc: 0.4196 

epoch: 88 avg_loss: 2.3265, acc: 0.4287 

epoch: 89 avg_loss: 2.3408, acc: 0.4313 

epoch: 90 avg_loss: 2.3622, acc: 0.4247 

epoch: 91 avg_loss: 2.3521, acc: 0.4244 

epoch: 92 avg_loss: 2.3860, acc: 0.4127 

epoch: 93 avg_loss: 2.3596, acc: 0.4262 

epoch: 94 avg_loss: 2.3512, acc: 0.4181 

epoch: 95 avg_loss: 2.4166, acc: 0.4075 

epoch: 96 avg_loss: 2.3832, acc: 0.4184 

epoch: 97 avg_loss: 2.4221, acc: 0.4026 

epoch: 98 avg_loss: 2.4072, acc: 0.4092 

epoch: 99 avg_loss: 2.3496, acc: 0.4167 
