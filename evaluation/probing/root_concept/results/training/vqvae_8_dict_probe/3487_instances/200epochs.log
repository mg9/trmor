
number of params: 271040 
Namespace(batchsize=128, beta=0.25, dec_dropout_in=0.0, dec_dropout_out=0.0, dec_nh=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='evaluation/probing/root_concept/results/training/vqvae_8_dict_probe/3487_instances/200epochs.png', log_path='evaluation/probing/root_concept/results/training/vqvae_8_dict_probe/3487_instances/200epochs.log', logger=<common.utils.Logger object at 0x7fc644c8ae90>, lr=0.01, maxtrnsize=57769, maxtstsize=10000, maxvalsize=10000, mname='vqvae_8_dict_probe', model=VQVAE_Probe(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=64, bias=True)
  (linear_3): Linear(in_features=512, out_features=64, bias=True)
  (linear_4): Linear(in_features=512, out_features=64, bias=True)
  (linear_5): Linear(in_features=512, out_features=64, bias=True)
  (linear_6): Linear(in_features=512, out_features=64, bias=True)
  (linear_7): Linear(in_features=512, out_features=64, bias=True)
  (linear_8): Linear(in_features=512, out_features=64, bias=True)
  (linear_9): Linear(in_features=512, out_features=64, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (linear): Linear(in_features=384, out_features=704, bias=True)
  (loss): CrossEntropyLoss()
), modelname='evaluation/probing/root_concept/results/training/vqvae_8_dict_probe/3487_instances/', nh=512, ni=256, num_embeddings=704, nz=512, opt='Adam', pretrained_model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(32, 256)
    (lstm): LSTM(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (linear_2): Linear(in_features=512, out_features=64, bias=True)
  (linear_3): Linear(in_features=512, out_features=64, bias=True)
  (linear_4): Linear(in_features=512, out_features=64, bias=True)
  (linear_5): Linear(in_features=512, out_features=64, bias=True)
  (linear_6): Linear(in_features=512, out_features=64, bias=True)
  (linear_7): Linear(in_features=512, out_features=64, bias=True)
  (linear_8): Linear(in_features=512, out_features=64, bias=True)
  (linear_9): Linear(in_features=512, out_features=64, bias=True)
  (vq_layer_2): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_3): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_4): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_5): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_6): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_7): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_8): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (vq_layer_9): VectorQuantizer(
    (embedding): Embedding(16, 64)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(32, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (dropout_out): Dropout(p=0.0, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=32, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='evaluation/probing/root_concept/results/training/vqvae_8_dict_probe/3487_instances/200epochs.pt', seq_to_no_pad='surface', task='surf2root_concept', trndata='evaluation/probing/root_concept/data/sosimple.new.trn.combined.txt', trnsize=3487, tstdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', tstsize=209, valdata='evaluation/probing/root_concept/data/sosimple.new.seenroots.val.txt', valsize=209)

encoder.embed.weight, torch.Size([32, 256]): False
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): False
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): False
encoder.lstm.bias_ih_l0, torch.Size([2048]): False
encoder.lstm.bias_hh_l0, torch.Size([2048]): False
linear_2.weight, torch.Size([64, 512]): False
linear_2.bias, torch.Size([64]): False
linear_3.weight, torch.Size([64, 512]): False
linear_3.bias, torch.Size([64]): False
linear_4.weight, torch.Size([64, 512]): False
linear_4.bias, torch.Size([64]): False
linear_5.weight, torch.Size([64, 512]): False
linear_5.bias, torch.Size([64]): False
linear_6.weight, torch.Size([64, 512]): False
linear_6.bias, torch.Size([64]): False
linear_7.weight, torch.Size([64, 512]): False
linear_7.bias, torch.Size([64]): False
linear_8.weight, torch.Size([64, 512]): False
linear_8.bias, torch.Size([64]): False
linear_9.weight, torch.Size([64, 512]): False
linear_9.bias, torch.Size([64]): False
vq_layer_2.embedding.weight, torch.Size([16, 64]): False
vq_layer_3.embedding.weight, torch.Size([16, 64]): False
vq_layer_4.embedding.weight, torch.Size([16, 64]): False
vq_layer_5.embedding.weight, torch.Size([16, 64]): False
vq_layer_6.embedding.weight, torch.Size([16, 64]): False
vq_layer_7.embedding.weight, torch.Size([16, 64]): False
vq_layer_8.embedding.weight, torch.Size([16, 64]): False
vq_layer_9.embedding.weight, torch.Size([16, 64]): False
linear.weight, torch.Size([704, 384]): True
linear.bias, torch.Size([704]): True
epoch: 0 avg_loss: 4.6462, acc: 0.2856 

epoch: 1 avg_loss: 1.3480, acc: 0.6002 

epoch: 2 avg_loss: 0.8894, acc: 0.7405 

epoch: 3 avg_loss: 0.6502, acc: 0.8013 

epoch: 4 avg_loss: 0.4734, acc: 0.8469 

epoch: 5 avg_loss: 0.3438, acc: 0.8859 

epoch: 6 avg_loss: 0.2957, acc: 0.9131 

epoch: 7 avg_loss: 0.2809, acc: 0.9183 

epoch: 8 avg_loss: 0.2675, acc: 0.9174 

epoch: 9 avg_loss: 0.2225, acc: 0.9340 

epoch: 10 avg_loss: 0.1916, acc: 0.9421 

epoch: 11 avg_loss: 0.1582, acc: 0.9478 

epoch: 12 avg_loss: 0.1577, acc: 0.9547 

epoch: 13 avg_loss: 0.1248, acc: 0.9650 

epoch: 14 avg_loss: 0.1533, acc: 0.9584 

epoch: 15 avg_loss: 0.1359, acc: 0.9621 

epoch: 16 avg_loss: 0.1167, acc: 0.9687 

epoch: 17 avg_loss: 0.1156, acc: 0.9707 

epoch: 18 avg_loss: 0.1060, acc: 0.9728 

epoch: 19 avg_loss: 0.1077, acc: 0.9722 

epoch: 20 avg_loss: 0.1325, acc: 0.9667 

epoch: 21 avg_loss: 0.1582, acc: 0.9581 

epoch: 22 avg_loss: 0.1638, acc: 0.9581 

epoch: 23 avg_loss: 0.1606, acc: 0.9544 

epoch: 24 avg_loss: 0.1504, acc: 0.9535 

epoch: 25 avg_loss: 0.1715, acc: 0.9550 

epoch: 26 avg_loss: 0.2193, acc: 0.9432 

epoch: 27 avg_loss: 0.2130, acc: 0.9429 

epoch: 28 avg_loss: 0.3753, acc: 0.9137 

epoch: 29 avg_loss: 0.3929, acc: 0.9028 

epoch: 30 avg_loss: 0.5404, acc: 0.8773 

epoch: 31 avg_loss: 0.5036, acc: 0.8775 

epoch: 32 avg_loss: 0.4439, acc: 0.8925 

epoch: 33 avg_loss: 0.3094, acc: 0.9171 

epoch: 34 avg_loss: 0.2305, acc: 0.9386 

epoch: 35 avg_loss: 0.1892, acc: 0.9449 

epoch: 36 avg_loss: 0.1836, acc: 0.9573 

epoch: 37 avg_loss: 0.1507, acc: 0.9624 

epoch: 38 avg_loss: 0.1543, acc: 0.9644 

epoch: 39 avg_loss: 0.1474, acc: 0.9664 

epoch: 40 avg_loss: 0.1082, acc: 0.9773 

epoch: 41 avg_loss: 0.0933, acc: 0.9802 

epoch: 42 avg_loss: 0.0783, acc: 0.9837 

epoch: 43 avg_loss: 0.0865, acc: 0.9825 

epoch: 44 avg_loss: 0.0796, acc: 0.9834 

epoch: 45 avg_loss: 0.0793, acc: 0.9868 

epoch: 46 avg_loss: 0.0726, acc: 0.9825 

epoch: 47 avg_loss: 0.0945, acc: 0.9816 

epoch: 48 avg_loss: 0.0792, acc: 0.9831 

epoch: 49 avg_loss: 0.0804, acc: 0.9854 

epoch: 50 avg_loss: 0.0768, acc: 0.9851 

epoch: 51 avg_loss: 0.0793, acc: 0.9865 

epoch: 52 avg_loss: 0.0833, acc: 0.9865 

epoch: 53 avg_loss: 0.0710, acc: 0.9880 

epoch: 54 avg_loss: 0.0653, acc: 0.9877 

epoch: 55 avg_loss: 0.0694, acc: 0.9877 

epoch: 56 avg_loss: 0.0730, acc: 0.9877 

epoch: 57 avg_loss: 0.0906, acc: 0.9859 

epoch: 58 avg_loss: 0.0620, acc: 0.9880 

epoch: 59 avg_loss: 0.0796, acc: 0.9865 

epoch: 60 avg_loss: 0.0535, acc: 0.9897 

epoch: 61 avg_loss: 0.0886, acc: 0.9874 

epoch: 62 avg_loss: 0.0656, acc: 0.9862 

epoch: 63 avg_loss: 0.0995, acc: 0.9825 

epoch: 64 avg_loss: 0.0867, acc: 0.9794 

epoch: 65 avg_loss: 0.1173, acc: 0.9742 

epoch: 66 avg_loss: 0.1582, acc: 0.9673 

epoch: 67 avg_loss: 0.2546, acc: 0.9415 

epoch: 68 avg_loss: 0.4101, acc: 0.9068 

epoch: 69 avg_loss: 0.5651, acc: 0.8839 

epoch: 70 avg_loss: 0.9961, acc: 0.8494 

epoch: 71 avg_loss: 1.0906, acc: 0.8431 

epoch: 72 avg_loss: 0.6727, acc: 0.8715 

epoch: 73 avg_loss: 0.6140, acc: 0.8801 

epoch: 74 avg_loss: 0.4033, acc: 0.9188 

epoch: 75 avg_loss: 0.2092, acc: 0.9498 

epoch: 76 avg_loss: 0.1799, acc: 0.9621 

epoch: 77 avg_loss: 0.1307, acc: 0.9728 

epoch: 78 avg_loss: 0.1142, acc: 0.9751 

epoch: 79 avg_loss: 0.0877, acc: 0.9831 

epoch: 80 avg_loss: 0.0997, acc: 0.9854 

epoch: 81 avg_loss: 0.0766, acc: 0.9874 

epoch: 82 avg_loss: 0.0582, acc: 0.9900 

epoch: 83 avg_loss: 0.0743, acc: 0.9885 

epoch: 84 avg_loss: 0.0535, acc: 0.9902 

epoch: 85 avg_loss: 0.0774, acc: 0.9897 

epoch: 86 avg_loss: 0.0646, acc: 0.9902 

epoch: 87 avg_loss: 0.0783, acc: 0.9888 

epoch: 88 avg_loss: 0.0638, acc: 0.9894 

epoch: 89 avg_loss: 0.0738, acc: 0.9897 

epoch: 90 avg_loss: 0.0561, acc: 0.9917 

epoch: 91 avg_loss: 0.0866, acc: 0.9908 

epoch: 92 avg_loss: 0.0728, acc: 0.9880 

epoch: 93 avg_loss: 0.0714, acc: 0.9917 

epoch: 94 avg_loss: 0.0612, acc: 0.9917 

epoch: 95 avg_loss: 0.0651, acc: 0.9905 

epoch: 96 avg_loss: 0.0595, acc: 0.9897 

epoch: 97 avg_loss: 0.0703, acc: 0.9905 

epoch: 98 avg_loss: 0.0757, acc: 0.9885 

epoch: 99 avg_loss: 0.0713, acc: 0.9900 

epoch: 100 avg_loss: 0.0674, acc: 0.9902 

epoch: 101 avg_loss: 0.0695, acc: 0.9902 

epoch: 102 avg_loss: 0.0796, acc: 0.9923 

epoch: 103 avg_loss: 0.0709, acc: 0.9917 

epoch: 104 avg_loss: 0.0622, acc: 0.9908 

epoch: 105 avg_loss: 0.0785, acc: 0.9902 

epoch: 106 avg_loss: 0.0739, acc: 0.9911 

epoch: 107 avg_loss: 0.0792, acc: 0.9908 

epoch: 108 avg_loss: 0.0692, acc: 0.9911 

epoch: 109 avg_loss: 0.0729, acc: 0.9914 

epoch: 110 avg_loss: 0.0653, acc: 0.9908 

epoch: 111 avg_loss: 0.0931, acc: 0.9880 

epoch: 112 avg_loss: 0.0969, acc: 0.9868 

epoch: 113 avg_loss: 0.0863, acc: 0.9851 

epoch: 114 avg_loss: 0.0942, acc: 0.9831 

epoch: 115 avg_loss: 0.1005, acc: 0.9831 

epoch: 116 avg_loss: 0.1336, acc: 0.9756 

epoch: 117 avg_loss: 0.1500, acc: 0.9687 

epoch: 118 avg_loss: 0.2306, acc: 0.9524 

epoch: 119 avg_loss: 0.3154, acc: 0.9392 

epoch: 120 avg_loss: 0.5337, acc: 0.9077 

epoch: 121 avg_loss: 0.6388, acc: 0.8884 

epoch: 122 avg_loss: 0.7328, acc: 0.8893 

epoch: 123 avg_loss: 0.5565, acc: 0.9091 

epoch: 124 avg_loss: 0.5127, acc: 0.9186 

epoch: 125 avg_loss: 0.3817, acc: 0.9320 

epoch: 126 avg_loss: 0.2620, acc: 0.9490 

epoch: 127 avg_loss: 0.1732, acc: 0.9656 

epoch: 128 avg_loss: 0.1433, acc: 0.9728 

epoch: 129 avg_loss: 0.1308, acc: 0.9779 

epoch: 130 avg_loss: 0.1073, acc: 0.9839 

epoch: 131 avg_loss: 0.0860, acc: 0.9842 

epoch: 132 avg_loss: 0.0775, acc: 0.9885 

epoch: 133 avg_loss: 0.0537, acc: 0.9891 

epoch: 134 avg_loss: 0.0955, acc: 0.9848 

epoch: 135 avg_loss: 0.0768, acc: 0.9874 

epoch: 136 avg_loss: 0.0811, acc: 0.9885 

epoch: 137 avg_loss: 0.0558, acc: 0.9923 

epoch: 138 avg_loss: 0.0774, acc: 0.9908 

epoch: 139 avg_loss: 0.0603, acc: 0.9917 

epoch: 140 avg_loss: 0.0676, acc: 0.9925 

epoch: 141 avg_loss: 0.0689, acc: 0.9917 

epoch: 142 avg_loss: 0.0727, acc: 0.9923 

epoch: 143 avg_loss: 0.0591, acc: 0.9923 

epoch: 144 avg_loss: 0.0682, acc: 0.9902 

epoch: 145 avg_loss: 0.0681, acc: 0.9920 

epoch: 146 avg_loss: 0.0731, acc: 0.9931 

epoch: 147 avg_loss: 0.0717, acc: 0.9908 

epoch: 148 avg_loss: 0.0842, acc: 0.9908 

epoch: 149 avg_loss: 0.0626, acc: 0.9911 

epoch: 150 avg_loss: 0.0806, acc: 0.9920 

epoch: 151 avg_loss: 0.0675, acc: 0.9917 

epoch: 152 avg_loss: 0.0801, acc: 0.9908 

epoch: 153 avg_loss: 0.0701, acc: 0.9925 

epoch: 154 avg_loss: 0.0713, acc: 0.9920 

epoch: 155 avg_loss: 0.0602, acc: 0.9923 

epoch: 156 avg_loss: 0.0686, acc: 0.9925 

epoch: 157 avg_loss: 0.0611, acc: 0.9914 

epoch: 158 avg_loss: 0.0876, acc: 0.9900 

epoch: 159 avg_loss: 0.0785, acc: 0.9905 

epoch: 160 avg_loss: 0.0696, acc: 0.9920 

epoch: 161 avg_loss: 0.0780, acc: 0.9908 

epoch: 162 avg_loss: 0.0775, acc: 0.9911 

epoch: 163 avg_loss: 0.0745, acc: 0.9908 

epoch: 164 avg_loss: 0.0815, acc: 0.9891 

epoch: 165 avg_loss: 0.0744, acc: 0.9882 

epoch: 166 avg_loss: 0.0930, acc: 0.9877 

epoch: 167 avg_loss: 0.1010, acc: 0.9857 

epoch: 168 avg_loss: 0.1057, acc: 0.9845 

epoch: 169 avg_loss: 0.1415, acc: 0.9785 

epoch: 170 avg_loss: 0.1817, acc: 0.9682 

epoch: 171 avg_loss: 0.3161, acc: 0.9501 

epoch: 172 avg_loss: 0.3035, acc: 0.9484 

epoch: 173 avg_loss: 0.3751, acc: 0.9395 

epoch: 174 avg_loss: 0.5028, acc: 0.9252 

epoch: 175 avg_loss: 0.4637, acc: 0.9326 

epoch: 176 avg_loss: 0.3402, acc: 0.9432 

epoch: 177 avg_loss: 0.3622, acc: 0.9395 

epoch: 178 avg_loss: 0.2631, acc: 0.9501 

epoch: 179 avg_loss: 0.1869, acc: 0.9653 

epoch: 180 avg_loss: 0.1700, acc: 0.9728 

epoch: 181 avg_loss: 0.1149, acc: 0.9802 

epoch: 182 avg_loss: 0.0950, acc: 0.9842 

epoch: 183 avg_loss: 0.0884, acc: 0.9868 

epoch: 184 avg_loss: 0.0978, acc: 0.9874 

epoch: 185 avg_loss: 0.0762, acc: 0.9888 

epoch: 186 avg_loss: 0.0835, acc: 0.9882 

epoch: 187 avg_loss: 0.0868, acc: 0.9891 

epoch: 188 avg_loss: 0.0902, acc: 0.9888 

epoch: 189 avg_loss: 0.0807, acc: 0.9900 

epoch: 190 avg_loss: 0.0775, acc: 0.9905 

epoch: 191 avg_loss: 0.0695, acc: 0.9920 

epoch: 192 avg_loss: 0.0684, acc: 0.9917 

epoch: 193 avg_loss: 0.0837, acc: 0.9925 

epoch: 194 avg_loss: 0.0739, acc: 0.9914 

epoch: 195 avg_loss: 0.0691, acc: 0.9914 

epoch: 196 avg_loss: 0.0666, acc: 0.9920 

epoch: 197 avg_loss: 0.0715, acc: 0.9920 

epoch: 198 avg_loss: 0.0754, acc: 0.9914 

epoch: 199 avg_loss: 0.0923, acc: 0.9902 
