
number of params: 8381440 
Namespace(batchsize=64, beta=0.5, dec_dropout_in=0.5, dec_dropout_out=0.5, dec_nh=512, dec_nh_tmp=512, device='cuda', embedding_dim=512, enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=200, fig_path='model/vqvae/results/training/12229_instances/discretelemma_bilstm_8x10_dec512_suffixd512/200epochs.png', incat=512, incat_tmp=512, lemmadict_emb_num=5000, log_path='model/vqvae/results/training/12229_instances/discretelemma_bilstm_8x10_dec512_suffixd512/200epochs.log', logger=<common.utils.Logger object at 0x7f9ab21972d0>, lr=0.001, maxtrnsize=10000000, maxtstsize=10000, maxvalsize=10000, mname='vqvae', model=VQVAE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(36, 256)
    (lstm): LSTM(256, 512, batch_first=True, bidirectional=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (vq_layer_lemma): VectorQuantizer(
    (embedding): Embedding(5000, 512)
  )
  (ord_vq_layers): ModuleList(
    (0): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (1): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (2): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (3): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (4): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (5): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (6): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
    (7): VectorQuantizer(
      (embedding): Embedding(10, 64)
    )
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(36, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.5, inplace=False)
    (dropout_out): Dropout(p=0.5, inplace=False)
    (lstm): LSTM(768, 512, batch_first=True)
    (pred_linear): Linear(in_features=512, out_features=36, bias=False)
    (loss): CrossEntropyLoss()
  )
), model_prefix='discretelemma_bilstm_8x10_dec512_suffixd512/', modelname='model/vqvae/results/training/12229_instances/discretelemma_bilstm_8x10_dec512_suffixd512/', ni=256, num_dicts=9, num_dicts_tmp=9, nz=128, opt='Adam', orddict_emb_num=10, outcat=0, outcat_tmp=0, pretrained_model=VQVAE_AE(
  (encoder): VQVAE_Encoder(
    (embed): Embedding(36, 256)
    (lstm): LSTM(256, 512, batch_first=True, bidirectional=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
  )
  (decoder): VQVAE_Decoder(
    (embed): Embedding(36, 256, padding_idx=0)
    (dropout_in): Dropout(p=0.5, inplace=False)
    (dropout_out): Dropout(p=0.5, inplace=False)
    (lstm): LSTM(256, 1024, batch_first=True)
    (pred_linear): Linear(in_features=1024, out_features=36, bias=False)
    (loss): CrossEntropyLoss()
  )
), save_path='model/vqvae/results/training/12229_instances/discretelemma_bilstm_8x10_dec512_suffixd512/200epochs.pt', seq_to_no_pad='surface', surf_vocab=<common.vocab.VocabEntry object at 0x7f9abb9089d0>, surface_vocab_file='data/sigmorphon2016/turkish-task3-train', task='vqvae', trndata='data/sigmorphon2016/turkish-task3-train', trnsize=12229, tstdata='data/sigmorphon2016/turkish-task3-dev', tstsize=12229, valdata='data/sigmorphon2016/turkish-task3-dev', valsize=1589)

encoder.embed.weight, torch.Size([36, 256]): True
encoder.lstm.weight_ih_l0, torch.Size([2048, 256]): True
encoder.lstm.weight_hh_l0, torch.Size([2048, 512]): True
encoder.lstm.bias_ih_l0, torch.Size([2048]): True
encoder.lstm.bias_hh_l0, torch.Size([2048]): True
encoder.lstm.weight_ih_l0_reverse, torch.Size([2048, 256]): True
encoder.lstm.weight_hh_l0_reverse, torch.Size([2048, 512]): True
encoder.lstm.bias_ih_l0_reverse, torch.Size([2048]): True
encoder.lstm.bias_hh_l0_reverse, torch.Size([2048]): True
vq_layer_lemma.embedding.weight, torch.Size([5000, 512]): True
ord_vq_layers.0.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.1.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.2.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.3.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.4.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.5.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.6.embedding.weight, torch.Size([10, 64]): True
ord_vq_layers.7.embedding.weight, torch.Size([10, 64]): True
decoder.embed.weight, torch.Size([36, 256]): True
decoder.lstm.weight_ih_l0, torch.Size([2048, 768]): True
decoder.lstm.weight_hh_l0, torch.Size([2048, 512]): True
decoder.lstm.bias_ih_l0, torch.Size([2048]): True
decoder.lstm.bias_hh_l0, torch.Size([2048]): True
decoder.pred_linear.weight, torch.Size([36, 512]): True
epoch: 0,  avg_loss: 27.3960, avg_recon_loss: 26.5949, avg_kl_loss: 0.0000, avg_vq_loss: 0.8011, ppl: 8.6473, acc: 0.3586, vq_inds: [3564, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 11442, unique_suffix_codes: 11442, logdet: 0.0000 
val ---  avg_loss: 17.4417, avg_recon_loss: 16.6840, avg_kl_loss: 0.0000, avg_vq_loss: 0.7577, ppl: 3.8494, acc: 0.5928, vq_inds: [1124, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1579, unique_suffix_codes: 1579, val_unique_suffix_codes: 1529 
update best loss 

epoch: 1,  avg_loss: 13.0077, avg_recon_loss: 12.0631, avg_kl_loss: 0.0000, avg_vq_loss: 0.9446, ppl: 2.6605, acc: 0.7136, vq_inds: [3751, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12075, unique_suffix_codes: 12075, logdet: 0.0000 
val ---  avg_loss: 9.5677, avg_recon_loss: 8.4943, avg_kl_loss: 0.0000, avg_vq_loss: 1.0734, ppl: 1.9863, acc: 0.8134, vq_inds: [1183, 10, 10, 9, 10, 10, 10, 10, 10], unique_vq_codes: 1584, unique_suffix_codes: 1584, val_unique_suffix_codes: 1551 
update best loss 

epoch: 2,  avg_loss: 7.5087, avg_recon_loss: 6.3430, avg_kl_loss: 0.0000, avg_vq_loss: 1.1657, ppl: 1.6728, acc: 0.8636, vq_inds: [3606, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12169, unique_suffix_codes: 12169, logdet: 0.0000 
val ---  avg_loss: 6.2557, avg_recon_loss: 5.0827, avg_kl_loss: 0.0000, avg_vq_loss: 1.1730, ppl: 1.5078, acc: 0.8998, vq_inds: [1171, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1588, unique_suffix_codes: 1588, val_unique_suffix_codes: 1561 
update best loss 

epoch: 3,  avg_loss: 5.0949, avg_recon_loss: 3.8705, avg_kl_loss: 0.0000, avg_vq_loss: 1.2244, ppl: 1.3688, acc: 0.9239, vq_inds: [3389, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12174, unique_suffix_codes: 12174, logdet: 0.0000 
val ---  avg_loss: 5.2529, avg_recon_loss: 3.9935, avg_kl_loss: 0.0000, avg_vq_loss: 1.2594, ppl: 1.3808, acc: 0.9238, vq_inds: [1155, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1537 
update best loss 

epoch: 4,  avg_loss: 4.0587, avg_recon_loss: 2.8045, avg_kl_loss: 0.0000, avg_vq_loss: 1.2542, ppl: 1.2554, acc: 0.9470, vq_inds: [3206, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12197, unique_suffix_codes: 12197, logdet: 0.0000 
val ---  avg_loss: 4.5772, avg_recon_loss: 3.2739, avg_kl_loss: 0.0000, avg_vq_loss: 1.3033, ppl: 1.3028, acc: 0.9389, vq_inds: [1134, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1534 
update best loss 

epoch: 5,  avg_loss: 3.3287, avg_recon_loss: 2.1094, avg_kl_loss: 0.0000, avg_vq_loss: 1.2193, ppl: 1.1866, acc: 0.9616, vq_inds: [3060, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12193, unique_suffix_codes: 12193, logdet: 0.0000 
val ---  avg_loss: 3.8952, avg_recon_loss: 2.6257, avg_kl_loss: 0.0000, avg_vq_loss: 1.2695, ppl: 1.2363, acc: 0.9528, vq_inds: [1132, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1540 
update best loss 

epoch: 6,  avg_loss: 2.9032, avg_recon_loss: 1.7250, avg_kl_loss: 0.0000, avg_vq_loss: 1.1783, ppl: 1.1502, acc: 0.9679, vq_inds: [2961, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12200, unique_suffix_codes: 12200, logdet: 0.0000 
val ---  avg_loss: 3.7857, avg_recon_loss: 2.5967, avg_kl_loss: 0.0000, avg_vq_loss: 1.1889, ppl: 1.2334, acc: 0.9519, vq_inds: [1128, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1557 
update best loss 

epoch: 7,  avg_loss: 2.8968, avg_recon_loss: 1.7374, avg_kl_loss: 0.0000, avg_vq_loss: 1.1595, ppl: 1.1513, acc: 0.9667, vq_inds: [2862, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12208, unique_suffix_codes: 12208, logdet: 0.0000 
val ---  avg_loss: 3.4331, avg_recon_loss: 2.2399, avg_kl_loss: 0.0000, avg_vq_loss: 1.1932, ppl: 1.1984, acc: 0.9597, vq_inds: [1125, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1550 
update best loss 

epoch: 8,  avg_loss: 2.3259, avg_recon_loss: 1.2035, avg_kl_loss: 0.0000, avg_vq_loss: 1.1225, ppl: 1.1025, acc: 0.9781, vq_inds: [2814, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12213, unique_suffix_codes: 12213, logdet: 0.0000 
val ---  avg_loss: 2.9883, avg_recon_loss: 1.8608, avg_kl_loss: 0.0000, avg_vq_loss: 1.1274, ppl: 1.1622, acc: 0.9667, vq_inds: [1110, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1526 
update best loss 

epoch: 9,  avg_loss: 1.9323, avg_recon_loss: 0.8839, avg_kl_loss: 0.0000, avg_vq_loss: 1.0484, ppl: 1.0743, acc: 0.9845, vq_inds: [2752, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12215, unique_suffix_codes: 12215, logdet: 0.0000 
val ---  avg_loss: 2.7141, avg_recon_loss: 1.6474, avg_kl_loss: 0.0000, avg_vq_loss: 1.0668, ppl: 1.1424, acc: 0.9693, vq_inds: [1113, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1521 
update best loss 

epoch: 10,  avg_loss: 1.6976, avg_recon_loss: 0.7056, avg_kl_loss: 0.0000, avg_vq_loss: 0.9921, ppl: 1.0589, acc: 0.9876, vq_inds: [2721, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 12208, unique_suffix_codes: 12208, logdet: 0.0000 
val ---  avg_loss: 2.5200, avg_recon_loss: 1.5601, avg_kl_loss: 0.0000, avg_vq_loss: 0.9599, ppl: 1.1343, acc: 0.9717, vq_inds: [1110, 10, 10, 10, 10, 10, 10, 10, 10], unique_vq_codes: 1589, unique_suffix_codes: 1589, val_unique_suffix_codes: 1508 
update best loss 
