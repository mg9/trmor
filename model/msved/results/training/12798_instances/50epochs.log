
number of params: 2630735 
Namespace(batchsize=128, dec_dropout_in=0.4, dec_nh=256, device='cuda', enc_dropout_in=0.0, enc_dropout_out=0.0, enc_nh=512, epochs=50, fig_path='model/msved/results/training/12798_instances/50epochs.png', kl_anneal=True, kl_start=0.1, log_path='model/msved/results/training/12798_instances/50epochs.log', logger=<common.utils.Logger object at 0x7f5ed2ab4550>, lr=0.001, maxtrnsize=7000000, maxtstsize=10000, maxvalsize=10000, mname='msved', model=MSVED(
  (encoder): MSVED_Encoder(
    (embed): Embedding(36, 256)
    (gru): GRU(256, 512, batch_first=True)
    (dropout_in): Dropout(p=0.0, inplace=False)
    (linear): Linear(in_features=512, out_features=300, bias=False)
  )
  (decoder): MSVED_Decoder(
    (char_embed): Embedding(36, 300, padding_idx=0)
    (dropout_in): Dropout(p=0.4, inplace=False)
    (trans_linear): Linear(in_features=150, out_features=256, bias=False)
    (gru): GRU(650, 256, batch_first=True)
    (attn): Linear(in_features=706, out_features=11, bias=True)
    (attn_combine): Linear(in_features=650, out_features=650, bias=True)
    (pred_linear): Linear(in_features=256, out_features=36, bias=False)
    (loss): CrossEntropyLoss()
  )
  (z_to_dec): Linear(in_features=150, out_features=256, bias=True)
  (tag_to_dec): Linear(in_features=200, out_features=256, bias=True)
  (tag_embeddings): ModuleList(
    (0): Embedding(7, 200)
    (1): Embedding(3, 200)
    (2): Embedding(2, 200)
    (3): Embedding(2, 200)
    (4): Embedding(3, 200)
    (5): Embedding(4, 200)
    (6): Embedding(3, 200)
    (7): Embedding(4, 200)
    (8): Embedding(5, 200)
    (9): Embedding(2, 200)
    (10): Embedding(7, 200)
  )
), modelname='model/msved/results/training/12798_instances/', ni=256, nz=150, opt='Adam', save_path='model/msved/results/training/12798_instances/50epochs.pt', seq_to_no_pad='surface', surface_vocab_file='data/sigmorphon2016/turkish-task3-train', task='msved', trndata='data/sigmorphon2016/turkish-task3-train', trnsize=12798, tstdata='data/sigmorphon2016/turkish-task3-dev', tstsize=12798, valdata='data/sigmorphon2016/turkish-task3-dev', valsize=1600, warm_up=10)

encoder.embed.weight, torch.Size([36, 256]): True
encoder.gru.weight_ih_l0, torch.Size([1536, 256]): True
encoder.gru.weight_hh_l0, torch.Size([1536, 512]): True
encoder.gru.bias_ih_l0, torch.Size([1536]): True
encoder.gru.bias_hh_l0, torch.Size([1536]): True
encoder.linear.weight, torch.Size([300, 512]): True
decoder.char_embed.weight, torch.Size([36, 300]): True
decoder.trans_linear.weight, torch.Size([256, 150]): True
decoder.gru.weight_ih_l0, torch.Size([768, 650]): True
decoder.gru.weight_hh_l0, torch.Size([768, 256]): True
decoder.gru.bias_ih_l0, torch.Size([768]): True
decoder.gru.bias_hh_l0, torch.Size([768]): True
decoder.attn.weight, torch.Size([11, 706]): True
decoder.attn.bias, torch.Size([11]): True
decoder.attn_combine.weight, torch.Size([650, 650]): True
decoder.attn_combine.bias, torch.Size([650]): True
decoder.pred_linear.weight, torch.Size([36, 256]): True
z_to_dec.weight, torch.Size([256, 150]): True
z_to_dec.bias, torch.Size([256]): True
tag_to_dec.weight, torch.Size([256, 200]): True
tag_to_dec.bias, torch.Size([256]): True
tag_embeddings.0.weight, torch.Size([7, 200]): True
tag_embeddings.1.weight, torch.Size([3, 200]): True
tag_embeddings.2.weight, torch.Size([2, 200]): True
tag_embeddings.3.weight, torch.Size([2, 200]): True
tag_embeddings.4.weight, torch.Size([3, 200]): True
tag_embeddings.5.weight, torch.Size([4, 200]): True
tag_embeddings.6.weight, torch.Size([3, 200]): True
tag_embeddings.7.weight, torch.Size([4, 200]): True
tag_embeddings.8.weight, torch.Size([5, 200]): True
tag_embeddings.9.weight, torch.Size([2, 200]): True
tag_embeddings.10.weight, torch.Size([7, 200]): True
epoch: 0, kl_weight: 0.19, avg_loss: 34.5669, ppl: 16.4690, nll: 34.5319, avg_kl_loss: 0.2087, acc: 0.1707
val --- kl_weight: 0.19, avg_loss: 33.8983, ppl: 15.1124, nll: 33.6197, avg_kl_loss: 1.4663, acc: 0.1674
update best loss 

epoch: 1, kl_weight: 0.20, avg_loss: 25.5167, ppl: 7.6815, nll: 25.1310, avg_kl_loss: 1.9318, acc: 0.3956
val --- kl_weight: 0.20, avg_loss: 35.4186, ppl: 16.8959, nll: 35.0009, avg_kl_loss: 2.0884, acc: 0.2268

epoch: 2, kl_weight: 0.20, avg_loss: 20.0091, ppl: 4.9166, nll: 19.6310, avg_kl_loss: 1.8905, acc: 0.5412
val --- kl_weight: 0.20, avg_loss: 38.6057, ppl: 21.9858, nll: 38.2611, avg_kl_loss: 1.7231, acc: 0.2540

epoch: 3, kl_weight: 0.20, avg_loss: 16.9906, ppl: 3.8626, nll: 16.6569, avg_kl_loss: 1.6685, acc: 0.6106
val --- kl_weight: 0.20, avg_loss: 41.8722, ppl: 28.5386, nll: 41.4907, avg_kl_loss: 1.9077, acc: 0.2680

epoch: 4, kl_weight: 0.20, avg_loss: 15.1764, ppl: 3.3302, nll: 14.8291, avg_kl_loss: 1.7368, acc: 0.6505
val --- kl_weight: 0.20, avg_loss: 41.7310, ppl: 28.2003, nll: 41.3430, avg_kl_loss: 1.9399, acc: 0.2816

epoch: 5, kl_weight: 0.20, avg_loss: 14.2225, ppl: 3.0644, nll: 13.8035, avg_kl_loss: 2.0950, acc: 0.6729
val --- kl_weight: 0.20, avg_loss: 46.4155, ppl: 41.0089, nll: 45.9790, avg_kl_loss: 2.1826, acc: 0.2918

epoch: 6, kl_weight: 0.20, avg_loss: 13.4098, ppl: 2.8477, nll: 12.8995, avg_kl_loss: 2.5517, acc: 0.6934
val --- kl_weight: 0.20, avg_loss: 46.2465, ppl: 39.9212, nll: 45.6462, avg_kl_loss: 3.0015, acc: 0.3114

epoch: 7, kl_weight: 0.20, avg_loss: 12.7574, ppl: 2.6721, nll: 12.1151, avg_kl_loss: 3.2113, acc: 0.7117
val --- kl_weight: 0.20, avg_loss: 45.7178, ppl: 38.1837, nll: 45.0953, avg_kl_loss: 3.1123, acc: 0.3129

epoch: 8, kl_weight: 0.20, avg_loss: 12.2673, ppl: 2.5372, nll: 11.4764, avg_kl_loss: 3.9546, acc: 0.7253
val --- kl_weight: 0.20, avg_loss: 48.4035, ppl: 46.3476, nll: 47.4942, avg_kl_loss: 4.5469, acc: 0.3305

epoch: 9, kl_weight: 0.20, avg_loss: 11.7335, ppl: 2.3991, nll: 10.7869, avg_kl_loss: 4.7329, acc: 0.7409
val --- kl_weight: 0.20, avg_loss: 47.0315, ppl: 41.0540, nll: 45.9926, avg_kl_loss: 5.1942, acc: 0.3520

epoch: 10, kl_weight: 0.20, avg_loss: 11.2630, ppl: 2.2735, nll: 10.1238, avg_kl_loss: 5.6956, acc: 0.7550
val --- kl_weight: 0.20, avg_loss: 46.2655, ppl: 37.6044, nll: 44.9060, avg_kl_loss: 6.7973, acc: 0.3789

epoch: 11, kl_weight: 0.20, avg_loss: 10.8159, ppl: 2.1642, nll: 9.5164, avg_kl_loss: 6.4971, acc: 0.7692
val --- kl_weight: 0.20, avg_loss: 48.2441, ppl: 43.8828, nll: 46.8176, avg_kl_loss: 7.1323, acc: 0.3841

epoch: 12, kl_weight: 0.20, avg_loss: 10.2687, ppl: 2.0423, nll: 8.8018, avg_kl_loss: 7.3345, acc: 0.7850
val --- kl_weight: 0.20, avg_loss: 47.5204, ppl: 40.8740, nll: 45.9383, avg_kl_loss: 7.9109, acc: 0.3936

epoch: 13, kl_weight: 0.20, avg_loss: 9.8503, ppl: 1.9521, nll: 8.2451, avg_kl_loss: 8.0260, acc: 0.7972
val --- kl_weight: 0.20, avg_loss: 46.1011, ppl: 36.1614, nll: 44.4216, avg_kl_loss: 8.3977, acc: 0.4143

epoch: 14, kl_weight: 0.20, avg_loss: 9.5663, ppl: 1.8879, nll: 7.8326, avg_kl_loss: 8.6683, acc: 0.8065
val --- kl_weight: 0.20, avg_loss: 47.0082, ppl: 38.6363, nll: 45.2412, avg_kl_loss: 8.8350, acc: 0.4288

epoch: 15, kl_weight: 0.20, avg_loss: 9.1802, ppl: 1.8191, nll: 7.3755, avg_kl_loss: 9.0235, acc: 0.8166
val --- kl_weight: 0.20, avg_loss: 46.1013, ppl: 35.8683, nll: 44.3208, avg_kl_loss: 8.9024, acc: 0.4492

epoch: 16, kl_weight: 0.20, avg_loss: 8.8333, ppl: 1.7548, nll: 6.9319, avg_kl_loss: 9.5070, acc: 0.8260
val --- kl_weight: 0.20, avg_loss: 46.9206, ppl: 37.2568, nll: 44.7911, avg_kl_loss: 10.6479, acc: 0.4453

epoch: 17, kl_weight: 0.20, avg_loss: 8.6061, ppl: 1.7130, nll: 6.6345, avg_kl_loss: 9.8580, acc: 0.8337
val --- kl_weight: 0.20, avg_loss: 46.0696, ppl: 35.0489, nll: 44.0347, avg_kl_loss: 10.1741, acc: 0.4649

epoch: 18, kl_weight: 0.20, avg_loss: 8.2419, ppl: 1.6545, nll: 6.2066, avg_kl_loss: 10.1763, acc: 0.8433
val --- kl_weight: 0.20, avg_loss: 46.6525, ppl: 36.7240, nll: 44.6127, avg_kl_loss: 10.1990, acc: 0.4694

epoch: 19, kl_weight: 0.20, avg_loss: 7.9969, ppl: 1.6153, nll: 5.9108, avg_kl_loss: 10.4309, acc: 0.8499
val --- kl_weight: 0.20, avg_loss: 45.4583, ppl: 32.8755, nll: 43.2422, avg_kl_loss: 11.0807, acc: 0.4882

epoch: 20, kl_weight: 0.20, avg_loss: 7.7902, ppl: 1.5789, nll: 5.6300, avg_kl_loss: 10.8011, acc: 0.8572
val --- kl_weight: 0.20, avg_loss: 45.0272, ppl: 31.5839, nll: 42.7459, avg_kl_loss: 11.4062, acc: 0.5032

epoch: 21, kl_weight: 0.20, avg_loss: 7.6190, ppl: 1.5518, nll: 5.4160, avg_kl_loss: 11.0150, acc: 0.8627
val --- kl_weight: 0.20, avg_loss: 45.5706, ppl: 33.0358, nll: 43.3024, avg_kl_loss: 11.3412, acc: 0.5049

epoch: 22, kl_weight: 0.20, avg_loss: 7.2672, ppl: 1.5056, nll: 5.0434, avg_kl_loss: 11.1188, acc: 0.8706
val --- kl_weight: 0.20, avg_loss: 44.4207, ppl: 30.0530, nll: 42.1308, avg_kl_loss: 11.4494, acc: 0.5204

epoch: 23, kl_weight: 0.20, avg_loss: 7.0411, ppl: 1.4748, nll: 4.7891, avg_kl_loss: 11.2600, acc: 0.8776
val --- kl_weight: 0.20, avg_loss: 46.3999, ppl: 35.6302, nll: 44.2384, avg_kl_loss: 10.8076, acc: 0.5033

epoch: 24, kl_weight: 0.20, avg_loss: 6.9621, ppl: 1.4559, nll: 4.6297, avg_kl_loss: 11.6621, acc: 0.8813
val --- kl_weight: 0.20, avg_loss: 44.6363, ppl: 30.2862, nll: 42.2265, avg_kl_loss: 12.0492, acc: 0.5294

epoch: 25, kl_weight: 0.20, avg_loss: 6.7109, ppl: 1.4271, nll: 4.3834, avg_kl_loss: 11.6376, acc: 0.8868
val --- kl_weight: 0.20, avg_loss: 43.0117, ppl: 26.6676, nll: 40.6511, avg_kl_loss: 11.8027, acc: 0.5559

epoch: 26, kl_weight: 0.20, avg_loss: 6.5621, ppl: 1.4048, nll: 4.1900, avg_kl_loss: 11.8606, acc: 0.8919
val --- kl_weight: 0.20, avg_loss: 44.6817, ppl: 30.5676, nll: 42.3410, avg_kl_loss: 11.7033, acc: 0.5489

epoch: 27, kl_weight: 0.20, avg_loss: 6.3564, ppl: 1.3817, nll: 3.9856, avg_kl_loss: 11.8539, acc: 0.8972
val --- kl_weight: 0.20, avg_loss: 43.2197, ppl: 26.8860, nll: 40.7522, avg_kl_loss: 12.3377, acc: 0.5651

epoch: 28, kl_weight: 0.20, avg_loss: 6.2146, ppl: 1.3669, nll: 3.8522, avg_kl_loss: 11.8118, acc: 0.9011
val --- kl_weight: 0.20, avg_loss: 46.1402, ppl: 34.3692, nll: 43.7923, avg_kl_loss: 11.7398, acc: 0.5442

epoch: 29, kl_weight: 0.20, avg_loss: 6.0118, ppl: 1.3413, nll: 3.6193, avg_kl_loss: 11.9627, acc: 0.9069
val --- kl_weight: 0.20, avg_loss: 42.9324, ppl: 26.3304, nll: 40.4936, avg_kl_loss: 12.1939, acc: 0.5830

epoch: 30, kl_weight: 0.20, avg_loss: 5.8927, ppl: 1.3289, nll: 3.5049, avg_kl_loss: 11.9388, acc: 0.9097
val --- kl_weight: 0.20, avg_loss: 41.7348, ppl: 23.9950, nll: 39.3437, avg_kl_loss: 11.9555, acc: 0.5919

epoch: 31, kl_weight: 0.20, avg_loss: 5.6723, ppl: 1.3038, nll: 3.2702, avg_kl_loss: 12.0105, acc: 0.9158
val --- kl_weight: 0.20, avg_loss: 43.0454, ppl: 26.4092, nll: 40.5306, avg_kl_loss: 12.5742, acc: 0.5892

epoch: 32, kl_weight: 0.20, avg_loss: 5.6284, ppl: 1.2984, nll: 3.2185, avg_kl_loss: 12.0495, acc: 0.9172
val --- kl_weight: 0.20, avg_loss: 44.4629, ppl: 29.8951, nll: 42.0656, avg_kl_loss: 11.9868, acc: 0.5799

epoch: 33, kl_weight: 0.20, avg_loss: 5.4675, ppl: 1.2823, nll: 3.0647, avg_kl_loss: 12.0141, acc: 0.9216
val --- kl_weight: 0.20, avg_loss: 41.2776, ppl: 22.8812, nll: 38.7552, avg_kl_loss: 12.6116, acc: 0.6149

epoch: 34, kl_weight: 0.20, avg_loss: 5.8543, ppl: 1.3114, nll: 3.3412, avg_kl_loss: 12.5658, acc: 0.9142
val --- kl_weight: 0.20, avg_loss: 44.6547, ppl: 30.4637, nll: 42.2989, avg_kl_loss: 11.7791, acc: 0.5894

epoch: 35, kl_weight: 0.20, avg_loss: 5.4571, ppl: 1.2778, nll: 3.0219, avg_kl_loss: 12.1758, acc: 0.9228
val --- kl_weight: 0.20, avg_loss: 43.7681, ppl: 28.3627, nll: 41.4141, avg_kl_loss: 11.7702, acc: 0.5948

epoch: 36, kl_weight: 0.20, avg_loss: 5.3832, ppl: 1.2693, nll: 2.9391, avg_kl_loss: 12.2208, acc: 0.9241
val --- kl_weight: 0.20, avg_loss: 40.3932, ppl: 21.4948, nll: 37.9814, avg_kl_loss: 12.0587, acc: 0.6286

epoch: 37, kl_weight: 0.20, avg_loss: 4.9925, ppl: 1.2356, nll: 2.6074, avg_kl_loss: 11.9256, acc: 0.9333
val --- kl_weight: 0.20, avg_loss: 41.7508, ppl: 24.1114, nll: 39.4036, avg_kl_loss: 11.7357, acc: 0.6229
